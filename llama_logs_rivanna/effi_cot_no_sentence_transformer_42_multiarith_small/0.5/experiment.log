2025-05-08 22:43:09,359 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_multiarith_small/0.5
2025-05-08 22:43:09,359 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/home/nee7ne/data/nee7ne/effi_cot/saved_models/effi_cot/no_sentence_transformer/small/multiarith/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_sentence_transformer/small/multiarith/0.5', 'result_path': './results/effi_cot/no_sentence_transformer/small/multiarith/0.5', 'experiment_name': 'effi_cot_no_sentence_transformer_42_multiarith_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.001, 'st_linear_epochs': 5, 'st_llm_lr': 1e-07, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.001, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-05, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/MultiArith', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-08 23:00:02,956 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_multiarith_small/0.5
2025-05-08 23:00:02,957 [INFO] Training contemplation generator with variation: no_sentence_transformer
2025-05-08 23:43:12,407 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_multiarith_small/0.5
2025-05-08 23:43:12,407 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/home/nee7ne/data/nee7ne/effi_cot/saved_models/effi_cot/no_sentence_transformer/small/multiarith/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_sentence_transformer/small/multiarith/0.5', 'result_path': './results/effi_cot/no_sentence_transformer/small/multiarith/0.5', 'experiment_name': 'effi_cot_no_sentence_transformer_42_multiarith_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.001, 'st_linear_epochs': 5, 'st_llm_lr': 1e-07, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.001, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-05, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/MultiArith', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-08 23:43:34,145 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_multiarith_small/0.5
2025-05-08 23:43:34,160 [INFO] Training contemplation generator with variation: no_sentence_transformer
2025-05-08 23:46:43,188 [INFO] Step 0 - total_loss: 1.2328, reason_loss: 0.7377, ans_loss: 1.7280, eval_loss: 1.0343
2025-05-08 23:48:59,787 [INFO] Step 1 - total_loss: 0.9529, reason_loss: 0.3689, ans_loss: 1.5368, eval_loss: 0.8846
2025-05-08 23:51:13,917 [INFO] Step 2 - total_loss: 0.8229, reason_loss: 0.1943, ans_loss: 1.4515, eval_loss: 0.7979
2025-05-08 23:51:47,075 [INFO] Loading best validation loss = 0.7978703176809682
2025-05-09 13:48:53,015 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_multiarith_small/0.5
2025-05-09 13:48:53,028 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/no_sentence_transformer/small/multiarith/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_sentence_transformer/small/multiarith/0.5', 'result_path': './results/effi_cot/no_sentence_transformer/small/multiarith/0.5', 'experiment_name': 'effi_cot_no_sentence_transformer_42_multiarith_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.001, 'st_linear_epochs': 5, 'st_llm_lr': 1e-07, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.001, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-05, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/MultiArith', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-09 13:49:01,099 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_multiarith_small/0.5
2025-05-09 13:49:01,099 [INFO] Training contemplation generator with variation: no_sentence_transformer
2025-05-09 13:51:54,867 [INFO] Step 0 - total_loss: 1.2328, reason_loss: 0.7377, ans_loss: 1.7280, eval_loss: 1.0343
2025-05-09 13:54:00,404 [INFO] Step 1 - total_loss: 0.9529, reason_loss: 0.3689, ans_loss: 1.5368, eval_loss: 0.8846
2025-05-09 13:56:13,145 [INFO] Step 2 - total_loss: 0.8229, reason_loss: 0.1943, ans_loss: 1.4515, eval_loss: 0.7979
2025-05-09 13:56:22,957 [INFO] Loading best validation loss = 0.7978703176809682
2025-05-09 15:13:08,866 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_multiarith_small/0.5
2025-05-09 15:13:08,880 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/no_sentence_transformer/small/multiarith/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_sentence_transformer/small/multiarith/0.5', 'result_path': './results/effi_cot/no_sentence_transformer/small/multiarith/0.5', 'experiment_name': 'effi_cot_no_sentence_transformer_42_multiarith_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.001, 'st_linear_epochs': 5, 'st_llm_lr': 1e-07, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.001, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-05, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/MultiArith', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-09 15:13:17,980 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_multiarith_small/0.5
2025-05-09 15:13:17,980 [INFO] Training contemplation generator with variation: no_sentence_transformer
2025-05-09 15:16:30,206 [INFO] Step 0 - total_loss: 1.2328, reason_loss: 0.7377, ans_loss: 1.7280, eval_loss: 1.0343
2025-05-09 15:18:42,855 [INFO] Step 1 - total_loss: 0.9529, reason_loss: 0.3689, ans_loss: 1.5368, eval_loss: 0.8846
2025-05-09 15:20:59,721 [INFO] Step 2 - total_loss: 0.8229, reason_loss: 0.1943, ans_loss: 1.4515, eval_loss: 0.7979
2025-05-09 15:21:12,019 [INFO] Loading best validation loss = 0.7978703176809682
2025-05-09 15:23:19,650 [INFO] Step 0 - total_loss: 0.7246, reason_loss: 0.1467, ans_loss: 1.3025, eval_loss: 0.7637
2025-05-09 15:25:31,201 [INFO] Step 1 - total_loss: 0.6992, reason_loss: 0.1386, ans_loss: 1.2597, eval_loss: 0.7568
2025-05-09 15:25:43,429 [INFO] Loading best validation loss = 0.7567989646560616
2025-05-09 15:52:57,270 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_multiarith_small/0.5
2025-05-09 15:52:57,271 [INFO] Training contemplation generator with variation: no_sentence_transformer
2025-05-09 15:55:50,426 [INFO] Step 0 - total_loss: 1.2772, reason_loss: 0.7927, ans_loss: 1.7617, eval_loss: 1.0518
2025-05-09 15:58:00,851 [INFO] Step 1 - total_loss: 0.9981, reason_loss: 0.4753, ans_loss: 1.5209, eval_loss: 0.9276
2025-05-09 16:00:12,532 [INFO] Step 2 - total_loss: 0.8912, reason_loss: 0.3217, ans_loss: 1.4607, eval_loss: 0.8371
2025-05-09 16:00:24,941 [INFO] Loading best validation loss = 0.8371494667397605
2025-05-09 16:02:31,944 [INFO] Step 0 - total_loss: 0.7875, reason_loss: 0.2728, ans_loss: 1.3022, eval_loss: 0.8221
2025-05-09 16:04:48,008 [INFO] Step 1 - total_loss: 0.7600, reason_loss: 0.2654, ans_loss: 1.2545, eval_loss: 0.8165
2025-05-09 16:05:00,539 [INFO] Loading best validation loss = 0.8165330742796262
2025-05-09 16:32:40,491 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_multiarith_small/0.5
2025-05-09 16:32:40,491 [INFO] Training contemplation generator with variation: no_sentence_transformer
2025-05-09 16:35:33,436 [INFO] Step 0 - total_loss: 1.4600, reason_loss: 0.9911, ans_loss: 1.9288, eval_loss: 1.2006
2025-05-09 16:37:43,744 [INFO] Step 1 - total_loss: 1.1461, reason_loss: 0.8585, ans_loss: 1.4337, eval_loss: 1.1110
2025-05-09 16:39:53,705 [INFO] Step 2 - total_loss: 1.0201, reason_loss: 0.6806, ans_loss: 1.3597, eval_loss: 1.0370
2025-05-09 16:40:06,658 [INFO] Loading best validation loss = 1.0370281123452716
2025-05-09 16:42:13,447 [INFO] Step 0 - total_loss: 0.8912, reason_loss: 0.5785, ans_loss: 1.2039, eval_loss: 0.9805
2025-05-09 16:44:26,249 [INFO] Step 1 - total_loss: 0.8488, reason_loss: 0.5621, ans_loss: 1.1355, eval_loss: 0.9688
2025-05-09 16:44:37,146 [INFO] Loading best validation loss = 0.9687889905439483
