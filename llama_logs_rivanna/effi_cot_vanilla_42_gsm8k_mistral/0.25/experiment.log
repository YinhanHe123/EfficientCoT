2025-05-17 11:27:34,807 [INFO] Logging to ./logs/effi_cot_vanilla_42_gsm8k_mistral/0.25
2025-05-17 11:27:34,807 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/gsm8k/0.25', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/gsm8k/0.25', 'result_path': './results/effi_cot/vanilla/mistral/gsm8k/0.25', 'experiment_name': 'effi_cot_vanilla_42_gsm8k_mistral/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.001, 'st_linear_epochs': 1, 'st_llm_lr': 1e-07, 'st_llm_wd': 1e-05, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'openai/gsm8k', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-17 11:27:35,167 [INFO] Logging to ./logs/effi_cot_vanilla_42_gsm8k_mistral/0.25
2025-05-17 11:27:35,167 [INFO] Training sentence transformer
2025-05-17 11:31:27,220 [INFO] Step 0 - train_loss: 0.9290, val_loss: 0.8627
2025-05-17 11:31:49,676 [INFO] Loading best validation loss = 0.8627083823084831
2025-05-17 11:33:08,604 [INFO] Step 0 - train_loss: 0.8373, val_loss: 0.8384
2025-05-17 11:33:37,018 [INFO] Loading best validation loss = 0.8383713409304618
2025-05-17 11:34:10,046 [INFO] Logging to ./logs/effi_cot_vanilla_42_gsm8k_mistral/0.25
2025-05-17 11:34:10,047 [INFO] Training contemplation generator with variation: vanilla
2025-05-17 11:39:48,706 [INFO] Step 0 - total_loss: 1.5948, reason_loss: 0.8566, ans_loss: 1.8409, eval_loss: 1.3529
2025-05-17 11:44:19,218 [INFO] Step 1 - total_loss: 1.2854, reason_loss: 0.4243, ans_loss: 1.5725, eval_loss: 1.3165
2025-05-17 11:48:50,154 [INFO] Step 2 - total_loss: 1.1811, reason_loss: 0.3189, ans_loss: 1.4686, eval_loss: 1.2623
2025-05-17 11:48:59,462 [INFO] Loading best validation loss = 1.2623195698112248
2025-05-17 11:53:29,014 [INFO] Step 0 - total_loss: 1.0671, reason_loss: 0.2994, ans_loss: 1.3230, eval_loss: 1.2617
2025-05-17 11:58:02,511 [INFO] Step 1 - total_loss: 1.0665, reason_loss: 0.2991, ans_loss: 1.3222, eval_loss: 1.2611
2025-05-17 11:58:11,460 [INFO] Loading best validation loss = 1.261132208034396
2025-05-17 12:31:12,464 [INFO] Logging to ./logs/effi_cot_vanilla_42_gsm8k_mistral/0.25
2025-05-17 12:31:12,464 [INFO] Training sentence transformer
2025-05-17 12:35:01,571 [INFO] Step 0 - train_loss: 0.9361, val_loss: 0.8388
2025-05-17 12:35:28,350 [INFO] Loading best validation loss = 0.838770279288292
2025-05-17 12:36:46,604 [INFO] Step 0 - train_loss: 0.8220, val_loss: 0.8357
2025-05-17 12:37:13,569 [INFO] Loading best validation loss = 0.8356917217373848
2025-05-17 12:37:38,054 [INFO] Logging to ./logs/effi_cot_vanilla_42_gsm8k_mistral/0.25
2025-05-17 12:37:38,054 [INFO] Training contemplation generator with variation: vanilla
2025-05-17 12:43:18,303 [INFO] Step 0 - total_loss: 1.6832, reason_loss: 0.7238, ans_loss: 2.0029, eval_loss: 1.4049
2025-05-17 12:47:49,769 [INFO] Step 1 - total_loss: 1.2698, reason_loss: 0.3994, ans_loss: 1.5599, eval_loss: 1.3081
2025-05-17 12:52:22,961 [INFO] Step 2 - total_loss: 1.1643, reason_loss: 0.3067, ans_loss: 1.4501, eval_loss: 1.3138
2025-05-17 12:52:30,021 [INFO] Loading best validation loss = 1.308068930953741
2025-05-17 12:56:57,169 [INFO] Step 0 - total_loss: 1.1386, reason_loss: 0.3213, ans_loss: 1.4110, eval_loss: 1.3075
2025-05-17 13:01:28,349 [INFO] Step 1 - total_loss: 1.1379, reason_loss: 0.3212, ans_loss: 1.4102, eval_loss: 1.3070
2025-05-17 13:01:37,821 [INFO] Loading best validation loss = 1.3069532488286495
2025-05-17 13:34:34,210 [INFO] Logging to ./logs/effi_cot_vanilla_42_gsm8k_mistral/0.25
2025-05-17 13:34:34,210 [INFO] Training sentence transformer
2025-05-17 13:38:35,747 [INFO] Step 0 - train_loss: 0.9266, val_loss: 0.8459
2025-05-17 13:39:00,143 [INFO] Loading best validation loss = 0.845870940387249
2025-05-17 13:40:19,379 [INFO] Step 0 - train_loss: 0.8361, val_loss: 0.8299
2025-05-17 13:40:45,457 [INFO] Loading best validation loss = 0.8298535019159317
2025-05-17 13:41:07,089 [INFO] Logging to ./logs/effi_cot_vanilla_42_gsm8k_mistral/0.25
2025-05-17 13:41:07,089 [INFO] Training contemplation generator with variation: vanilla
2025-05-17 13:46:47,276 [INFO] Step 0 - total_loss: 1.5966, reason_loss: 0.6805, ans_loss: 1.9020, eval_loss: 1.4239
2025-05-17 13:51:18,255 [INFO] Step 1 - total_loss: 1.3735, reason_loss: 0.3228, ans_loss: 1.7238, eval_loss: 1.3300
2025-05-17 13:55:49,185 [INFO] Step 2 - total_loss: 1.2656, reason_loss: 0.2594, ans_loss: 1.6010, eval_loss: 1.2755
2025-05-17 13:55:59,192 [INFO] Loading best validation loss = 1.2754612743854523
2025-05-17 14:00:26,124 [INFO] Step 0 - total_loss: 1.1707, reason_loss: 0.2581, ans_loss: 1.4749, eval_loss: 1.2749
2025-05-17 14:04:58,482 [INFO] Step 1 - total_loss: 1.1702, reason_loss: 0.2579, ans_loss: 1.4743, eval_loss: 1.2743
2025-05-17 14:05:07,126 [INFO] Loading best validation loss = 1.2742783910036086
2025-05-17 18:45:28,753 [INFO] Logging to ./logs/effi_cot_vanilla_42_gsm8k_mistral/0.25
2025-05-17 18:45:28,765 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/gsm8k/0.25', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/gsm8k/0.25', 'result_path': './results/effi_cot/vanilla/mistral/gsm8k/0.25', 'experiment_name': 'effi_cot_vanilla_42_gsm8k_mistral/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 2, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.001, 'st_linear_epochs': 1, 'st_llm_lr': 1e-07, 'st_llm_wd': 1e-05, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'openai/gsm8k', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-17 18:57:52,991 [INFO] Logging to ./logs/effi_cot_vanilla_42_gsm8k_mistral/0.25
2025-05-17 18:57:52,991 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/gsm8k/0.25', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/gsm8k/0.25', 'result_path': './results/effi_cot/vanilla/mistral/gsm8k/0.25', 'experiment_name': 'effi_cot_vanilla_42_gsm8k_mistral/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 3, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.001, 'st_linear_epochs': 1, 'st_llm_lr': 1e-07, 'st_llm_wd': 1e-05, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'openai/gsm8k', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-17 19:02:01,938 [INFO] Logging to ./logs/effi_cot_vanilla_42_gsm8k_mistral/0.25
2025-05-17 19:02:01,938 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/gsm8k/0.25', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/gsm8k/0.25', 'result_path': './results/effi_cot/vanilla/mistral/gsm8k/0.25', 'experiment_name': 'effi_cot_vanilla_42_gsm8k_mistral/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 4, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.001, 'st_linear_epochs': 1, 'st_llm_lr': 1e-07, 'st_llm_wd': 1e-05, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'openai/gsm8k', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-17 19:03:07,995 [INFO] Logging to ./logs/effi_cot_vanilla_42_gsm8k_mistral/0.25
2025-05-17 19:03:07,996 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/gsm8k/0.25', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/gsm8k/0.25', 'result_path': './results/effi_cot/vanilla/mistral/gsm8k/0.25', 'experiment_name': 'effi_cot_vanilla_42_gsm8k_mistral/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 5, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.001, 'st_linear_epochs': 1, 'st_llm_lr': 1e-07, 'st_llm_wd': 1e-05, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'openai/gsm8k', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
