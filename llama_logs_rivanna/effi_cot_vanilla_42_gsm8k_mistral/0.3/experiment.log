2025-05-17 12:49:34,389 [INFO] Logging to ./logs/effi_cot_vanilla_42_gsm8k_mistral/0.3
2025-05-17 12:49:34,389 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/gsm8k/0.3', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/gsm8k/0.3', 'result_path': './results/effi_cot/vanilla/mistral/gsm8k/0.3', 'experiment_name': 'effi_cot_vanilla_42_gsm8k_mistral/0.3', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.3, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.001, 'st_linear_epochs': 1, 'st_llm_lr': 1e-07, 'st_llm_wd': 1e-05, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'openai/gsm8k', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-17 12:49:34,801 [INFO] Logging to ./logs/effi_cot_vanilla_42_gsm8k_mistral/0.3
2025-05-17 12:49:34,802 [INFO] Training sentence transformer
2025-05-17 12:53:13,540 [INFO] Step 0 - train_loss: 0.9290, val_loss: 0.8627
2025-05-17 12:53:37,694 [INFO] Loading best validation loss = 0.8627083823084831
2025-05-17 12:54:55,438 [INFO] Step 0 - train_loss: 0.8373, val_loss: 0.8384
2025-05-17 12:55:21,290 [INFO] Loading best validation loss = 0.8383713409304618
2025-05-17 12:55:45,010 [INFO] Logging to ./logs/effi_cot_vanilla_42_gsm8k_mistral/0.3
2025-05-17 12:55:45,011 [INFO] Training contemplation generator with variation: vanilla
2025-05-17 13:01:16,255 [INFO] Step 0 - total_loss: 1.5413, reason_loss: 0.8415, ans_loss: 1.8412, eval_loss: 1.2893
2025-05-17 13:05:40,806 [INFO] Step 1 - total_loss: 1.2161, reason_loss: 0.3921, ans_loss: 1.5693, eval_loss: 1.2296
2025-05-17 13:10:05,524 [INFO] Step 2 - total_loss: 1.1138, reason_loss: 0.2902, ans_loss: 1.4668, eval_loss: 1.2017
2025-05-17 13:10:16,970 [INFO] Loading best validation loss = 1.2016736743599177
2025-05-17 13:14:38,421 [INFO] Step 0 - total_loss: 1.0080, reason_loss: 0.2750, ans_loss: 1.3221, eval_loss: 1.2012
2025-05-17 13:19:04,000 [INFO] Step 1 - total_loss: 1.0074, reason_loss: 0.2746, ans_loss: 1.3215, eval_loss: 1.2008
2025-05-17 13:19:12,098 [INFO] Loading best validation loss = 1.200809382945299
2025-05-17 13:50:03,992 [INFO] Logging to ./logs/effi_cot_vanilla_42_gsm8k_mistral/0.3
2025-05-17 13:50:03,992 [INFO] Training sentence transformer
2025-05-17 13:53:48,258 [INFO] Step 0 - train_loss: 0.9361, val_loss: 0.8388
2025-05-17 13:54:12,002 [INFO] Loading best validation loss = 0.838770279288292
2025-05-17 13:55:29,714 [INFO] Step 0 - train_loss: 0.8220, val_loss: 0.8357
2025-05-17 13:55:53,537 [INFO] Loading best validation loss = 0.8356917217373848
2025-05-17 13:56:12,320 [INFO] Logging to ./logs/effi_cot_vanilla_42_gsm8k_mistral/0.3
2025-05-17 13:56:12,320 [INFO] Training contemplation generator with variation: vanilla
2025-05-17 14:01:41,892 [INFO] Step 0 - total_loss: 1.6075, reason_loss: 0.6798, ans_loss: 2.0051, eval_loss: 1.3346
2025-05-17 14:06:08,024 [INFO] Step 1 - total_loss: 1.2066, reason_loss: 0.3574, ans_loss: 1.5705, eval_loss: 1.2259
2025-05-17 14:10:34,258 [INFO] Step 2 - total_loss: 1.1019, reason_loss: 0.2738, ans_loss: 1.4569, eval_loss: 1.2200
2025-05-17 14:10:45,250 [INFO] Loading best validation loss = 1.220046627521515
2025-05-17 14:15:07,590 [INFO] Step 0 - total_loss: 0.9915, reason_loss: 0.2671, ans_loss: 1.3019, eval_loss: 1.2196
2025-05-17 14:19:32,874 [INFO] Step 1 - total_loss: 0.9908, reason_loss: 0.2667, ans_loss: 1.3011, eval_loss: 1.2192
2025-05-17 14:19:43,806 [INFO] Loading best validation loss = 1.219186264425516
2025-05-17 14:50:50,615 [INFO] Logging to ./logs/effi_cot_vanilla_42_gsm8k_mistral/0.3
2025-05-17 14:50:50,616 [INFO] Training sentence transformer
2025-05-17 14:54:37,417 [INFO] Step 0 - train_loss: 0.9266, val_loss: 0.8459
2025-05-17 14:55:07,242 [INFO] Loading best validation loss = 0.845870940387249
2025-05-17 14:56:25,040 [INFO] Step 0 - train_loss: 0.8361, val_loss: 0.8299
2025-05-17 14:56:56,284 [INFO] Loading best validation loss = 0.8298535019159317
2025-05-17 14:57:22,953 [INFO] Logging to ./logs/effi_cot_vanilla_42_gsm8k_mistral/0.3
2025-05-17 14:57:22,953 [INFO] Training contemplation generator with variation: vanilla
2025-05-17 15:02:53,234 [INFO] Step 0 - total_loss: 1.5282, reason_loss: 0.6753, ans_loss: 1.8937, eval_loss: 1.3609
2025-05-17 15:07:18,205 [INFO] Step 1 - total_loss: 1.3214, reason_loss: 0.3017, ans_loss: 1.7585, eval_loss: 1.3278
2025-05-17 15:11:43,465 [INFO] Step 2 - total_loss: 1.2572, reason_loss: 0.2310, ans_loss: 1.6970, eval_loss: 1.2410
2025-05-17 15:11:55,242 [INFO] Loading best validation loss = 1.2410431596636773
2025-05-17 15:16:16,154 [INFO] Step 0 - total_loss: 1.1635, reason_loss: 0.2248, ans_loss: 1.5658, eval_loss: 1.2407
2025-05-17 15:20:40,796 [INFO] Step 1 - total_loss: 1.1632, reason_loss: 0.2246, ans_loss: 1.5654, eval_loss: 1.2403
2025-05-17 15:20:51,500 [INFO] Loading best validation loss = 1.2403095358610152
