2025-05-17 13:26:19,048 [INFO] Logging to ./logs/effi_cot_vanilla_42_gsm8k_mistral/0.9
2025-05-17 13:26:19,048 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/gsm8k/0.9', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/gsm8k/0.9', 'result_path': './results/effi_cot/vanilla/mistral/gsm8k/0.9', 'experiment_name': 'effi_cot_vanilla_42_gsm8k_mistral/0.9', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.9, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.001, 'st_linear_epochs': 1, 'st_llm_lr': 1e-07, 'st_llm_wd': 1e-05, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'openai/gsm8k', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-17 13:26:19,376 [INFO] Logging to ./logs/effi_cot_vanilla_42_gsm8k_mistral/0.9
2025-05-17 13:26:19,376 [INFO] Training sentence transformer
2025-05-17 13:29:53,151 [INFO] Step 0 - train_loss: 0.9290, val_loss: 0.8627
2025-05-17 13:30:08,674 [INFO] Loading best validation loss = 0.8627083823084831
2025-05-17 13:31:25,972 [INFO] Step 0 - train_loss: 0.8373, val_loss: 0.8384
2025-05-17 13:31:41,944 [INFO] Loading best validation loss = 0.8383713409304618
2025-05-17 13:31:54,976 [INFO] Logging to ./logs/effi_cot_vanilla_42_gsm8k_mistral/0.9
2025-05-17 13:31:54,976 [INFO] Training contemplation generator with variation: vanilla
2025-05-17 13:37:16,947 [INFO] Step 0 - total_loss: 1.0656, reason_loss: 0.9721, ans_loss: 1.9074, eval_loss: 0.9776
2025-05-17 13:41:40,970 [INFO] Step 1 - total_loss: 0.5951, reason_loss: 0.4748, ans_loss: 1.6776, eval_loss: 0.4194
2025-05-17 13:46:05,316 [INFO] Step 2 - total_loss: 0.3550, reason_loss: 0.2203, ans_loss: 1.5672, eval_loss: 0.4118
2025-05-17 13:46:13,366 [INFO] Loading best validation loss = 0.4118154663965106
2025-05-17 13:50:34,802 [INFO] Step 0 - total_loss: 0.3466, reason_loss: 0.2221, ans_loss: 1.4677, eval_loss: 0.4109
2025-05-17 13:54:59,264 [INFO] Step 1 - total_loss: 0.3457, reason_loss: 0.2211, ans_loss: 1.4672, eval_loss: 0.4100
2025-05-17 13:55:07,377 [INFO] Loading best validation loss = 0.40999519065022466
2025-05-17 14:25:58,591 [INFO] Logging to ./logs/effi_cot_vanilla_42_gsm8k_mistral/0.9
2025-05-17 14:25:58,591 [INFO] Training sentence transformer
2025-05-17 14:29:29,865 [INFO] Step 0 - train_loss: 0.9361, val_loss: 0.8388
2025-05-17 14:29:44,116 [INFO] Loading best validation loss = 0.838770279288292
2025-05-17 14:31:01,484 [INFO] Step 0 - train_loss: 0.8220, val_loss: 0.8357
2025-05-17 14:31:16,315 [INFO] Loading best validation loss = 0.8356917217373848
2025-05-17 14:31:28,841 [INFO] Logging to ./logs/effi_cot_vanilla_42_gsm8k_mistral/0.9
2025-05-17 14:31:28,842 [INFO] Training contemplation generator with variation: vanilla
2025-05-17 14:36:50,689 [INFO] Step 0 - total_loss: 1.0845, reason_loss: 0.9674, ans_loss: 2.1385, eval_loss: 0.9464
2025-05-17 14:41:15,048 [INFO] Step 1 - total_loss: 0.6408, reason_loss: 0.5263, ans_loss: 1.6711, eval_loss: 0.4290
2025-05-17 14:45:38,618 [INFO] Step 2 - total_loss: 0.3438, reason_loss: 0.2099, ans_loss: 1.5490, eval_loss: 0.3676
2025-05-17 14:45:46,605 [INFO] Loading best validation loss = 0.36757924273610115
2025-05-17 14:50:07,032 [INFO] Step 0 - total_loss: 0.3040, reason_loss: 0.1810, ans_loss: 1.4109, eval_loss: 0.3671
2025-05-17 14:54:30,612 [INFO] Step 1 - total_loss: 0.3033, reason_loss: 0.1803, ans_loss: 1.4103, eval_loss: 0.3666
2025-05-17 14:54:38,513 [INFO] Loading best validation loss = 0.3666175878047943
2025-05-17 15:25:26,215 [INFO] Logging to ./logs/effi_cot_vanilla_42_gsm8k_mistral/0.9
2025-05-17 15:25:26,215 [INFO] Training sentence transformer
2025-05-17 15:28:57,400 [INFO] Step 0 - train_loss: 0.9266, val_loss: 0.8459
2025-05-17 15:29:11,536 [INFO] Loading best validation loss = 0.845870940387249
2025-05-17 15:30:28,857 [INFO] Step 0 - train_loss: 0.8361, val_loss: 0.8299
2025-05-17 15:30:43,448 [INFO] Loading best validation loss = 0.8298535019159317
2025-05-17 15:30:55,425 [INFO] Logging to ./logs/effi_cot_vanilla_42_gsm8k_mistral/0.9
2025-05-17 15:30:55,425 [INFO] Training contemplation generator with variation: vanilla
2025-05-17 15:36:17,747 [INFO] Step 0 - total_loss: 1.0347, reason_loss: 0.9358, ans_loss: 1.9247, eval_loss: 0.9546
2025-05-17 15:40:41,605 [INFO] Step 1 - total_loss: 0.6882, reason_loss: 0.5682, ans_loss: 1.7677, eval_loss: 0.3973
2025-05-17 15:45:05,679 [INFO] Step 2 - total_loss: 0.3512, reason_loss: 0.2063, ans_loss: 1.6553, eval_loss: 0.3577
2025-05-17 15:45:13,530 [INFO] Loading best validation loss = 0.35774325162172316
2025-05-17 15:49:34,805 [INFO] Step 0 - total_loss: 0.3037, reason_loss: 0.1706, ans_loss: 1.5016, eval_loss: 0.3573
2025-05-17 15:53:59,106 [INFO] Step 1 - total_loss: 0.3033, reason_loss: 0.1702, ans_loss: 1.5012, eval_loss: 0.3569
2025-05-17 15:54:07,073 [INFO] Loading best validation loss = 0.356919267103076
