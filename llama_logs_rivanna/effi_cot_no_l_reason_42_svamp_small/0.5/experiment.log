2025-05-07 21:44:46,002 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_small/0.5
2025-05-07 21:44:46,002 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/home/nee7ne/data/nee7ne/effi_cot/saved_models/effi_cot/no_l_reason/small/svamp/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_l_reason/small/svamp/0.5', 'result_path': './results/effi_cot/no_l_reason/small/svamp/0.5', 'experiment_name': 'effi_cot_no_l_reason_42_svamp_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.001, 'st_linear_epochs': 5, 'st_llm_lr': 1e-07, 'st_llm_wd': 1e-05, 'st_llm_epochs': 2, 'cg_linear_lr': 0.0001, 'cg_linear_wd': 0.001, 'cg_linear_epochs': 5, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 1e-05, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/SVAMP', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-07 21:44:46,403 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_small/0.5
2025-05-07 21:44:46,403 [INFO] Training sentence transformer
2025-05-07 21:47:09,658 [INFO] Step 0 - train_loss: 1.0696, val_loss: 0.9881
2025-05-07 21:47:26,065 [INFO] Step 1 - train_loss: 0.9570, val_loss: 0.9603
2025-05-07 21:47:42,746 [INFO] Step 2 - train_loss: 0.9117, val_loss: 0.9260
2025-05-07 21:47:59,421 [INFO] Step 3 - train_loss: 0.8670, val_loss: 0.9239
2025-05-07 21:48:15,978 [INFO] Step 4 - train_loss: 0.8599, val_loss: 0.8969
2025-05-07 21:48:42,702 [INFO] Loading best validation loss = 0.8968526141984122
2025-05-07 21:49:26,639 [INFO] Step 0 - train_loss: 0.8398, val_loss: 0.8807
2025-05-07 21:50:12,030 [INFO] Step 1 - train_loss: 0.8442, val_loss: 0.8758
2025-05-07 21:50:30,545 [INFO] Loading best validation loss = 0.8758160063198634
2025-05-07 21:50:48,007 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_small/0.5
2025-05-07 21:50:48,007 [INFO] Training contemplation generator
2025-05-07 22:19:04,165 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_small/0.5
2025-05-07 22:19:04,165 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/home/nee7ne/data/nee7ne/effi_cot/saved_models/effi_cot/no_l_reason/small/svamp/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_l_reason/small/svamp/0.5', 'result_path': './results/effi_cot/no_l_reason/small/svamp/0.5', 'experiment_name': 'effi_cot_no_l_reason_42_svamp_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.001, 'st_linear_epochs': 5, 'st_llm_lr': 1e-07, 'st_llm_wd': 1e-05, 'st_llm_epochs': 2, 'cg_linear_lr': 0.0001, 'cg_linear_wd': 0.001, 'cg_linear_epochs': 5, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 1e-05, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/SVAMP', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-07 22:19:05,573 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_small/0.5
2025-05-07 22:19:05,573 [INFO] Training sentence transformer
2025-05-07 22:21:32,198 [INFO] Step 0 - train_loss: 1.0696, val_loss: 0.9881
2025-05-07 22:21:49,030 [INFO] Step 1 - train_loss: 0.9570, val_loss: 0.9603
2025-05-07 22:22:06,047 [INFO] Step 2 - train_loss: 0.9117, val_loss: 0.9260
2025-05-07 22:22:22,754 [INFO] Step 3 - train_loss: 0.8670, val_loss: 0.9239
2025-05-07 22:22:39,509 [INFO] Step 4 - train_loss: 0.8599, val_loss: 0.8969
2025-05-07 22:22:56,424 [INFO] Loading best validation loss = 0.8968526141984122
2025-05-07 22:23:39,745 [INFO] Step 0 - train_loss: 0.8398, val_loss: 0.8807
2025-05-07 22:24:25,801 [INFO] Step 1 - train_loss: 0.8442, val_loss: 0.8758
2025-05-07 22:24:43,221 [INFO] Loading best validation loss = 0.8758160063198634
2025-05-07 22:25:00,696 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_small/0.5
2025-05-07 22:25:00,696 [INFO] Training contemplation generator
2025-05-07 22:28:39,730 [INFO] Step 0 - total_loss: 1.8506, reason_loss: 0.0000, ans_loss: 1.8506, eval_loss: 1.7497
2025-05-07 22:31:53,374 [INFO] Step 1 - total_loss: 1.6474, reason_loss: 0.0000, ans_loss: 1.6474, eval_loss: 1.6992
2025-05-07 22:35:06,764 [INFO] Step 2 - total_loss: 1.5644, reason_loss: 0.0000, ans_loss: 1.5644, eval_loss: 1.4496
2025-05-07 22:38:20,557 [INFO] Step 3 - total_loss: 1.1872, reason_loss: 0.0000, ans_loss: 1.1872, eval_loss: 1.3191
2025-05-07 22:41:34,981 [INFO] Step 4 - total_loss: 0.9852, reason_loss: 0.0000, ans_loss: 0.9852, eval_loss: 1.2936
2025-05-07 22:41:46,563 [INFO] Loading best validation loss = 1.2936424166010694
2025-05-07 22:47:44,393 [INFO] Step 0 - total_loss: 0.8052, reason_loss: 0.0000, ans_loss: 0.8052, eval_loss: 1.2683
2025-05-07 22:53:44,533 [INFO] Step 1 - total_loss: 0.7038, reason_loss: 0.0000, ans_loss: 0.7038, eval_loss: 1.3098
2025-05-07 22:54:03,694 [INFO] Loading best validation loss = 1.2683194166235625
2025-05-08 22:51:45,597 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_small/0.5
2025-05-08 22:51:45,611 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/home/nee7ne/data/nee7ne/effi_cot/saved_models/effi_cot/no_l_reason/small/svamp/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_l_reason/small/svamp/0.5', 'result_path': './results/effi_cot/no_l_reason/small/svamp/0.5', 'experiment_name': 'effi_cot_no_l_reason_42_svamp_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.001, 'st_linear_epochs': 5, 'st_llm_lr': 1e-07, 'st_llm_wd': 1e-05, 'st_llm_epochs': 2, 'cg_linear_lr': 0.0001, 'cg_linear_wd': 0.001, 'cg_linear_epochs': 5, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 1e-05, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/SVAMP', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-08 22:51:46,389 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_small/0.5
2025-05-08 22:51:46,389 [INFO] Training sentence transformer
2025-05-08 22:54:49,554 [INFO] Step 0 - train_loss: 1.0696, val_loss: 0.9881
2025-05-08 22:55:06,680 [INFO] Step 1 - train_loss: 0.9570, val_loss: 0.9603
2025-05-08 22:55:23,781 [INFO] Step 2 - train_loss: 0.9117, val_loss: 0.9260
2025-05-08 22:55:41,186 [INFO] Step 3 - train_loss: 0.8670, val_loss: 0.9239
2025-05-08 22:55:57,914 [INFO] Step 4 - train_loss: 0.8599, val_loss: 0.8969
2025-05-08 22:56:29,429 [INFO] Loading best validation loss = 0.8968526141984122
2025-05-08 22:57:14,266 [INFO] Step 0 - train_loss: 0.8398, val_loss: 0.8807
2025-05-08 22:58:00,752 [INFO] Step 1 - train_loss: 0.8442, val_loss: 0.8758
2025-05-08 22:58:19,782 [INFO] Loading best validation loss = 0.8758160063198634
2025-05-08 22:58:52,326 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_small/0.5
2025-05-08 22:58:52,326 [INFO] Training contemplation generator with variation: no_l_reason
2025-05-08 23:43:58,999 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_small/0.5
2025-05-08 23:43:59,010 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/home/nee7ne/data/nee7ne/effi_cot/saved_models/effi_cot/no_l_reason/small/svamp/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_l_reason/small/svamp/0.5', 'result_path': './results/effi_cot/no_l_reason/small/svamp/0.5', 'experiment_name': 'effi_cot_no_l_reason_42_svamp_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.001, 'st_linear_epochs': 5, 'st_llm_lr': 1e-07, 'st_llm_wd': 1e-05, 'st_llm_epochs': 2, 'cg_linear_lr': 0.0001, 'cg_linear_wd': 0.001, 'cg_linear_epochs': 5, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 1e-05, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/SVAMP', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-08 23:43:59,548 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_small/0.5
2025-05-08 23:43:59,548 [INFO] Training sentence transformer
2025-05-08 23:46:45,576 [INFO] Step 0 - train_loss: 1.0696, val_loss: 0.9881
2025-05-08 23:47:03,660 [INFO] Step 1 - train_loss: 0.9570, val_loss: 0.9603
2025-05-08 23:47:21,631 [INFO] Step 2 - train_loss: 0.9117, val_loss: 0.9260
2025-05-08 23:47:39,225 [INFO] Step 3 - train_loss: 0.8670, val_loss: 0.9239
2025-05-08 23:47:57,039 [INFO] Step 4 - train_loss: 0.8599, val_loss: 0.8969
2025-05-08 23:48:20,552 [INFO] Loading best validation loss = 0.8968526141984122
2025-05-08 23:49:04,222 [INFO] Step 0 - train_loss: 0.8398, val_loss: 0.8807
2025-05-08 23:49:55,393 [INFO] Step 1 - train_loss: 0.8442, val_loss: 0.8758
2025-05-08 23:50:17,390 [INFO] Loading best validation loss = 0.8758160063198634
2025-05-08 23:50:49,637 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_small/0.5
2025-05-08 23:50:49,637 [INFO] Training contemplation generator with variation: no_l_reason
2025-05-08 23:54:48,855 [INFO] Step 0 - total_loss: 1.8506, reason_loss: 0.0000, ans_loss: 1.8506, eval_loss: 1.7497
2025-05-08 23:58:12,624 [INFO] Step 1 - total_loss: 1.6474, reason_loss: 0.0000, ans_loss: 1.6474, eval_loss: 1.6992
2025-05-09 00:01:37,224 [INFO] Step 2 - total_loss: 1.5644, reason_loss: 0.0000, ans_loss: 1.5644, eval_loss: 1.4496
2025-05-09 00:04:57,364 [INFO] Step 3 - total_loss: 1.1872, reason_loss: 0.0000, ans_loss: 1.1872, eval_loss: 1.3191
2025-05-09 00:08:25,860 [INFO] Step 4 - total_loss: 0.9852, reason_loss: 0.0000, ans_loss: 0.9852, eval_loss: 1.2936
2025-05-09 00:09:09,992 [INFO] Loading best validation loss = 1.2936424166010694
2025-05-09 13:48:54,026 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_small/0.5
2025-05-09 13:48:54,035 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/no_l_reason/small/svamp/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_l_reason/small/svamp/0.5', 'result_path': './results/effi_cot/no_l_reason/small/svamp/0.5', 'experiment_name': 'effi_cot_no_l_reason_42_svamp_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.001, 'st_linear_epochs': 5, 'st_llm_lr': 1e-07, 'st_llm_wd': 1e-05, 'st_llm_epochs': 2, 'cg_linear_lr': 0.0001, 'cg_linear_wd': 0.001, 'cg_linear_epochs': 5, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 1e-05, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/SVAMP', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-09 13:48:54,500 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_small/0.5
2025-05-09 13:48:54,500 [INFO] Training sentence transformer
2025-05-09 13:51:57,338 [INFO] Step 0 - train_loss: 1.0696, val_loss: 0.9881
2025-05-09 13:52:20,823 [INFO] Step 1 - train_loss: 0.9570, val_loss: 0.9603
2025-05-09 13:52:39,090 [INFO] Step 2 - train_loss: 0.9117, val_loss: 0.9260
2025-05-09 13:53:04,599 [INFO] Step 3 - train_loss: 0.8670, val_loss: 0.9239
2025-05-09 13:53:29,644 [INFO] Step 4 - train_loss: 0.8599, val_loss: 0.8969
2025-05-09 13:53:57,022 [INFO] Loading best validation loss = 0.8968526141984122
2025-05-09 13:54:41,940 [INFO] Step 0 - train_loss: 0.8398, val_loss: 0.8807
2025-05-09 13:55:30,694 [INFO] Step 1 - train_loss: 0.8442, val_loss: 0.8758
2025-05-09 13:55:46,218 [INFO] Loading best validation loss = 0.8758160063198634
2025-05-09 13:56:00,001 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_small/0.5
2025-05-09 13:56:00,001 [INFO] Training contemplation generator with variation: no_l_reason
2025-05-09 13:59:56,247 [INFO] Step 0 - total_loss: 1.8506, reason_loss: 0.0000, ans_loss: 1.8506, eval_loss: 1.7497
2025-05-09 14:03:19,655 [INFO] Step 1 - total_loss: 1.6474, reason_loss: 0.0000, ans_loss: 1.6474, eval_loss: 1.6992
2025-05-09 14:06:41,251 [INFO] Step 2 - total_loss: 1.5644, reason_loss: 0.0000, ans_loss: 1.5644, eval_loss: 1.4496
2025-05-09 14:10:05,334 [INFO] Step 3 - total_loss: 1.1872, reason_loss: 0.0000, ans_loss: 1.1872, eval_loss: 1.3191
2025-05-09 14:13:24,102 [INFO] Step 4 - total_loss: 0.9852, reason_loss: 0.0000, ans_loss: 0.9852, eval_loss: 1.2936
2025-05-09 14:13:34,030 [INFO] Loading best validation loss = 1.2936424166010694
2025-05-09 15:54:59,138 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_small/0.5
2025-05-09 15:54:59,201 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/no_l_reason/small/svamp/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_l_reason/small/svamp/0.5', 'result_path': './results/effi_cot/no_l_reason/small/svamp/0.5', 'experiment_name': 'effi_cot_no_l_reason_42_svamp_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.001, 'st_linear_epochs': 5, 'st_llm_lr': 1e-07, 'st_llm_wd': 1e-05, 'st_llm_epochs': 2, 'cg_linear_lr': 0.0001, 'cg_linear_wd': 0.001, 'cg_linear_epochs': 5, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 1e-05, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/SVAMP', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-09 15:54:59,681 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_small/0.5
2025-05-09 15:54:59,681 [INFO] Training sentence transformer
2025-05-09 15:57:17,369 [INFO] Step 0 - train_loss: 1.0696, val_loss: 0.9881
2025-05-09 15:57:35,428 [INFO] Step 1 - train_loss: 0.9570, val_loss: 0.9603
2025-05-09 15:57:53,706 [INFO] Step 2 - train_loss: 0.9117, val_loss: 0.9260
2025-05-09 15:58:12,032 [INFO] Step 3 - train_loss: 0.8670, val_loss: 0.9239
2025-05-09 15:58:30,354 [INFO] Step 4 - train_loss: 0.8599, val_loss: 0.8969
2025-05-09 15:58:45,876 [INFO] Loading best validation loss = 0.8968526141984122
2025-05-09 15:59:29,560 [INFO] Step 0 - train_loss: 0.8398, val_loss: 0.8807
2025-05-09 16:00:17,072 [INFO] Step 1 - train_loss: 0.8442, val_loss: 0.8758
2025-05-09 16:00:32,588 [INFO] Loading best validation loss = 0.8758160063198634
2025-05-09 16:00:47,482 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_small/0.5
2025-05-09 16:00:47,482 [INFO] Training contemplation generator with variation: no_l_reason
2025-05-09 16:04:22,827 [INFO] Step 0 - total_loss: 1.8506, reason_loss: 0.0000, ans_loss: 1.8506, eval_loss: 1.7497
2025-05-09 16:07:35,874 [INFO] Step 1 - total_loss: 1.6474, reason_loss: 0.0000, ans_loss: 1.6474, eval_loss: 1.6992
2025-05-09 16:10:48,821 [INFO] Step 2 - total_loss: 1.5644, reason_loss: 0.0000, ans_loss: 1.5644, eval_loss: 1.4496
2025-05-09 16:14:02,039 [INFO] Step 3 - total_loss: 1.1872, reason_loss: 0.0000, ans_loss: 1.1872, eval_loss: 1.3191
2025-05-09 16:17:15,147 [INFO] Step 4 - total_loss: 0.9852, reason_loss: 0.0000, ans_loss: 0.9852, eval_loss: 1.2936
2025-05-09 16:17:25,880 [INFO] Loading best validation loss = 1.2936424166010694
2025-05-09 16:20:36,433 [INFO] Step 0 - total_loss: 0.8895, reason_loss: 0.0000, ans_loss: 0.8895, eval_loss: 1.2804
2025-05-09 16:23:50,766 [INFO] Step 1 - total_loss: 0.8715, reason_loss: 0.0000, ans_loss: 0.8715, eval_loss: 1.2703
2025-05-09 16:24:01,300 [INFO] Loading best validation loss = 1.2703177446546032
2025-05-09 16:53:38,877 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_small/0.5
2025-05-09 16:53:38,878 [INFO] Training sentence transformer
2025-05-09 16:55:45,545 [INFO] Step 0 - train_loss: 1.0643, val_loss: 1.0163
2025-05-09 16:56:03,759 [INFO] Step 1 - train_loss: 0.9306, val_loss: 0.9350
2025-05-09 16:56:21,910 [INFO] Step 2 - train_loss: 0.8983, val_loss: 0.9160
2025-05-09 16:56:40,014 [INFO] Step 3 - train_loss: 0.8803, val_loss: 0.8990
2025-05-09 16:56:58,195 [INFO] Step 4 - train_loss: 0.8871, val_loss: 0.8994
2025-05-09 16:57:09,380 [INFO] Loading best validation loss = 0.8989986624036517
2025-05-09 16:57:52,986 [INFO] Step 0 - train_loss: 0.8494, val_loss: 0.8854
2025-05-09 16:58:40,370 [INFO] Step 1 - train_loss: 0.8487, val_loss: 0.8793
2025-05-09 16:58:54,853 [INFO] Loading best validation loss = 0.8792606064251491
2025-05-09 16:59:07,030 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_small/0.5
2025-05-09 16:59:07,030 [INFO] Training contemplation generator with variation: no_l_reason
2025-05-09 17:02:42,188 [INFO] Step 0 - total_loss: 1.8145, reason_loss: 0.0000, ans_loss: 1.8145, eval_loss: 1.7235
2025-05-09 17:05:56,282 [INFO] Step 1 - total_loss: 1.6595, reason_loss: 0.0000, ans_loss: 1.6595, eval_loss: 1.7351
2025-05-09 17:09:06,558 [INFO] Step 2 - total_loss: 1.5189, reason_loss: 0.0000, ans_loss: 1.5189, eval_loss: 1.5467
2025-05-09 17:12:20,655 [INFO] Step 3 - total_loss: 1.2062, reason_loss: 0.0000, ans_loss: 1.2062, eval_loss: 1.3778
2025-05-09 17:15:34,778 [INFO] Step 4 - total_loss: 1.0393, reason_loss: 0.0000, ans_loss: 1.0393, eval_loss: 1.3099
2025-05-09 17:15:44,684 [INFO] Loading best validation loss = 1.3098540552856865
2025-05-09 17:18:57,358 [INFO] Step 0 - total_loss: 0.8249, reason_loss: 0.0000, ans_loss: 0.8249, eval_loss: 1.3077
2025-05-09 17:22:13,941 [INFO] Step 1 - total_loss: 0.8201, reason_loss: 0.0000, ans_loss: 0.8201, eval_loss: 1.3063
2025-05-09 17:22:23,975 [INFO] Loading best validation loss = 1.3062727116944735
2025-05-09 17:52:06,041 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_small/0.5
2025-05-09 17:52:06,042 [INFO] Training sentence transformer
2025-05-09 17:54:13,086 [INFO] Step 0 - train_loss: 1.0915, val_loss: 1.0285
2025-05-09 17:54:31,234 [INFO] Step 1 - train_loss: 0.9528, val_loss: 0.9395
2025-05-09 17:54:49,421 [INFO] Step 2 - train_loss: 0.9092, val_loss: 0.9046
2025-05-09 17:55:07,549 [INFO] Step 3 - train_loss: 0.9026, val_loss: 0.8927
2025-05-09 17:55:25,683 [INFO] Step 4 - train_loss: 0.8749, val_loss: 0.9125
2025-05-09 17:55:36,291 [INFO] Loading best validation loss = 0.8927352070808411
2025-05-09 17:56:19,959 [INFO] Step 0 - train_loss: 0.8486, val_loss: 0.8878
2025-05-09 17:57:07,264 [INFO] Step 1 - train_loss: 0.8360, val_loss: 0.8852
2025-05-09 17:57:21,456 [INFO] Loading best validation loss = 0.8852230855396815
2025-05-09 17:57:33,800 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_small/0.5
2025-05-09 17:57:33,800 [INFO] Training contemplation generator with variation: no_l_reason
2025-05-09 18:01:11,732 [INFO] Step 0 - total_loss: 1.5472, reason_loss: 0.0000, ans_loss: 1.5472, eval_loss: 1.7988
2025-05-09 18:04:28,282 [INFO] Step 1 - total_loss: 1.2507, reason_loss: 0.0000, ans_loss: 1.2507, eval_loss: 1.3296
2025-05-09 18:07:45,064 [INFO] Step 2 - total_loss: 1.0434, reason_loss: 0.0000, ans_loss: 1.0434, eval_loss: 1.2541
2025-05-09 18:11:01,684 [INFO] Step 3 - total_loss: 0.8880, reason_loss: 0.0000, ans_loss: 0.8880, eval_loss: 1.2506
2025-05-09 18:14:18,429 [INFO] Step 4 - total_loss: 0.7925, reason_loss: 0.0000, ans_loss: 0.7925, eval_loss: 1.2075
2025-05-09 18:14:28,314 [INFO] Loading best validation loss = 1.207483005678514
2025-05-09 18:17:41,865 [INFO] Step 0 - total_loss: 0.5940, reason_loss: 0.0000, ans_loss: 0.5940, eval_loss: 1.2029
2025-05-09 18:20:59,497 [INFO] Step 1 - total_loss: 0.5853, reason_loss: 0.0000, ans_loss: 0.5853, eval_loss: 1.2017
2025-05-09 18:21:09,662 [INFO] Loading best validation loss = 1.2016911123378669
