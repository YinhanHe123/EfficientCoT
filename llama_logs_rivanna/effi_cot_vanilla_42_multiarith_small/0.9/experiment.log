2025-05-16 23:46:37,773 [INFO] Logging to ./logs/effi_cot_vanilla_42_multiarith_small/0.9
2025-05-16 23:46:37,773 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/small/multiarith/0.9', 'checkpoint_path': './checkpoints/effi_cot/vanilla/small/multiarith/0.9', 'result_path': './results/effi_cot/vanilla/small/multiarith/0.9', 'experiment_name': 'effi_cot_vanilla_42_multiarith_small/0.9', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.9, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.001, 'st_linear_epochs': 5, 'st_llm_lr': 1e-07, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.001, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-05, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/MultiArith', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-16 23:46:37,810 [INFO] Logging to ./logs/effi_cot_vanilla_42_multiarith_small/0.9
2025-05-16 23:46:37,810 [INFO] Training sentence transformer
2025-05-16 23:49:46,697 [INFO] Step 0 - train_loss: 1.0177, val_loss: 0.9310
2025-05-16 23:50:09,337 [INFO] Step 1 - train_loss: 0.8672, val_loss: 0.8763
2025-05-16 23:50:32,309 [INFO] Step 2 - train_loss: 0.8490, val_loss: 0.8764
2025-05-16 23:50:51,384 [INFO] Step 3 - train_loss: 0.8291, val_loss: 0.8607
2025-05-16 23:51:14,295 [INFO] Step 4 - train_loss: 0.8128, val_loss: 0.8509
2025-05-16 23:51:40,220 [INFO] Loading best validation loss = 0.8508716679754711
2025-05-16 23:52:33,543 [INFO] Step 0 - train_loss: 0.7951, val_loss: 0.8397
2025-05-16 23:52:54,799 [INFO] Loading best validation loss = 0.8397188498860314
2025-05-16 23:53:20,393 [INFO] Logging to ./logs/effi_cot_vanilla_42_multiarith_small/0.9
2025-05-16 23:53:20,393 [INFO] Training contemplation generator with variation: vanilla
2025-05-16 23:57:03,174 [INFO] Step 0 - total_loss: 0.8013, reason_loss: 0.6971, ans_loss: 1.7387, eval_loss: 0.4890
2025-05-16 23:59:22,289 [INFO] Step 1 - total_loss: 0.3686, reason_loss: 0.2492, ans_loss: 1.4427, eval_loss: 0.3787
2025-05-17 00:01:42,041 [INFO] Step 2 - total_loss: 0.2809, reason_loss: 0.1629, ans_loss: 1.3426, eval_loss: 0.3041
2025-05-17 00:01:54,846 [INFO] Loading best validation loss = 0.30408933709065117
2025-05-17 00:04:10,982 [INFO] Step 0 - total_loss: 0.2140, reason_loss: 0.1036, ans_loss: 1.2077, eval_loss: 0.2493
2025-05-17 00:06:31,654 [INFO] Step 1 - total_loss: 0.2014, reason_loss: 0.0972, ans_loss: 1.1392, eval_loss: 0.2459
2025-05-17 00:06:45,977 [INFO] Loading best validation loss = 0.245910048029489
2025-05-17 00:34:12,841 [INFO] Logging to ./logs/effi_cot_vanilla_42_multiarith_small/0.9
2025-05-17 00:34:12,841 [INFO] Training sentence transformer
2025-05-17 00:36:56,836 [INFO] Step 0 - train_loss: 1.0057, val_loss: 0.9259
2025-05-17 00:37:19,081 [INFO] Step 1 - train_loss: 0.8799, val_loss: 0.8792
2025-05-17 00:37:41,702 [INFO] Step 2 - train_loss: 0.8461, val_loss: 0.8645
2025-05-17 00:38:04,489 [INFO] Step 3 - train_loss: 0.8353, val_loss: 0.8520
2025-05-17 00:38:27,428 [INFO] Step 4 - train_loss: 0.8081, val_loss: 0.8517
2025-05-17 00:38:42,384 [INFO] Loading best validation loss = 0.8517278631528219
2025-05-17 00:39:36,575 [INFO] Step 0 - train_loss: 0.8140, val_loss: 0.8347
2025-05-17 00:39:54,439 [INFO] Loading best validation loss = 0.834709556329818
2025-05-17 00:40:09,309 [INFO] Logging to ./logs/effi_cot_vanilla_42_multiarith_small/0.9
2025-05-17 00:40:09,309 [INFO] Training contemplation generator with variation: vanilla
2025-05-17 00:43:42,988 [INFO] Step 0 - total_loss: 0.7858, reason_loss: 0.6691, ans_loss: 1.8358, eval_loss: 0.4721
2025-05-17 00:46:02,215 [INFO] Step 1 - total_loss: 0.3765, reason_loss: 0.2535, ans_loss: 1.4842, eval_loss: 0.3307
2025-05-17 00:48:22,997 [INFO] Step 2 - total_loss: 0.2856, reason_loss: 0.1648, ans_loss: 1.3725, eval_loss: 0.2907
2025-05-17 00:48:34,908 [INFO] Loading best validation loss = 0.29068618565797805
2025-05-17 00:50:55,108 [INFO] Step 0 - total_loss: 0.2165, reason_loss: 0.1057, ans_loss: 1.2139, eval_loss: 0.2506
2025-05-17 00:53:15,853 [INFO] Step 1 - total_loss: 0.2060, reason_loss: 0.0987, ans_loss: 1.1716, eval_loss: 0.2477
2025-05-17 00:53:27,229 [INFO] Loading best validation loss = 0.24771787168251144
2025-05-17 01:20:44,977 [INFO] Logging to ./logs/effi_cot_vanilla_42_multiarith_small/0.9
2025-05-17 01:20:44,977 [INFO] Training sentence transformer
2025-05-17 01:23:29,128 [INFO] Step 0 - train_loss: 0.9927, val_loss: 0.9115
2025-05-17 01:23:51,182 [INFO] Step 1 - train_loss: 0.8700, val_loss: 0.9108
2025-05-17 01:24:13,965 [INFO] Step 2 - train_loss: 0.8362, val_loss: 0.8705
2025-05-17 01:24:36,158 [INFO] Step 3 - train_loss: 0.8185, val_loss: 0.8579
2025-05-17 01:24:58,388 [INFO] Step 4 - train_loss: 0.8170, val_loss: 0.8690
2025-05-17 01:25:12,350 [INFO] Loading best validation loss = 0.8579295760109311
2025-05-17 01:26:06,470 [INFO] Step 0 - train_loss: 0.8175, val_loss: 0.8505
2025-05-17 01:26:21,250 [INFO] Loading best validation loss = 0.8504539274034046
2025-05-17 01:26:35,754 [INFO] Logging to ./logs/effi_cot_vanilla_42_multiarith_small/0.9
2025-05-17 01:26:35,754 [INFO] Training contemplation generator with variation: vanilla
2025-05-17 01:30:05,055 [INFO] Step 0 - total_loss: 0.7640, reason_loss: 0.6513, ans_loss: 1.7789, eval_loss: 0.4223
2025-05-17 01:32:23,558 [INFO] Step 1 - total_loss: 0.3690, reason_loss: 0.2401, ans_loss: 1.5285, eval_loss: 0.3784
2025-05-17 01:34:42,932 [INFO] Step 2 - total_loss: 0.2811, reason_loss: 0.1532, ans_loss: 1.4320, eval_loss: 0.3107
2025-05-17 01:34:54,420 [INFO] Loading best validation loss = 0.31066569478975403
2025-05-17 01:37:08,459 [INFO] Step 0 - total_loss: 0.2252, reason_loss: 0.1042, ans_loss: 1.3138, eval_loss: 0.2533
2025-05-17 01:39:30,971 [INFO] Step 1 - total_loss: 0.2154, reason_loss: 0.0968, ans_loss: 1.2829, eval_loss: 0.2513
2025-05-17 01:39:43,140 [INFO] Loading best validation loss = 0.2512837801542547
