2025-05-17 08:00:22,061 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_mistral/0.25
2025-05-17 08:00:22,061 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/no_l_reason/mistral/svamp/0.25', 'checkpoint_path': './checkpoints/effi_cot/no_l_reason/mistral/svamp/0.25', 'result_path': './results/effi_cot/no_l_reason/mistral/svamp/0.25', 'experiment_name': 'effi_cot_no_l_reason_42_svamp_mistral/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.01, 'st_linear_wd': 0.0001, 'st_linear_epochs': 3, 'st_llm_lr': 1e-05, 'st_llm_wd': 1e-05, 'st_llm_epochs': 2, 'cg_linear_lr': 0.0001, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-05, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 1, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/SVAMP', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-17 08:00:22,981 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_mistral/0.25
2025-05-17 08:00:22,981 [INFO] Training sentence transformer
2025-05-17 08:03:07,913 [INFO] Step 0 - train_loss: 1.0422, val_loss: 0.9674
2025-05-17 08:03:26,642 [INFO] Step 1 - train_loss: 0.9531, val_loss: 0.9429
2025-05-17 08:03:45,881 [INFO] Step 2 - train_loss: 0.9113, val_loss: 0.8961
2025-05-17 08:04:12,832 [INFO] Loading best validation loss = 0.8960726090839931
2025-05-17 08:05:00,162 [INFO] Step 0 - train_loss: 1.0964, val_loss: 0.9125
2025-05-17 08:05:51,359 [INFO] Step 1 - train_loss: 0.9476, val_loss: 0.8541
2025-05-17 08:06:17,899 [INFO] Loading best validation loss = 0.8540690268789018
2025-05-17 08:06:41,760 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_mistral/0.25
2025-05-17 08:06:41,760 [INFO] Training contemplation generator with variation: no_l_reason
2025-05-17 08:10:38,673 [INFO] Step 0 - total_loss: 1.3337, reason_loss: 0.0000, ans_loss: 1.3337, eval_loss: 1.1743
2025-05-17 08:14:00,133 [INFO] Step 1 - total_loss: 0.8701, reason_loss: 0.0000, ans_loss: 0.8701, eval_loss: 1.1574
2025-05-17 08:17:22,902 [INFO] Step 2 - total_loss: 0.7169, reason_loss: 0.0000, ans_loss: 0.7169, eval_loss: 1.0840
2025-05-17 08:17:31,628 [INFO] Loading best validation loss = 1.0840492527768948
2025-05-17 08:20:50,653 [INFO] Step 0 - total_loss: 0.5063, reason_loss: 0.0000, ans_loss: 0.5063, eval_loss: 1.0764
2025-05-17 08:21:00,256 [INFO] Loading best validation loss = 1.076415436890238
2025-05-17 08:53:24,320 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_mistral/0.25
2025-05-17 08:53:24,321 [INFO] Training sentence transformer
2025-05-17 08:55:46,718 [INFO] Step 0 - train_loss: 1.0216, val_loss: 0.9859
2025-05-17 08:56:05,701 [INFO] Step 1 - train_loss: 0.9157, val_loss: 0.9505
2025-05-17 08:56:24,899 [INFO] Step 2 - train_loss: 0.8732, val_loss: 0.9202
2025-05-17 08:56:46,839 [INFO] Loading best validation loss = 0.92023161309106
2025-05-17 08:57:35,336 [INFO] Step 0 - train_loss: 0.9627, val_loss: 0.9631
2025-05-17 08:58:26,265 [INFO] Step 1 - train_loss: 0.8899, val_loss: 0.8717
2025-05-17 08:58:47,796 [INFO] Loading best validation loss = 0.871707216330937
2025-05-17 08:59:07,503 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_mistral/0.25
2025-05-17 08:59:07,503 [INFO] Training contemplation generator with variation: no_l_reason
2025-05-17 09:02:56,689 [INFO] Step 0 - total_loss: 1.8375, reason_loss: 0.0000, ans_loss: 1.8375, eval_loss: 1.7763
2025-05-17 09:06:18,366 [INFO] Step 1 - total_loss: 1.2999, reason_loss: 0.0000, ans_loss: 1.2999, eval_loss: 1.3556
2025-05-17 09:09:40,012 [INFO] Step 2 - total_loss: 0.9964, reason_loss: 0.0000, ans_loss: 0.9964, eval_loss: 1.1743
2025-05-17 09:09:48,737 [INFO] Loading best validation loss = 1.174318395187147
2025-05-17 09:13:11,431 [INFO] Step 0 - total_loss: 0.7237, reason_loss: 0.0000, ans_loss: 0.7237, eval_loss: 1.1254
2025-05-17 09:13:23,202 [INFO] Loading best validation loss = 1.1253950191108744
2025-05-17 09:45:47,350 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_mistral/0.25
2025-05-17 09:45:47,350 [INFO] Training sentence transformer
2025-05-17 09:48:18,525 [INFO] Step 0 - train_loss: 1.0299, val_loss: 1.0326
2025-05-17 09:48:37,526 [INFO] Step 1 - train_loss: 0.9307, val_loss: 0.9287
2025-05-17 09:48:56,820 [INFO] Step 2 - train_loss: 0.9072, val_loss: 0.8814
2025-05-17 09:49:18,918 [INFO] Loading best validation loss = 0.8814285687037877
2025-05-17 09:50:06,390 [INFO] Step 0 - train_loss: 1.0023, val_loss: 0.9854
2025-05-17 09:50:57,722 [INFO] Step 1 - train_loss: 0.9106, val_loss: 0.9985
2025-05-17 09:51:15,657 [INFO] Loading best validation loss = 0.985399157660348
2025-05-17 09:51:34,907 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_svamp_mistral/0.25
2025-05-17 09:51:34,907 [INFO] Training contemplation generator with variation: no_l_reason
2025-05-17 09:55:26,480 [INFO] Step 0 - total_loss: 1.4506, reason_loss: 0.0000, ans_loss: 1.4506, eval_loss: 1.3358
2025-05-17 09:58:48,336 [INFO] Step 1 - total_loss: 0.9884, reason_loss: 0.0000, ans_loss: 0.9884, eval_loss: 1.2350
2025-05-17 10:02:10,842 [INFO] Step 2 - total_loss: 0.8059, reason_loss: 0.0000, ans_loss: 0.8059, eval_loss: 1.1577
2025-05-17 10:02:19,562 [INFO] Loading best validation loss = 1.157740878217446
2025-05-17 10:05:39,632 [INFO] Step 0 - total_loss: 0.5882, reason_loss: 0.0000, ans_loss: 0.5882, eval_loss: 1.1807
2025-05-17 10:05:48,449 [INFO] Loading best validation loss = 1.1807095283988747
