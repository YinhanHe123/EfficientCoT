2025-05-17 10:59:39,380 [INFO] Logging to ./logs/effi_cot_vanilla_42_coin_flip_mistral/0.25
2025-05-17 10:59:39,380 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/coin_flip/0.25', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/coin_flip/0.25', 'result_path': './results/effi_cot/vanilla/mistral/coin_flip/0.25', 'experiment_name': 'effi_cot_vanilla_42_coin_flip_mistral/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.001, 'st_linear_epochs': 5, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.001, 'cg_linear_wd': 0.001, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-05, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 1, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'skrishna/coin_flip', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-17 10:59:39,469 [INFO] Logging to ./logs/effi_cot_vanilla_42_coin_flip_mistral/0.25
2025-05-17 10:59:39,469 [INFO] Training sentence transformer
2025-05-17 11:03:40,635 [INFO] Step 0 - train_loss: 0.9699, val_loss: 0.8942
2025-05-17 11:04:09,708 [INFO] Step 1 - train_loss: 0.8860, val_loss: 0.9066
2025-05-17 11:04:35,185 [INFO] Step 2 - train_loss: 0.8694, val_loss: 0.8849
2025-05-17 11:05:04,677 [INFO] Step 3 - train_loss: 0.8738, val_loss: 0.8763
2025-05-17 11:05:34,065 [INFO] Step 4 - train_loss: 0.8564, val_loss: 0.8776
2025-05-17 11:05:56,329 [INFO] Loading best validation loss = 0.876336646080017
2025-05-17 11:07:11,922 [INFO] Step 0 - train_loss: 0.9119, val_loss: 0.8801
2025-05-17 11:07:38,530 [INFO] Loading best validation loss = 0.8801236093044281
2025-05-17 11:08:02,073 [INFO] Logging to ./logs/effi_cot_vanilla_42_coin_flip_mistral/0.25
2025-05-17 11:08:02,073 [INFO] Training contemplation generator with variation: vanilla
2025-05-17 11:13:15,962 [INFO] Step 0 - total_loss: 0.3310, reason_loss: 0.8697, ans_loss: 0.1514, eval_loss: 1.1934
2025-05-17 11:17:14,444 [INFO] Step 1 - total_loss: 0.2052, reason_loss: 0.8202, ans_loss: 0.0002, eval_loss: 1.3861
2025-05-17 11:21:09,920 [INFO] Step 2 - total_loss: 0.1831, reason_loss: 0.7321, ans_loss: 0.0001, eval_loss: 1.5910
2025-05-17 11:21:18,913 [INFO] Loading best validation loss = 1.1933979134727268
2025-05-17 11:25:15,997 [INFO] Step 0 - total_loss: 0.2062, reason_loss: 0.8241, ans_loss: 0.0003, eval_loss: 1.2991
2025-05-17 11:25:27,401 [INFO] Loading best validation loss = 1.2991097843181343
2025-05-17 11:56:08,490 [INFO] Logging to ./logs/effi_cot_vanilla_42_coin_flip_mistral/0.25
2025-05-17 11:56:08,490 [INFO] Training sentence transformer
2025-05-17 11:59:45,194 [INFO] Step 0 - train_loss: 0.9605, val_loss: 0.9208
2025-05-17 12:00:13,959 [INFO] Step 1 - train_loss: 0.8946, val_loss: 0.8774
2025-05-17 12:00:43,149 [INFO] Step 2 - train_loss: 0.8674, val_loss: 0.8881
2025-05-17 12:01:08,531 [INFO] Step 3 - train_loss: 0.8565, val_loss: 0.8698
2025-05-17 12:01:37,810 [INFO] Step 4 - train_loss: 0.8237, val_loss: 0.8638
2025-05-17 12:02:02,101 [INFO] Loading best validation loss = 0.8638151183724403
2025-05-17 12:03:17,771 [INFO] Step 0 - train_loss: 0.9402, val_loss: 0.8884
2025-05-17 12:03:42,492 [INFO] Loading best validation loss = 0.8884035184979439
2025-05-17 12:04:08,761 [INFO] Logging to ./logs/effi_cot_vanilla_42_coin_flip_mistral/0.25
2025-05-17 12:04:08,761 [INFO] Training contemplation generator with variation: vanilla
2025-05-17 12:09:21,952 [INFO] Step 0 - total_loss: 0.3168, reason_loss: 0.9540, ans_loss: 0.1043, eval_loss: 1.5412
2025-05-17 12:13:20,655 [INFO] Step 1 - total_loss: 0.1472, reason_loss: 0.5877, ans_loss: 0.0003, eval_loss: 1.6870
2025-05-17 12:17:16,079 [INFO] Step 2 - total_loss: 0.2140, reason_loss: 0.5477, ans_loss: 0.1028, eval_loss: 1.7492
2025-05-17 12:17:23,786 [INFO] Loading best validation loss = 1.5412053124606608
2025-05-17 12:21:20,380 [INFO] Step 0 - total_loss: 0.1722, reason_loss: 0.6879, ans_loss: 0.0003, eval_loss: 1.4322
2025-05-17 12:21:31,916 [INFO] Loading best validation loss = 1.4321744346432388
2025-05-17 12:52:27,754 [INFO] Logging to ./logs/effi_cot_vanilla_42_coin_flip_mistral/0.25
2025-05-17 12:52:27,755 [INFO] Training sentence transformer
2025-05-17 12:56:04,982 [INFO] Step 0 - train_loss: 0.9866, val_loss: 0.9217
2025-05-17 12:56:34,071 [INFO] Step 1 - train_loss: 0.8788, val_loss: 0.9005
2025-05-17 12:57:03,328 [INFO] Step 2 - train_loss: 0.8358, val_loss: 0.8679
2025-05-17 12:57:32,696 [INFO] Step 3 - train_loss: 0.8757, val_loss: 0.8776
2025-05-17 12:57:58,109 [INFO] Step 4 - train_loss: 0.8396, val_loss: 0.8719
2025-05-17 12:58:18,176 [INFO] Loading best validation loss = 0.8679114177823066
2025-05-17 12:59:33,797 [INFO] Step 0 - train_loss: 0.9294, val_loss: 0.8625
2025-05-17 12:59:57,585 [INFO] Loading best validation loss = 0.8624768704175949
2025-05-17 13:00:18,362 [INFO] Logging to ./logs/effi_cot_vanilla_42_coin_flip_mistral/0.25
2025-05-17 13:00:18,362 [INFO] Training contemplation generator with variation: vanilla
2025-05-17 13:05:37,381 [INFO] Step 0 - total_loss: 0.3649, reason_loss: 0.9779, ans_loss: 0.1606, eval_loss: 2.3548
2025-05-17 13:09:36,735 [INFO] Step 1 - total_loss: 0.1939, reason_loss: 0.7742, ans_loss: 0.0004, eval_loss: 2.2249
2025-05-17 13:13:37,139 [INFO] Step 2 - total_loss: 0.1086, reason_loss: 0.4337, ans_loss: 0.0002, eval_loss: 2.5070
2025-05-17 13:13:44,027 [INFO] Loading best validation loss = 2.2248678073927293
2025-05-17 13:17:38,771 [INFO] Step 0 - total_loss: 0.0675, reason_loss: 0.2692, ans_loss: 0.0003, eval_loss: 2.2475
2025-05-17 13:17:48,862 [INFO] Loading best validation loss = 2.2475494044623336
2025-05-17 18:31:55,096 [INFO] Logging to ./logs/effi_cot_vanilla_42_coin_flip_mistral/0.25
2025-05-17 18:31:55,107 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/coin_flip/0.25', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/coin_flip/0.25', 'result_path': './results/effi_cot/vanilla/mistral/coin_flip/0.25', 'experiment_name': 'effi_cot_vanilla_42_coin_flip_mistral/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 2, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.001, 'st_linear_epochs': 5, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.001, 'cg_linear_wd': 0.001, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-05, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 1, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'skrishna/coin_flip', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-17 18:32:58,501 [INFO] Logging to ./logs/effi_cot_vanilla_42_coin_flip_mistral/0.25
2025-05-17 18:32:58,502 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/coin_flip/0.25', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/coin_flip/0.25', 'result_path': './results/effi_cot/vanilla/mistral/coin_flip/0.25', 'experiment_name': 'effi_cot_vanilla_42_coin_flip_mistral/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 3, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.001, 'st_linear_epochs': 5, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.001, 'cg_linear_wd': 0.001, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-05, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 1, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'skrishna/coin_flip', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-17 18:32:58,793 [INFO] Logging to ./logs/effi_cot_vanilla_42_coin_flip_mistral/0.25
2025-05-17 18:32:58,794 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/coin_flip/0.25', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/coin_flip/0.25', 'result_path': './results/effi_cot/vanilla/mistral/coin_flip/0.25', 'experiment_name': 'effi_cot_vanilla_42_coin_flip_mistral/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 4, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.001, 'st_linear_epochs': 5, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.001, 'cg_linear_wd': 0.001, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-05, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 1, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'skrishna/coin_flip', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-17 18:38:12,836 [INFO] Logging to ./logs/effi_cot_vanilla_42_coin_flip_mistral/0.25
2025-05-17 18:38:12,837 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/coin_flip/0.25', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/coin_flip/0.25', 'result_path': './results/effi_cot/vanilla/mistral/coin_flip/0.25', 'experiment_name': 'effi_cot_vanilla_42_coin_flip_mistral/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 5, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.001, 'st_linear_epochs': 5, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.001, 'cg_linear_wd': 0.001, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-05, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 1, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'skrishna/coin_flip', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
