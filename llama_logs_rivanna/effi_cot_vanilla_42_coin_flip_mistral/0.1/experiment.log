2025-05-17 15:51:06,001 [INFO] Logging to ./logs/effi_cot_vanilla_42_coin_flip_mistral/0.1
2025-05-17 15:51:06,001 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/coin_flip/0.1', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/coin_flip/0.1', 'result_path': './results/effi_cot/vanilla/mistral/coin_flip/0.1', 'experiment_name': 'effi_cot_vanilla_42_coin_flip_mistral/0.1', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.1, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.001, 'st_linear_epochs': 5, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.001, 'cg_linear_wd': 0.001, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-05, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 1, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'skrishna/coin_flip', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-17 15:51:06,096 [INFO] Logging to ./logs/effi_cot_vanilla_42_coin_flip_mistral/0.1
2025-05-17 15:51:06,096 [INFO] Training sentence transformer
2025-05-17 15:54:31,135 [INFO] Step 0 - train_loss: 0.9699, val_loss: 0.8942
2025-05-17 15:54:59,686 [INFO] Step 1 - train_loss: 0.8860, val_loss: 0.9066
2025-05-17 15:55:24,980 [INFO] Step 2 - train_loss: 0.8694, val_loss: 0.8849
2025-05-17 15:55:53,776 [INFO] Step 3 - train_loss: 0.8738, val_loss: 0.8763
2025-05-17 15:56:22,368 [INFO] Step 4 - train_loss: 0.8564, val_loss: 0.8776
2025-05-17 15:56:34,444 [INFO] Loading best validation loss = 0.876336646080017
2025-05-17 15:57:49,791 [INFO] Step 0 - train_loss: 0.9119, val_loss: 0.8801
2025-05-17 15:58:05,124 [INFO] Loading best validation loss = 0.8801236093044281
2025-05-17 15:58:17,859 [INFO] Logging to ./logs/effi_cot_vanilla_42_coin_flip_mistral/0.1
2025-05-17 15:58:17,859 [INFO] Training contemplation generator with variation: vanilla
2025-05-17 16:03:23,733 [INFO] Step 0 - total_loss: 0.3007, reason_loss: 0.9044, ans_loss: 0.2337, eval_loss: 1.1029
2025-05-17 16:07:21,948 [INFO] Step 1 - total_loss: 0.0882, reason_loss: 0.8557, ans_loss: 0.0030, eval_loss: 1.3838
2025-05-17 16:11:16,553 [INFO] Step 2 - total_loss: 0.0836, reason_loss: 0.8342, ans_loss: 0.0002, eval_loss: 1.5199
2025-05-17 16:11:21,168 [INFO] Loading best validation loss = 1.1029167151078583
2025-05-17 16:15:16,923 [INFO] Step 0 - total_loss: 0.0914, reason_loss: 0.8638, ans_loss: 0.0056, eval_loss: 1.2761
2025-05-17 16:15:24,739 [INFO] Loading best validation loss = 1.2761334116663783
2025-05-17 16:45:31,805 [INFO] Logging to ./logs/effi_cot_vanilla_42_coin_flip_mistral/0.1
2025-05-17 16:45:31,805 [INFO] Training sentence transformer
2025-05-17 16:48:51,844 [INFO] Step 0 - train_loss: 0.9605, val_loss: 0.9208
2025-05-17 16:49:20,172 [INFO] Step 1 - train_loss: 0.8946, val_loss: 0.8774
2025-05-17 16:49:48,629 [INFO] Step 2 - train_loss: 0.8674, val_loss: 0.8881
2025-05-17 16:50:13,889 [INFO] Step 3 - train_loss: 0.8565, val_loss: 0.8698
2025-05-17 16:50:42,474 [INFO] Step 4 - train_loss: 0.8237, val_loss: 0.8638
2025-05-17 16:50:57,683 [INFO] Loading best validation loss = 0.8638151183724403
2025-05-17 16:52:13,012 [INFO] Step 0 - train_loss: 0.9402, val_loss: 0.8884
2025-05-17 16:52:28,026 [INFO] Loading best validation loss = 0.8884035184979439
2025-05-17 16:52:40,709 [INFO] Logging to ./logs/effi_cot_vanilla_42_coin_flip_mistral/0.1
2025-05-17 16:52:40,709 [INFO] Training contemplation generator with variation: vanilla
2025-05-17 16:57:45,356 [INFO] Step 0 - total_loss: 0.2793, reason_loss: 0.9860, ans_loss: 0.2008, eval_loss: 1.6035
2025-05-17 17:01:42,330 [INFO] Step 1 - total_loss: 0.0879, reason_loss: 0.8774, ans_loss: 0.0002, eval_loss: 2.0331
2025-05-17 17:05:36,272 [INFO] Step 2 - total_loss: 0.1859, reason_loss: 0.6364, ans_loss: 0.1359, eval_loss: 0.8852
2025-05-17 17:05:44,147 [INFO] Loading best validation loss = 0.885202638283372
2025-05-17 17:09:38,965 [INFO] Step 0 - total_loss: 0.2842, reason_loss: 0.8532, ans_loss: 0.2210, eval_loss: 0.8762
2025-05-17 17:09:46,697 [INFO] Loading best validation loss = 0.8761753614991903
2025-05-17 17:40:13,539 [INFO] Logging to ./logs/effi_cot_vanilla_42_coin_flip_mistral/0.1
2025-05-17 17:40:13,539 [INFO] Training sentence transformer
2025-05-17 17:43:34,107 [INFO] Step 0 - train_loss: 0.9866, val_loss: 0.9217
2025-05-17 17:44:02,475 [INFO] Step 1 - train_loss: 0.8788, val_loss: 0.9005
2025-05-17 17:44:31,049 [INFO] Step 2 - train_loss: 0.8358, val_loss: 0.8679
2025-05-17 17:44:59,658 [INFO] Step 3 - train_loss: 0.8757, val_loss: 0.8776
2025-05-17 17:45:24,907 [INFO] Step 4 - train_loss: 0.8396, val_loss: 0.8719
2025-05-17 17:45:35,977 [INFO] Loading best validation loss = 0.8679114177823066
2025-05-17 17:46:51,269 [INFO] Step 0 - train_loss: 0.9294, val_loss: 0.8625
2025-05-17 17:47:05,640 [INFO] Loading best validation loss = 0.8624768704175949
2025-05-17 17:47:17,594 [INFO] Logging to ./logs/effi_cot_vanilla_42_coin_flip_mistral/0.1
2025-05-17 17:47:17,594 [INFO] Training contemplation generator with variation: vanilla
2025-05-17 17:52:22,384 [INFO] Step 0 - total_loss: 0.2179, reason_loss: 0.9919, ans_loss: 0.1319, eval_loss: 1.8977
2025-05-17 17:56:20,155 [INFO] Step 1 - total_loss: 0.0935, reason_loss: 0.9315, ans_loss: 0.0004, eval_loss: 2.1672
2025-05-17 18:00:15,446 [INFO] Step 2 - total_loss: 0.0696, reason_loss: 0.6949, ans_loss: 0.0002, eval_loss: 2.0218
2025-05-17 18:00:19,945 [INFO] Loading best validation loss = 1.8977450827136635
2025-05-17 18:04:13,912 [INFO] Step 0 - total_loss: 0.0959, reason_loss: 0.9547, ans_loss: 0.0004, eval_loss: 2.1573
2025-05-17 18:04:21,588 [INFO] Loading best validation loss = 2.1573313412815334
