2025-05-10 05:10:08,459 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_coin_flip_small/0.25
2025-05-10 05:10:08,460 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/no_sentence_transformer/small/coin_flip/0.25', 'checkpoint_path': './checkpoints/effi_cot/no_sentence_transformer/small/coin_flip/0.25', 'result_path': './results/effi_cot/no_sentence_transformer/small/coin_flip/0.25', 'experiment_name': 'effi_cot_no_sentence_transformer_42_coin_flip_small/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.0001, 'st_linear_epochs': 3, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 1, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'skrishna/coin_flip', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-10 05:10:18,283 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_coin_flip_small/0.25
2025-05-10 05:10:18,283 [INFO] Training contemplation generator with variation: no_sentence_transformer
2025-05-10 05:16:39,167 [INFO] Step 0 - total_loss: 0.3468, reason_loss: 0.8931, ans_loss: 0.1648, eval_loss: 3.9163
2025-05-10 05:16:51,166 [INFO] Loading best validation loss = 3.9162501721084118
2025-05-10 05:20:28,391 [INFO] Step 0 - total_loss: 0.1816, reason_loss: 0.7255, ans_loss: 0.0003, eval_loss: 3.9154
2025-05-10 05:24:12,489 [INFO] Step 1 - total_loss: 0.1816, reason_loss: 0.7254, ans_loss: 0.0003, eval_loss: 3.9128
2025-05-10 05:24:24,583 [INFO] Loading best validation loss = 3.9127512557059525
2025-05-10 08:51:03,443 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_coin_flip_small/0.25
2025-05-10 08:51:03,443 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/no_sentence_transformer/small/coin_flip/0.25', 'checkpoint_path': './checkpoints/effi_cot/no_sentence_transformer/small/coin_flip/0.25', 'result_path': './results/effi_cot/no_sentence_transformer/small/coin_flip/0.25', 'experiment_name': 'effi_cot_no_sentence_transformer_42_coin_flip_small/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.0001, 'st_linear_epochs': 3, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 1, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'skrishna/coin_flip', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-10 08:51:08,773 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_coin_flip_small/0.25
2025-05-10 08:51:08,773 [INFO] Training contemplation generator with variation: no_sentence_transformer
2025-05-10 08:57:21,296 [INFO] Step 0 - total_loss: 0.3468, reason_loss: 0.8931, ans_loss: 0.1648, eval_loss: 3.9163
2025-05-10 08:57:31,294 [INFO] Loading best validation loss = 3.9162501721084118
2025-05-10 09:01:04,652 [INFO] Step 0 - total_loss: 0.1816, reason_loss: 0.7255, ans_loss: 0.0003, eval_loss: 3.9154
2025-05-10 09:04:42,362 [INFO] Step 1 - total_loss: 0.1816, reason_loss: 0.7254, ans_loss: 0.0003, eval_loss: 3.9128
2025-05-10 09:04:55,380 [INFO] Loading best validation loss = 3.9127512557059525
2025-05-10 09:34:49,561 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_coin_flip_small/0.25
2025-05-10 09:34:49,561 [INFO] Training contemplation generator with variation: no_sentence_transformer
2025-05-10 09:40:56,501 [INFO] Step 0 - total_loss: 0.5662, reason_loss: 0.9441, ans_loss: 0.4403, eval_loss: 3.6531
2025-05-10 09:41:06,269 [INFO] Loading best validation loss = 3.653092702552676
2025-05-10 09:44:41,979 [INFO] Step 0 - total_loss: 0.2165, reason_loss: 0.8428, ans_loss: 0.0077, eval_loss: 3.6537
2025-05-10 09:48:17,414 [INFO] Step 1 - total_loss: 0.2165, reason_loss: 0.8428, ans_loss: 0.0077, eval_loss: 3.6536
2025-05-10 09:48:27,655 [INFO] Loading best validation loss = 3.653622655197978
2025-05-10 10:18:15,862 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_coin_flip_small/0.25
2025-05-10 10:18:15,862 [INFO] Training contemplation generator with variation: no_sentence_transformer
2025-05-10 10:24:24,201 [INFO] Step 0 - total_loss: 0.7449, reason_loss: 0.8568, ans_loss: 0.7077, eval_loss: 1.0561
2025-05-10 10:24:33,982 [INFO] Loading best validation loss = 1.0560670711100102
2025-05-10 10:28:04,976 [INFO] Step 0 - total_loss: 0.4346, reason_loss: 0.6521, ans_loss: 0.3621, eval_loss: 1.0424
2025-05-10 10:31:39,955 [INFO] Step 1 - total_loss: 0.4285, reason_loss: 0.6521, ans_loss: 0.3540, eval_loss: 1.0291
2025-05-10 10:31:50,259 [INFO] Loading best validation loss = 1.029107405692339
