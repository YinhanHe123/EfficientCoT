2025-05-08 22:55:10,442 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_coin_flip_small/0.5
2025-05-08 22:55:10,443 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/home/nee7ne/data/nee7ne/effi_cot/saved_models/effi_cot/no_sentence_transformer/small/coin_flip/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_sentence_transformer/small/coin_flip/0.5', 'result_path': './results/effi_cot/no_sentence_transformer/small/coin_flip/0.5', 'experiment_name': 'effi_cot_no_sentence_transformer_42_coin_flip_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.0001, 'st_linear_epochs': 3, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 1, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'skrishna/coin_flip', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-08 23:43:12,120 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_coin_flip_small/0.5
2025-05-08 23:43:12,121 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/home/nee7ne/data/nee7ne/effi_cot/saved_models/effi_cot/no_sentence_transformer/small/coin_flip/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_sentence_transformer/small/coin_flip/0.5', 'result_path': './results/effi_cot/no_sentence_transformer/small/coin_flip/0.5', 'experiment_name': 'effi_cot_no_sentence_transformer_42_coin_flip_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.0001, 'st_linear_epochs': 3, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 1, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'skrishna/coin_flip', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-08 23:43:34,365 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_coin_flip_small/0.5
2025-05-08 23:43:34,365 [INFO] Training contemplation generator with variation: no_sentence_transformer
2025-05-08 23:50:12,709 [INFO] Step 0 - total_loss: 0.4736, reason_loss: 0.7478, ans_loss: 0.1993, eval_loss: 1.9523
2025-05-08 23:50:30,541 [INFO] Loading best validation loss = 1.9523334512114525
2025-05-09 13:51:39,652 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_coin_flip_small/0.5
2025-05-09 13:51:39,666 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/no_sentence_transformer/small/coin_flip/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_sentence_transformer/small/coin_flip/0.5', 'result_path': './results/effi_cot/no_sentence_transformer/small/coin_flip/0.5', 'experiment_name': 'effi_cot_no_sentence_transformer_42_coin_flip_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.0001, 'st_linear_epochs': 3, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 1, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'skrishna/coin_flip', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-09 13:51:46,657 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_coin_flip_small/0.5
2025-05-09 13:51:46,658 [INFO] Training contemplation generator with variation: no_sentence_transformer
2025-05-09 13:59:03,710 [INFO] Step 0 - total_loss: 0.4736, reason_loss: 0.7478, ans_loss: 0.1993, eval_loss: 1.9523
2025-05-09 13:59:16,924 [INFO] Loading best validation loss = 1.9523334512114525
2025-05-09 15:09:39,732 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_coin_flip_small/0.5
2025-05-09 15:09:39,749 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/no_sentence_transformer/small/coin_flip/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_sentence_transformer/small/coin_flip/0.5', 'result_path': './results/effi_cot/no_sentence_transformer/small/coin_flip/0.5', 'experiment_name': 'effi_cot_no_sentence_transformer_42_coin_flip_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.0001, 'st_linear_epochs': 3, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 1, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'skrishna/coin_flip', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-09 15:09:48,624 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_coin_flip_small/0.5
2025-05-09 15:09:48,624 [INFO] Training contemplation generator with variation: no_sentence_transformer
2025-05-09 15:16:06,460 [INFO] Step 0 - total_loss: 0.4736, reason_loss: 0.7478, ans_loss: 0.1993, eval_loss: 1.9523
2025-05-09 15:16:17,959 [INFO] Loading best validation loss = 1.9523334512114525
2025-05-09 15:19:55,241 [INFO] Step 0 - total_loss: 0.2255, reason_loss: 0.4497, ans_loss: 0.0012, eval_loss: 1.9514
2025-05-09 15:23:39,510 [INFO] Step 1 - total_loss: 0.2254, reason_loss: 0.4495, ans_loss: 0.0012, eval_loss: 1.9508
2025-05-09 15:23:51,063 [INFO] Loading best validation loss = 1.950788356885314
2025-05-09 15:54:10,204 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_coin_flip_small/0.5
2025-05-09 15:54:10,205 [INFO] Training contemplation generator with variation: no_sentence_transformer
2025-05-09 16:00:23,454 [INFO] Step 0 - total_loss: 0.6599, reason_loss: 0.8010, ans_loss: 0.5188, eval_loss: 1.2360
2025-05-09 16:00:34,900 [INFO] Loading best validation loss = 1.235973121970892
2025-05-09 16:04:13,091 [INFO] Step 0 - total_loss: 0.3117, reason_loss: 0.6149, ans_loss: 0.0086, eval_loss: 1.2370
2025-05-09 16:07:53,022 [INFO] Step 1 - total_loss: 0.3117, reason_loss: 0.6148, ans_loss: 0.0086, eval_loss: 1.2374
2025-05-09 16:07:59,953 [INFO] Loading best validation loss = 1.2369609421491623
2025-05-09 16:38:23,164 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_coin_flip_small/0.5
2025-05-09 16:38:23,164 [INFO] Training contemplation generator with variation: no_sentence_transformer
2025-05-09 16:44:37,642 [INFO] Step 0 - total_loss: 0.7107, reason_loss: 0.7071, ans_loss: 0.7143, eval_loss: 0.8324
2025-05-09 16:44:48,762 [INFO] Loading best validation loss = 0.8323783438652754
2025-05-09 16:48:22,908 [INFO] Step 0 - total_loss: 0.4158, reason_loss: 0.4197, ans_loss: 0.4118, eval_loss: 0.8220
2025-05-09 16:52:02,840 [INFO] Step 1 - total_loss: 0.4105, reason_loss: 0.4196, ans_loss: 0.4014, eval_loss: 0.8120
2025-05-09 16:52:14,513 [INFO] Loading best validation loss = 0.8119969395548106
