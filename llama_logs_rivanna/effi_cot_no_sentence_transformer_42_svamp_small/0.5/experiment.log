2025-05-07 21:44:45,855 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_svamp_small/0.5
2025-05-07 21:44:45,855 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/home/nee7ne/data/nee7ne/effi_cot/saved_models/effi_cot/no_sentence_transformer/small/svamp/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_sentence_transformer/small/svamp/0.5', 'result_path': './results/effi_cot/no_sentence_transformer/small/svamp/0.5', 'experiment_name': 'effi_cot_no_sentence_transformer_42_svamp_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.001, 'st_linear_epochs': 5, 'st_llm_lr': 1e-07, 'st_llm_wd': 1e-05, 'st_llm_epochs': 2, 'cg_linear_lr': 0.0001, 'cg_linear_wd': 0.001, 'cg_linear_epochs': 5, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 1e-05, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/SVAMP', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-07 21:44:56,900 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_svamp_small/0.5
2025-05-07 21:44:56,900 [INFO] Training contemplation generator
2025-05-07 22:19:03,919 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_svamp_small/0.5
2025-05-07 22:19:03,919 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/home/nee7ne/data/nee7ne/effi_cot/saved_models/effi_cot/no_sentence_transformer/small/svamp/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_sentence_transformer/small/svamp/0.5', 'result_path': './results/effi_cot/no_sentence_transformer/small/svamp/0.5', 'experiment_name': 'effi_cot_no_sentence_transformer_42_svamp_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.001, 'st_linear_epochs': 5, 'st_llm_lr': 1e-07, 'st_llm_wd': 1e-05, 'st_llm_epochs': 2, 'cg_linear_lr': 0.0001, 'cg_linear_wd': 0.001, 'cg_linear_epochs': 5, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 1e-05, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/SVAMP', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-07 22:19:11,630 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_svamp_small/0.5
2025-05-07 22:19:11,634 [INFO] Training contemplation generator
2025-05-07 22:23:50,559 [INFO] Step 0 - total_loss: 1.2299, reason_loss: 0.7208, ans_loss: 1.7391, eval_loss: 1.0493
2025-05-07 22:27:11,135 [INFO] Step 1 - total_loss: 0.8312, reason_loss: 0.3247, ans_loss: 1.3376, eval_loss: 0.8376
2025-05-07 22:30:27,645 [INFO] Step 2 - total_loss: 0.7084, reason_loss: 0.1882, ans_loss: 1.2285, eval_loss: 0.8254
2025-05-07 22:33:43,726 [INFO] Step 3 - total_loss: 0.6306, reason_loss: 0.1364, ans_loss: 1.1248, eval_loss: 0.7924
2025-05-07 22:36:59,793 [INFO] Step 4 - total_loss: 0.5813, reason_loss: 0.1137, ans_loss: 1.0489, eval_loss: 0.7697
2025-05-07 22:37:13,585 [INFO] Loading best validation loss = 0.7696719070151449
2025-05-07 22:43:10,629 [INFO] Step 0 - total_loss: 0.4714, reason_loss: 0.0962, ans_loss: 0.8467, eval_loss: 0.7798
2025-05-07 22:49:11,497 [INFO] Step 1 - total_loss: 0.4299, reason_loss: 0.0951, ans_loss: 0.7647, eval_loss: 0.8082
2025-05-07 22:49:21,284 [INFO] Loading best validation loss = 0.7798273934423924
2025-05-08 22:46:57,956 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_svamp_small/0.5
2025-05-08 22:46:57,978 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/home/nee7ne/data/nee7ne/effi_cot/saved_models/effi_cot/no_sentence_transformer/small/svamp/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_sentence_transformer/small/svamp/0.5', 'result_path': './results/effi_cot/no_sentence_transformer/small/svamp/0.5', 'experiment_name': 'effi_cot_no_sentence_transformer_42_svamp_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.001, 'st_linear_epochs': 5, 'st_llm_lr': 1e-07, 'st_llm_wd': 1e-05, 'st_llm_epochs': 2, 'cg_linear_lr': 0.0001, 'cg_linear_wd': 0.001, 'cg_linear_epochs': 5, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 1e-05, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/SVAMP', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-08 22:47:14,606 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_svamp_small/0.5
2025-05-08 22:47:14,606 [INFO] Training contemplation generator with variation: no_sentence_transformer
2025-05-08 22:51:57,457 [INFO] Step 0 - total_loss: 1.2299, reason_loss: 0.7208, ans_loss: 1.7391, eval_loss: 1.0493
2025-05-08 22:55:21,578 [INFO] Step 1 - total_loss: 0.8312, reason_loss: 0.3247, ans_loss: 1.3376, eval_loss: 0.8376
2025-05-08 22:58:43,766 [INFO] Step 2 - total_loss: 0.7084, reason_loss: 0.1882, ans_loss: 1.2285, eval_loss: 0.8254
2025-05-08 23:43:12,899 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_svamp_small/0.5
2025-05-08 23:43:12,920 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/home/nee7ne/data/nee7ne/effi_cot/saved_models/effi_cot/no_sentence_transformer/small/svamp/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_sentence_transformer/small/svamp/0.5', 'result_path': './results/effi_cot/no_sentence_transformer/small/svamp/0.5', 'experiment_name': 'effi_cot_no_sentence_transformer_42_svamp_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.001, 'st_linear_epochs': 5, 'st_llm_lr': 1e-07, 'st_llm_wd': 1e-05, 'st_llm_epochs': 2, 'cg_linear_lr': 0.0001, 'cg_linear_wd': 0.001, 'cg_linear_epochs': 5, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 1e-05, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/SVAMP', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-08 23:43:34,146 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_svamp_small/0.5
2025-05-08 23:43:34,163 [INFO] Training contemplation generator with variation: no_sentence_transformer
2025-05-08 23:48:20,668 [INFO] Step 0 - total_loss: 1.2299, reason_loss: 0.7208, ans_loss: 1.7391, eval_loss: 1.0493
2025-05-08 23:51:45,844 [INFO] Step 1 - total_loss: 0.8312, reason_loss: 0.3247, ans_loss: 1.3376, eval_loss: 0.8376
2025-05-08 23:55:04,967 [INFO] Step 2 - total_loss: 0.7084, reason_loss: 0.1882, ans_loss: 1.2285, eval_loss: 0.8254
2025-05-08 23:58:21,228 [INFO] Step 3 - total_loss: 0.6306, reason_loss: 0.1364, ans_loss: 1.1248, eval_loss: 0.7924
2025-05-09 00:01:39,915 [INFO] Step 4 - total_loss: 0.5813, reason_loss: 0.1137, ans_loss: 1.0489, eval_loss: 0.7697
2025-05-09 00:02:18,773 [INFO] Loading best validation loss = 0.7696719070151449
2025-05-09 13:48:54,410 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_svamp_small/0.5
2025-05-09 13:48:54,436 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/no_sentence_transformer/small/svamp/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_sentence_transformer/small/svamp/0.5', 'result_path': './results/effi_cot/no_sentence_transformer/small/svamp/0.5', 'experiment_name': 'effi_cot_no_sentence_transformer_42_svamp_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.001, 'st_linear_epochs': 5, 'st_llm_lr': 1e-07, 'st_llm_wd': 1e-05, 'st_llm_epochs': 2, 'cg_linear_lr': 0.0001, 'cg_linear_wd': 0.001, 'cg_linear_epochs': 5, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 1e-05, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/SVAMP', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-09 13:49:05,165 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_svamp_small/0.5
2025-05-09 13:49:05,165 [INFO] Training contemplation generator with variation: no_sentence_transformer
2025-05-09 13:54:46,122 [INFO] Step 0 - total_loss: 1.2299, reason_loss: 0.7208, ans_loss: 1.7391, eval_loss: 1.0493
2025-05-09 13:58:22,595 [INFO] Step 1 - total_loss: 0.8312, reason_loss: 0.3247, ans_loss: 1.3376, eval_loss: 0.8376
2025-05-09 14:01:56,447 [INFO] Step 2 - total_loss: 0.7084, reason_loss: 0.1882, ans_loss: 1.2285, eval_loss: 0.8254
2025-05-09 14:05:20,779 [INFO] Step 3 - total_loss: 0.6306, reason_loss: 0.1364, ans_loss: 1.1248, eval_loss: 0.7924
2025-05-09 14:08:50,065 [INFO] Step 4 - total_loss: 0.5813, reason_loss: 0.1137, ans_loss: 1.0489, eval_loss: 0.7697
2025-05-09 14:09:01,378 [INFO] Loading best validation loss = 0.7696719070151449
2025-05-09 15:54:36,771 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_svamp_small/0.5
2025-05-09 15:54:36,790 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/no_sentence_transformer/small/svamp/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_sentence_transformer/small/svamp/0.5', 'result_path': './results/effi_cot/no_sentence_transformer/small/svamp/0.5', 'experiment_name': 'effi_cot_no_sentence_transformer_42_svamp_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.001, 'st_linear_epochs': 5, 'st_llm_lr': 1e-07, 'st_llm_wd': 1e-05, 'st_llm_epochs': 2, 'cg_linear_lr': 0.0001, 'cg_linear_wd': 0.001, 'cg_linear_epochs': 5, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 1e-05, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/SVAMP', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-09 15:54:43,841 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_svamp_small/0.5
2025-05-09 15:54:43,841 [INFO] Training contemplation generator with variation: no_sentence_transformer
2025-05-09 15:59:09,828 [INFO] Step 0 - total_loss: 1.2299, reason_loss: 0.7208, ans_loss: 1.7391, eval_loss: 1.0493
2025-05-09 16:02:27,253 [INFO] Step 1 - total_loss: 0.8312, reason_loss: 0.3247, ans_loss: 1.3376, eval_loss: 0.8376
2025-05-09 16:05:44,177 [INFO] Step 2 - total_loss: 0.7084, reason_loss: 0.1882, ans_loss: 1.2285, eval_loss: 0.8254
2025-05-09 16:09:01,951 [INFO] Step 3 - total_loss: 0.6306, reason_loss: 0.1364, ans_loss: 1.1248, eval_loss: 0.7924
2025-05-09 16:12:19,888 [INFO] Step 4 - total_loss: 0.5813, reason_loss: 0.1137, ans_loss: 1.0489, eval_loss: 0.7697
2025-05-09 16:12:29,971 [INFO] Loading best validation loss = 0.7696719070151449
2025-05-09 16:15:42,893 [INFO] Step 0 - total_loss: 0.5009, reason_loss: 0.1014, ans_loss: 0.9004, eval_loss: 0.7647
2025-05-09 16:19:00,025 [INFO] Step 1 - total_loss: 0.4944, reason_loss: 0.1008, ans_loss: 0.8880, eval_loss: 0.7618
2025-05-09 16:19:09,952 [INFO] Loading best validation loss = 0.7617649033293128
2025-05-09 16:48:34,531 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_svamp_small/0.5
2025-05-09 16:48:34,531 [INFO] Training contemplation generator with variation: no_sentence_transformer
2025-05-09 16:52:56,162 [INFO] Step 0 - total_loss: 1.2394, reason_loss: 0.6220, ans_loss: 1.8568, eval_loss: 1.0458
2025-05-09 16:56:12,867 [INFO] Step 1 - total_loss: 0.9583, reason_loss: 0.2240, ans_loss: 1.6926, eval_loss: 0.9373
2025-05-09 16:59:28,046 [INFO] Step 2 - total_loss: 0.8807, reason_loss: 0.1268, ans_loss: 1.6347, eval_loss: 0.9319
2025-05-09 17:02:44,687 [INFO] Step 3 - total_loss: 0.8255, reason_loss: 0.0809, ans_loss: 1.5701, eval_loss: 0.8842
2025-05-09 17:06:00,333 [INFO] Step 4 - total_loss: 0.7368, reason_loss: 0.0858, ans_loss: 1.3878, eval_loss: 0.7993
2025-05-09 17:06:10,361 [INFO] Loading best validation loss = 0.7992710352689028
2025-05-09 17:09:25,924 [INFO] Step 0 - total_loss: 0.6569, reason_loss: 0.1048, ans_loss: 1.2090, eval_loss: 0.7876
2025-05-09 17:12:45,351 [INFO] Step 1 - total_loss: 0.6462, reason_loss: 0.1045, ans_loss: 1.1878, eval_loss: 0.7816
2025-05-09 17:12:55,490 [INFO] Loading best validation loss = 0.7815723358467221
2025-05-09 17:42:17,532 [INFO] Logging to ./logs/effi_cot_no_sentence_transformer_42_svamp_small/0.5
2025-05-09 17:42:17,532 [INFO] Training contemplation generator with variation: no_sentence_transformer
2025-05-09 17:46:36,418 [INFO] Step 0 - total_loss: 1.3015, reason_loss: 0.7883, ans_loss: 1.8148, eval_loss: 1.2131
2025-05-09 17:49:51,000 [INFO] Step 1 - total_loss: 1.0708, reason_loss: 0.4352, ans_loss: 1.7064, eval_loss: 1.0409
2025-05-09 17:53:05,492 [INFO] Step 2 - total_loss: 0.8969, reason_loss: 0.2921, ans_loss: 1.5018, eval_loss: 0.8673
2025-05-09 17:56:19,027 [INFO] Step 3 - total_loss: 0.7397, reason_loss: 0.2641, ans_loss: 1.2153, eval_loss: 0.8244
2025-05-09 17:59:33,425 [INFO] Step 4 - total_loss: 0.6402, reason_loss: 0.2193, ans_loss: 1.0611, eval_loss: 0.7422
2025-05-09 17:59:43,267 [INFO] Loading best validation loss = 0.7421526358276606
2025-05-09 18:02:57,930 [INFO] Step 0 - total_loss: 0.5251, reason_loss: 0.1923, ans_loss: 0.8579, eval_loss: 0.7405
2025-05-09 18:06:15,353 [INFO] Step 1 - total_loss: 0.5198, reason_loss: 0.1917, ans_loss: 0.8479, eval_loss: 0.7391
2025-05-09 18:06:25,449 [INFO] Loading best validation loss = 0.7391261614114046
