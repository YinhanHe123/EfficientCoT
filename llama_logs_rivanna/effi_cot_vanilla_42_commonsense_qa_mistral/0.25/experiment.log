2025-05-17 10:52:47,146 [INFO] Logging to ./logs/effi_cot_vanilla_42_commonsense_qa_mistral/0.25
2025-05-17 10:52:47,146 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/commonsense_qa/0.25', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/commonsense_qa/0.25', 'result_path': './results/effi_cot/vanilla/mistral/commonsense_qa/0.25', 'experiment_name': 'effi_cot_vanilla_42_commonsense_qa_mistral/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.01, 'st_linear_wd': 0.001, 'st_linear_epochs': 1, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-05, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 1, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'tau/commonsense_qa', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-17 10:52:47,289 [INFO] Logging to ./logs/effi_cot_vanilla_42_commonsense_qa_mistral/0.25
2025-05-17 10:52:47,289 [INFO] Training sentence transformer
2025-05-17 10:59:22,956 [INFO] Step 0 - train_loss: 1.0285, val_loss: 0.9265
2025-05-17 11:00:07,642 [INFO] Loading best validation loss = 0.9265320658683777
2025-05-17 11:02:29,887 [INFO] Step 0 - train_loss: 0.8895, val_loss: 0.8886
2025-05-17 11:03:14,770 [INFO] Loading best validation loss = 0.8885633841156959
2025-05-17 11:03:57,216 [INFO] Logging to ./logs/effi_cot_vanilla_42_commonsense_qa_mistral/0.25
2025-05-17 11:03:57,216 [INFO] Training contemplation generator with variation: vanilla
2025-05-17 11:11:34,637 [INFO] Step 0 - total_loss: 0.9074, reason_loss: 0.7994, ans_loss: 0.9434, eval_loss: 0.7111
2025-05-17 11:16:36,895 [INFO] Step 1 - total_loss: 0.6945, reason_loss: 0.4697, ans_loss: 0.7694, eval_loss: 0.5993
2025-05-17 11:21:39,619 [INFO] Step 2 - total_loss: 0.6289, reason_loss: 0.3714, ans_loss: 0.7147, eval_loss: 0.5895
2025-05-17 11:21:59,643 [INFO] Loading best validation loss = 0.589531562589109
2025-05-17 11:27:00,157 [INFO] Step 0 - total_loss: 0.5051, reason_loss: 0.2900, ans_loss: 0.5768, eval_loss: 0.5781
2025-05-17 11:27:20,533 [INFO] Loading best validation loss = 0.5781420147605241
2025-05-17 12:05:45,217 [INFO] Logging to ./logs/effi_cot_vanilla_42_commonsense_qa_mistral/0.25
2025-05-17 12:05:45,218 [INFO] Training sentence transformer
2025-05-17 12:12:29,547 [INFO] Step 0 - train_loss: 1.0150, val_loss: 0.9222
2025-05-17 12:13:11,881 [INFO] Loading best validation loss = 0.9221680849790573
2025-05-17 12:15:34,828 [INFO] Step 0 - train_loss: 0.9214, val_loss: 0.9043
2025-05-17 12:16:21,086 [INFO] Loading best validation loss = 0.904264971613884
2025-05-17 12:17:03,407 [INFO] Logging to ./logs/effi_cot_vanilla_42_commonsense_qa_mistral/0.25
2025-05-17 12:17:03,407 [INFO] Training contemplation generator with variation: vanilla
2025-05-17 12:24:33,836 [INFO] Step 0 - total_loss: 1.0087, reason_loss: 0.9868, ans_loss: 1.0160, eval_loss: 0.7754
2025-05-17 12:29:31,702 [INFO] Step 1 - total_loss: 0.7042, reason_loss: 0.5454, ans_loss: 0.7571, eval_loss: 0.6259
2025-05-17 12:34:31,900 [INFO] Step 2 - total_loss: 0.6229, reason_loss: 0.3955, ans_loss: 0.6986, eval_loss: 0.6325
2025-05-17 12:34:45,399 [INFO] Loading best validation loss = 0.6258968772925436
2025-05-17 12:39:40,494 [INFO] Step 0 - total_loss: 0.5694, reason_loss: 0.3664, ans_loss: 0.6370, eval_loss: 0.6040
2025-05-17 12:40:01,259 [INFO] Loading best validation loss = 0.6039545742981136
2025-05-17 13:15:49,274 [INFO] Logging to ./logs/effi_cot_vanilla_42_commonsense_qa_mistral/0.25
2025-05-17 13:15:49,274 [INFO] Training sentence transformer
2025-05-17 13:23:12,168 [INFO] Step 0 - train_loss: 1.0455, val_loss: 0.9290
2025-05-17 13:23:55,244 [INFO] Loading best validation loss = 0.9290053844451904
2025-05-17 13:26:17,414 [INFO] Step 0 - train_loss: 0.9147, val_loss: 0.9011
2025-05-17 13:26:57,584 [INFO] Loading best validation loss = 0.9011017739772796
2025-05-17 13:27:39,280 [INFO] Logging to ./logs/effi_cot_vanilla_42_commonsense_qa_mistral/0.25
2025-05-17 13:27:39,280 [INFO] Training contemplation generator with variation: vanilla
2025-05-17 13:35:14,837 [INFO] Step 0 - total_loss: 1.0678, reason_loss: 0.7587, ans_loss: 1.1709, eval_loss: 0.6886
2025-05-17 13:40:18,026 [INFO] Step 1 - total_loss: 0.6888, reason_loss: 0.4598, ans_loss: 0.7651, eval_loss: 0.6563
2025-05-17 13:45:22,088 [INFO] Step 2 - total_loss: 0.6180, reason_loss: 0.3840, ans_loss: 0.6960, eval_loss: 0.6362
2025-05-17 13:45:41,715 [INFO] Loading best validation loss = 0.6362112053669989
2025-05-17 13:50:41,070 [INFO] Step 0 - total_loss: 0.5597, reason_loss: 0.3090, ans_loss: 0.6433, eval_loss: 0.5951
2025-05-17 13:51:00,311 [INFO] Loading best validation loss = 0.5950825259741396
2025-05-17 17:19:52,880 [INFO] Logging to ./logs/effi_cot_vanilla_42_commonsense_qa_mistral/0.25
2025-05-17 17:19:52,897 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/commonsense_qa/0.25', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/commonsense_qa/0.25', 'result_path': './results/effi_cot/vanilla/mistral/commonsense_qa/0.25', 'experiment_name': 'effi_cot_vanilla_42_commonsense_qa_mistral/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 2, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.01, 'st_linear_wd': 0.001, 'st_linear_epochs': 1, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-05, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 1, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'tau/commonsense_qa', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-17 17:24:33,831 [INFO] Logging to ./logs/effi_cot_vanilla_42_commonsense_qa_mistral/0.25
2025-05-17 17:24:33,831 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/commonsense_qa/0.25', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/commonsense_qa/0.25', 'result_path': './results/effi_cot/vanilla/mistral/commonsense_qa/0.25', 'experiment_name': 'effi_cot_vanilla_42_commonsense_qa_mistral/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 3, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.01, 'st_linear_wd': 0.001, 'st_linear_epochs': 1, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-05, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 1, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'tau/commonsense_qa', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-17 17:28:26,597 [INFO] Logging to ./logs/effi_cot_vanilla_42_commonsense_qa_mistral/0.25
2025-05-17 17:28:26,598 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/commonsense_qa/0.25', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/commonsense_qa/0.25', 'result_path': './results/effi_cot/vanilla/mistral/commonsense_qa/0.25', 'experiment_name': 'effi_cot_vanilla_42_commonsense_qa_mistral/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 4, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.01, 'st_linear_wd': 0.001, 'st_linear_epochs': 1, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-05, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 1, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'tau/commonsense_qa', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-17 17:46:54,889 [INFO] Logging to ./logs/effi_cot_vanilla_42_commonsense_qa_mistral/0.25
2025-05-17 17:46:54,890 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/commonsense_qa/0.25', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/commonsense_qa/0.25', 'result_path': './results/effi_cot/vanilla/mistral/commonsense_qa/0.25', 'experiment_name': 'effi_cot_vanilla_42_commonsense_qa_mistral/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 5, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.01, 'st_linear_wd': 0.001, 'st_linear_epochs': 1, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-05, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 1, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'tau/commonsense_qa', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
