2025-05-08 23:43:12,022 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-08 23:43:12,022 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/home/nee7ne/data/nee7ne/effi_cot/saved_models/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'result_path': './results/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'experiment_name': 'effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.0001, 'st_linear_epochs': 3, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 1, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'skrishna/coin_flip', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-08 23:43:12,097 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-08 23:43:12,097 [INFO] Training sentence transformer
2025-05-08 23:46:56,993 [INFO] Step 0 - train_loss: 1.1046, val_loss: 0.9209
2025-05-08 23:47:24,614 [INFO] Step 1 - train_loss: 0.8920, val_loss: 0.9050
2025-05-08 23:47:52,401 [INFO] Step 2 - train_loss: 0.8686, val_loss: 0.8656
2025-05-08 23:48:12,813 [INFO] Loading best validation loss = 0.8656159207224846
2025-05-08 23:49:26,513 [INFO] Step 0 - train_loss: 0.8818, val_loss: 0.8712
2025-05-08 23:49:45,940 [INFO] Loading best validation loss = 0.8711538553237915
2025-05-08 23:50:17,024 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-08 23:50:17,024 [INFO] Training contemplation generator with variation: no_small_contemp_gen
2025-05-08 23:58:13,183 [INFO] Step 0 - total_loss: 0.7793, reason_loss: 0.8598, ans_loss: 0.6988, eval_loss: 0.9517
2025-05-09 06:18:55,519 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 06:18:55,519 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/home/nee7ne/data/nee7ne/effi_cot/saved_models/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'result_path': './results/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'experiment_name': 'effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.0001, 'st_linear_epochs': 3, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 1, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'skrishna/coin_flip', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-09 06:18:55,700 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 06:18:55,700 [INFO] Training sentence transformer
2025-05-09 06:21:33,769 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 06:21:33,786 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/home/nee7ne/data/nee7ne/effi_cot/saved_models/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'result_path': './results/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'experiment_name': 'effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.0001, 'st_linear_epochs': 3, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 1, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'skrishna/coin_flip', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-09 06:21:34,223 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 06:21:34,223 [INFO] Training sentence transformer
2025-05-09 06:26:43,317 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 06:26:44,095 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/home/nee7ne/data/nee7ne/effi_cot/saved_models/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'result_path': './results/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'experiment_name': 'effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.0001, 'st_linear_epochs': 1, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 1, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 1, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'skrishna/coin_flip', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-09 06:26:44,538 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 06:26:44,538 [INFO] Training sentence transformer
2025-05-09 06:30:29,958 [INFO] Step 0 - train_loss: 1.1046, val_loss: 0.9209
2025-05-09 06:43:00,076 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 06:43:00,090 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/home/nee7ne/data/nee7ne/effi_cot/saved_models/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'result_path': './results/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'experiment_name': 'effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.0001, 'st_linear_epochs': 1, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 1, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 1, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'skrishna/coin_flip', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-09 06:43:00,416 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 06:43:00,416 [INFO] Training sentence transformer
2025-05-09 06:46:55,410 [INFO] Step 0 - train_loss: 1.1046, val_loss: 0.9209
2025-05-09 06:57:39,233 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 06:57:39,251 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/home/nee7ne/data/nee7ne/effi_cot/saved_models/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'result_path': './results/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'experiment_name': 'effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.0001, 'st_linear_epochs': 1, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 1, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 1, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'skrishna/coin_flip', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-09 06:57:39,417 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 06:57:39,417 [INFO] Training sentence transformer
2025-05-09 07:01:21,918 [INFO] Step 0 - train_loss: 1.1046, val_loss: 0.9209
2025-05-09 07:01:40,243 [INFO] Loading best validation loss = 0.9209072083234787
2025-05-09 07:02:54,200 [INFO] Step 0 - train_loss: 0.8878, val_loss: 0.8813
2025-05-09 07:03:11,587 [INFO] Loading best validation loss = 0.881330831348896
2025-05-09 07:03:39,074 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 07:03:39,074 [INFO] Training contemplation generator with variation: no_small_contemp_gen
2025-05-09 07:11:33,128 [INFO] Step 0 - total_loss: 0.5819, reason_loss: 0.7775, ans_loss: 0.3862, eval_loss: 1.1247
2025-05-09 07:52:37,538 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 07:52:37,552 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/home/nee7ne/data/nee7ne/effi_cot/saved_models/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'result_path': './results/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'experiment_name': 'effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.0001, 'st_linear_epochs': 3, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 1, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'skrishna/coin_flip', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-09 07:52:37,828 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 07:52:37,828 [INFO] Training sentence transformer
2025-05-09 07:56:27,302 [INFO] Step 0 - train_loss: 1.1046, val_loss: 0.9209
2025-05-09 07:57:28,908 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 08:27:21,568 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 08:27:21,582 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'result_path': './results/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'experiment_name': 'effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.0001, 'st_linear_epochs': 3, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 1, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'skrishna/coin_flip', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-09 08:27:21,730 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 08:27:21,730 [INFO] Training sentence transformer
2025-05-09 08:31:16,121 [INFO] Step 0 - train_loss: 1.1046, val_loss: 0.9209
2025-05-09 08:31:44,697 [INFO] Step 1 - train_loss: 0.8920, val_loss: 0.9050
2025-05-09 08:35:19,648 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 08:35:19,660 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'result_path': './results/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'experiment_name': 'effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.0001, 'st_linear_epochs': 1, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 1, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 1, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'skrishna/coin_flip', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-09 08:35:19,788 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 08:35:19,788 [INFO] Training sentence transformer
2025-05-09 08:38:50,155 [INFO] Step 0 - train_loss: 1.1046, val_loss: 0.9209
2025-05-09 08:39:06,592 [INFO] Loading best validation loss = 0.9209072083234787
2025-05-09 08:40:19,615 [INFO] Step 0 - train_loss: 0.8878, val_loss: 0.8813
2025-05-09 08:40:34,773 [INFO] Loading best validation loss = 0.881330831348896
2025-05-09 08:40:56,261 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 08:40:56,261 [INFO] Training contemplation generator with variation: no_small_contemp_gen
2025-05-09 08:48:44,803 [INFO] Step 0 - total_loss: 0.5819, reason_loss: 0.7775, ans_loss: 0.3862, eval_loss: 1.1247
2025-05-09 08:50:09,032 [INFO] Loading best validation loss = 1.124724799990654
2025-05-09 09:04:00,890 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 09:04:00,900 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'result_path': './results/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'experiment_name': 'effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.0001, 'st_linear_epochs': 1, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 1, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 1, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'skrishna/coin_flip', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-09 09:04:01,058 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 09:04:01,058 [INFO] Training sentence transformer
2025-05-09 09:07:33,698 [INFO] Step 0 - train_loss: 1.1046, val_loss: 0.9209
2025-05-09 09:07:48,878 [INFO] Loading best validation loss = 0.9209072083234787
2025-05-09 09:09:01,803 [INFO] Step 0 - train_loss: 0.8878, val_loss: 0.8813
2025-05-09 09:09:17,202 [INFO] Loading best validation loss = 0.881330831348896
2025-05-09 09:09:38,520 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 09:09:38,520 [INFO] Training contemplation generator with variation: no_small_contemp_gen
2025-05-09 09:17:29,651 [INFO] Step 0 - total_loss: 0.5819, reason_loss: 0.7775, ans_loss: 0.3862, eval_loss: 1.1247
2025-05-09 09:28:19,593 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 09:28:19,611 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'result_path': './results/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'experiment_name': 'effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.0001, 'st_linear_epochs': 1, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 1, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 1, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'skrishna/coin_flip', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-09 09:28:19,750 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 09:28:19,751 [INFO] Training sentence transformer
2025-05-09 09:31:56,909 [INFO] Step 0 - train_loss: 1.1046, val_loss: 0.9209
2025-05-09 09:32:11,974 [INFO] Loading best validation loss = 0.9209072083234787
2025-05-09 09:33:25,036 [INFO] Step 0 - train_loss: 0.8878, val_loss: 0.8813
2025-05-09 09:33:40,272 [INFO] Loading best validation loss = 0.881330831348896
2025-05-09 09:34:01,465 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 09:34:01,466 [INFO] Training contemplation generator with variation: no_small_contemp_gen
2025-05-09 09:41:53,519 [INFO] Step 0 - total_loss: 0.5819, reason_loss: 0.7775, ans_loss: 0.3862, eval_loss: 1.1247
2025-05-09 09:43:00,334 [INFO] Loading best validation loss = 1.124724799990654
2025-05-09 13:48:53,447 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 13:48:53,460 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'result_path': './results/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'experiment_name': 'effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.0001, 'st_linear_epochs': 3, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 1, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'skrishna/coin_flip', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-09 13:48:53,575 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 13:48:53,575 [INFO] Training sentence transformer
2025-05-09 13:52:28,002 [INFO] Step 0 - train_loss: 1.1046, val_loss: 0.9209
2025-05-09 13:52:56,147 [INFO] Step 1 - train_loss: 0.8920, val_loss: 0.9050
2025-05-09 13:53:24,771 [INFO] Step 2 - train_loss: 0.8686, val_loss: 0.8656
2025-05-09 13:53:39,822 [INFO] Loading best validation loss = 0.8656159207224846
2025-05-09 13:54:52,632 [INFO] Step 0 - train_loss: 0.8818, val_loss: 0.8712
2025-05-09 13:55:07,908 [INFO] Loading best validation loss = 0.8711538553237915
2025-05-09 13:55:29,320 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 13:55:29,321 [INFO] Training contemplation generator with variation: no_small_contemp_gen
2025-05-09 14:03:47,980 [INFO] Step 0 - total_loss: 0.7793, reason_loss: 0.8598, ans_loss: 0.6988, eval_loss: 0.9517
2025-05-09 14:04:41,476 [INFO] Loading best validation loss = 0.9516536942496896
2025-05-09 14:11:42,922 [INFO] Step 0 - total_loss: 0.4382, reason_loss: 0.8611, ans_loss: 0.0154, eval_loss: 0.9517
2025-05-09 14:18:56,386 [INFO] Step 1 - total_loss: 0.4382, reason_loss: 0.8610, ans_loss: 0.0153, eval_loss: 0.9517
2025-05-09 14:19:26,497 [INFO] Loading best validation loss = 0.9516536942496896
2025-05-09 14:50:01,311 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 14:50:01,323 [INFO] Training sentence transformer
2025-05-09 14:53:22,666 [INFO] Step 0 - train_loss: 0.9646, val_loss: 0.8732
2025-05-09 14:53:50,928 [INFO] Step 1 - train_loss: 0.8646, val_loss: 0.8725
2025-05-09 14:54:19,531 [INFO] Step 2 - train_loss: 0.8560, val_loss: 0.9090
2025-05-09 14:54:30,937 [INFO] Loading best validation loss = 0.8724935933947563
2025-05-09 14:55:43,779 [INFO] Step 0 - train_loss: 0.8652, val_loss: 0.8916
2025-05-09 14:55:58,624 [INFO] Loading best validation loss = 0.8916406780481339
2025-05-09 14:56:20,000 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 14:56:20,000 [INFO] Training contemplation generator with variation: no_small_contemp_gen
2025-05-09 15:04:01,100 [INFO] Step 0 - total_loss: 0.6053, reason_loss: 0.7731, ans_loss: 0.4374, eval_loss: 0.5843
2025-05-09 15:04:55,256 [INFO] Loading best validation loss = 0.5842631600052118
2025-05-09 15:11:24,248 [INFO] Step 0 - total_loss: 0.5543, reason_loss: 0.7639, ans_loss: 0.3447, eval_loss: 0.5843
2025-05-09 15:18:13,434 [INFO] Step 1 - total_loss: 0.5543, reason_loss: 0.7639, ans_loss: 0.3447, eval_loss: 0.5843
2025-05-09 17:31:06,155 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 17:31:06,168 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'result_path': './results/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'experiment_name': 'effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.0001, 'st_linear_epochs': 1, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 1, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 1, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'skrishna/coin_flip', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-09 17:31:06,279 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 17:31:06,279 [INFO] Training sentence transformer
2025-05-09 17:34:34,909 [INFO] Step 0 - train_loss: 1.1046, val_loss: 0.9209
2025-05-09 17:34:49,686 [INFO] Loading best validation loss = 0.9209072083234787
2025-05-09 17:36:02,466 [INFO] Step 0 - train_loss: 0.8878, val_loss: 0.8813
2025-05-09 17:36:17,370 [INFO] Loading best validation loss = 0.881330831348896
2025-05-09 17:36:38,398 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 17:36:38,398 [INFO] Training contemplation generator with variation: no_small_contemp_gen
2025-05-09 17:44:20,468 [INFO] Step 0 - total_loss: 0.5819, reason_loss: 0.7775, ans_loss: 0.3862, eval_loss: 1.1247
2025-05-09 17:50:49,722 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 17:50:49,738 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'checkpoint_path': './checkpoints/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'result_path': './results/effi_cot/no_small_contemp_gen/small/coin_flip/0.5', 'experiment_name': 'effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.5, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.0001, 'st_linear_epochs': 1, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 1, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 1, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'skrishna/coin_flip', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-09 17:50:49,878 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 17:50:49,878 [INFO] Training sentence transformer
2025-05-09 17:54:20,031 [INFO] Step 0 - train_loss: 1.1046, val_loss: 0.9209
2025-05-09 17:54:34,689 [INFO] Loading best validation loss = 0.9209072083234787
2025-05-09 17:55:47,636 [INFO] Step 0 - train_loss: 0.8878, val_loss: 0.8813
2025-05-09 17:56:02,451 [INFO] Loading best validation loss = 0.881330831348896
2025-05-09 17:56:23,383 [INFO] Logging to ./logs/effi_cot_no_small_contemp_gen_42_coin_flip_small/0.5
2025-05-09 17:56:23,383 [INFO] Training contemplation generator with variation: no_small_contemp_gen
2025-05-09 18:04:20,120 [INFO] Step 0 - total_loss: 0.5819, reason_loss: 0.7775, ans_loss: 0.3862, eval_loss: 1.1247
2025-05-09 18:05:09,844 [INFO] Loading best validation loss = 1.124724799990654
2025-05-09 18:12:03,038 [INFO] Step 0 - total_loss: 0.3760, reason_loss: 0.7461, ans_loss: 0.0060, eval_loss: 1.1250
2025-05-09 18:12:52,436 [INFO] Loading best validation loss = 1.1249798910319806
