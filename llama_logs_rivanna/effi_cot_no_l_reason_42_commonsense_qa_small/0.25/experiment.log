2025-05-16 19:36:17,689 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_commonsense_qa_small/0.25
2025-05-16 19:36:17,689 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/no_l_reason/small/commonsense_qa/0.25', 'checkpoint_path': './checkpoints/effi_cot/no_l_reason/small/commonsense_qa/0.25', 'result_path': './results/effi_cot/no_l_reason/small/commonsense_qa/0.25', 'experiment_name': 'effi_cot_no_l_reason_42_commonsense_qa_small/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.001, 'st_linear_epochs': 1, 'st_llm_lr': 1e-05, 'st_llm_wd': 1e-05, 'st_llm_epochs': 2, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-05, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 1, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'tau/commonsense_qa', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-16 19:36:17,843 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_commonsense_qa_small/0.25
2025-05-16 19:36:17,843 [INFO] Training sentence transformer
2025-05-16 19:42:28,075 [INFO] Step 0 - train_loss: 1.0071, val_loss: 0.9482
2025-05-16 19:42:43,314 [INFO] Loading best validation loss = 0.9481542468070984
2025-05-16 19:44:52,783 [INFO] Step 0 - train_loss: 0.8491, val_loss: 0.8528
2025-05-16 19:47:05,565 [INFO] Step 1 - train_loss: 0.8067, val_loss: 0.8302
2025-05-16 19:47:21,092 [INFO] Loading best validation loss = 0.8302357286214829
2025-05-16 19:47:35,039 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_commonsense_qa_small/0.25
2025-05-16 19:47:35,040 [INFO] Training contemplation generator with variation: no_l_reason
2025-05-16 19:52:20,559 [INFO] Step 0 - total_loss: 1.1637, reason_loss: 0.0000, ans_loss: 1.1637, eval_loss: 1.0350
2025-05-16 19:55:54,657 [INFO] Step 1 - total_loss: 0.9625, reason_loss: 0.0000, ans_loss: 0.9625, eval_loss: 0.9995
2025-05-16 19:59:27,026 [INFO] Step 2 - total_loss: 0.8866, reason_loss: 0.0000, ans_loss: 0.8866, eval_loss: 0.9285
2025-05-16 19:59:37,405 [INFO] Loading best validation loss = 0.9284780977619812
2025-05-16 20:03:06,302 [INFO] Step 0 - total_loss: 0.7541, reason_loss: 0.0000, ans_loss: 0.7541, eval_loss: 0.9192
2025-05-16 20:03:16,749 [INFO] Loading best validation loss = 0.9191762282093987
2025-05-16 20:29:33,830 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_commonsense_qa_small/0.25
2025-05-16 20:29:33,830 [INFO] Training sentence transformer
2025-05-16 20:35:36,314 [INFO] Step 0 - train_loss: 0.9983, val_loss: 0.9459
2025-05-16 20:35:50,363 [INFO] Loading best validation loss = 0.9459194138646125
2025-05-16 20:37:59,416 [INFO] Step 0 - train_loss: 0.8485, val_loss: 0.8454
2025-05-16 20:40:11,724 [INFO] Step 1 - train_loss: 0.8020, val_loss: 0.8299
2025-05-16 20:40:27,340 [INFO] Loading best validation loss = 0.8299040928483009
2025-05-16 20:40:40,391 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_commonsense_qa_small/0.25
2025-05-16 20:40:40,391 [INFO] Training contemplation generator with variation: no_l_reason
2025-05-16 20:45:25,316 [INFO] Step 0 - total_loss: 1.6140, reason_loss: 0.0000, ans_loss: 1.6140, eval_loss: 1.1645
2025-05-16 20:48:56,895 [INFO] Step 1 - total_loss: 0.9910, reason_loss: 0.0000, ans_loss: 0.9910, eval_loss: 0.9108
2025-05-16 20:52:29,292 [INFO] Step 2 - total_loss: 0.8942, reason_loss: 0.0000, ans_loss: 0.8942, eval_loss: 0.8860
2025-05-16 20:52:39,966 [INFO] Loading best validation loss = 0.8859769674483686
2025-05-16 20:56:10,328 [INFO] Step 0 - total_loss: 0.7202, reason_loss: 0.0000, ans_loss: 0.7202, eval_loss: 0.8752
2025-05-16 20:56:21,026 [INFO] Loading best validation loss = 0.8751622372120619
2025-05-16 21:24:47,616 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_commonsense_qa_small/0.25
2025-05-16 21:24:47,616 [INFO] Training sentence transformer
2025-05-16 21:30:49,712 [INFO] Step 0 - train_loss: 1.0060, val_loss: 0.9449
2025-05-16 21:31:03,548 [INFO] Loading best validation loss = 0.9449324086308479
2025-05-16 21:33:12,699 [INFO] Step 0 - train_loss: 0.8498, val_loss: 0.8350
2025-05-16 21:35:25,072 [INFO] Step 1 - train_loss: 0.8072, val_loss: 0.8270
2025-05-16 21:35:39,099 [INFO] Loading best validation loss = 0.8269963666796685
2025-05-16 21:35:51,984 [INFO] Logging to ./logs/effi_cot_no_l_reason_42_commonsense_qa_small/0.25
2025-05-16 21:35:51,984 [INFO] Training contemplation generator with variation: no_l_reason
2025-05-16 21:40:34,795 [INFO] Step 0 - total_loss: 1.0519, reason_loss: 0.0000, ans_loss: 1.0519, eval_loss: 0.9738
2025-05-16 21:44:06,704 [INFO] Step 1 - total_loss: 0.9389, reason_loss: 0.0000, ans_loss: 0.9389, eval_loss: 0.9033
2025-05-16 21:47:38,957 [INFO] Step 2 - total_loss: 0.8747, reason_loss: 0.0000, ans_loss: 0.8747, eval_loss: 0.9865
2025-05-16 21:47:45,317 [INFO] Loading best validation loss = 0.9032570971921087
2025-05-16 21:51:15,273 [INFO] Step 0 - total_loss: 0.8033, reason_loss: 0.0000, ans_loss: 0.8033, eval_loss: 0.8926
2025-05-16 21:51:25,655 [INFO] Loading best validation loss = 0.8926389842573553
