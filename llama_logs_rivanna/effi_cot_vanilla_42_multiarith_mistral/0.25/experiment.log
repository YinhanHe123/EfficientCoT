2025-05-17 11:42:42,373 [INFO] Logging to ./logs/effi_cot_vanilla_42_multiarith_mistral/0.25
2025-05-17 11:42:42,373 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/multiarith/0.25', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/multiarith/0.25', 'result_path': './results/effi_cot/vanilla/mistral/multiarith/0.25', 'experiment_name': 'effi_cot_vanilla_42_multiarith_mistral/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.01, 'st_linear_epochs': 1, 'st_llm_lr': 1e-07, 'st_llm_wd': 0.001, 'st_llm_epochs': 2, 'cg_linear_lr': 0.001, 'cg_linear_wd': 0.0001, 'cg_linear_epochs': 5, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/MultiArith', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-17 11:42:42,429 [INFO] Logging to ./logs/effi_cot_vanilla_42_multiarith_mistral/0.25
2025-05-17 11:42:42,429 [INFO] Training sentence transformer
2025-05-17 11:45:29,339 [INFO] Step 0 - train_loss: 1.0129, val_loss: 0.9368
2025-05-17 11:45:43,563 [INFO] Loading best validation loss = 0.9368025774047488
2025-05-17 11:46:39,441 [INFO] Step 0 - train_loss: 0.8640, val_loss: 0.8942
2025-05-17 11:47:38,727 [INFO] Step 1 - train_loss: 0.8424, val_loss: 0.8805
2025-05-17 11:47:53,887 [INFO] Loading best validation loss = 0.8804978955359686
2025-05-17 11:48:06,023 [INFO] Logging to ./logs/effi_cot_vanilla_42_multiarith_mistral/0.25
2025-05-17 11:48:06,023 [INFO] Training contemplation generator with variation: vanilla
2025-05-17 11:51:35,976 [INFO] Step 0 - total_loss: 1.4788, reason_loss: 0.9898, ans_loss: 1.6418, eval_loss: 1.1835
2025-05-17 11:53:53,495 [INFO] Step 1 - total_loss: 1.1519, reason_loss: 0.9554, ans_loss: 1.2174, eval_loss: 1.1652
2025-05-17 11:56:10,982 [INFO] Step 2 - total_loss: 1.0677, reason_loss: 0.9229, ans_loss: 1.1159, eval_loss: 1.1088
2025-05-17 11:58:28,532 [INFO] Step 3 - total_loss: 0.9508, reason_loss: 0.8969, ans_loss: 0.9687, eval_loss: 1.0783
2025-05-17 12:00:46,489 [INFO] Step 4 - total_loss: 0.8782, reason_loss: 0.8747, ans_loss: 0.8793, eval_loss: 1.1376
2025-05-17 12:00:50,920 [INFO] Loading best validation loss = 1.078263838423623
2025-05-17 12:03:05,346 [INFO] Step 0 - total_loss: 0.8516, reason_loss: 0.8815, ans_loss: 0.8416, eval_loss: 1.0780
2025-05-17 12:05:23,596 [INFO] Step 1 - total_loss: 0.8504, reason_loss: 0.8815, ans_loss: 0.8400, eval_loss: 1.0778
2025-05-17 12:05:31,526 [INFO] Loading best validation loss = 1.0778091591265466
2025-05-17 12:32:55,933 [INFO] Logging to ./logs/effi_cot_vanilla_42_multiarith_mistral/0.25
2025-05-17 12:32:55,933 [INFO] Training sentence transformer
2025-05-17 12:35:39,150 [INFO] Step 0 - train_loss: 1.0019, val_loss: 0.9385
2025-05-17 12:35:53,488 [INFO] Loading best validation loss = 0.9385396440823873
2025-05-17 12:36:49,385 [INFO] Step 0 - train_loss: 0.8731, val_loss: 0.8918
2025-05-17 12:37:48,629 [INFO] Step 1 - train_loss: 0.8431, val_loss: 0.8796
2025-05-17 12:38:02,913 [INFO] Loading best validation loss = 0.8796093378748212
2025-05-17 12:38:14,727 [INFO] Logging to ./logs/effi_cot_vanilla_42_multiarith_mistral/0.25
2025-05-17 12:38:14,727 [INFO] Training contemplation generator with variation: vanilla
2025-05-17 12:41:44,683 [INFO] Step 0 - total_loss: 1.5811, reason_loss: 0.9828, ans_loss: 1.7805, eval_loss: 1.1694
2025-05-17 12:44:02,278 [INFO] Step 1 - total_loss: 1.1288, reason_loss: 0.9577, ans_loss: 1.1859, eval_loss: 1.1194
2025-05-17 12:46:20,186 [INFO] Step 2 - total_loss: 1.0200, reason_loss: 0.9349, ans_loss: 1.0483, eval_loss: 1.0871
2025-05-17 12:48:37,821 [INFO] Step 3 - total_loss: 0.9431, reason_loss: 0.9172, ans_loss: 0.9517, eval_loss: 1.1249
2025-05-17 12:50:52,637 [INFO] Step 4 - total_loss: 0.8878, reason_loss: 0.9016, ans_loss: 0.8832, eval_loss: 1.1689
2025-05-17 12:50:57,181 [INFO] Loading best validation loss = 1.0871024997697936
2025-05-17 12:53:12,301 [INFO] Step 0 - total_loss: 0.9120, reason_loss: 0.9217, ans_loss: 0.9088, eval_loss: 1.0860
2025-05-17 12:55:30,989 [INFO] Step 1 - total_loss: 0.9100, reason_loss: 0.9217, ans_loss: 0.9061, eval_loss: 1.0850
2025-05-17 12:55:38,818 [INFO] Loading best validation loss = 1.0850414344006114
2025-05-17 13:23:04,309 [INFO] Logging to ./logs/effi_cot_vanilla_42_multiarith_mistral/0.25
2025-05-17 13:23:04,309 [INFO] Training sentence transformer
2025-05-17 13:25:47,264 [INFO] Step 0 - train_loss: 1.0085, val_loss: 0.9217
2025-05-17 13:26:01,040 [INFO] Loading best validation loss = 0.9216579255603609
2025-05-17 13:26:56,918 [INFO] Step 0 - train_loss: 0.8427, val_loss: 0.8923
2025-05-17 13:27:55,962 [INFO] Step 1 - train_loss: 0.8469, val_loss: 0.8797
2025-05-17 13:28:10,037 [INFO] Loading best validation loss = 0.8796951458567664
2025-05-17 13:28:21,662 [INFO] Logging to ./logs/effi_cot_vanilla_42_multiarith_mistral/0.25
2025-05-17 13:28:21,662 [INFO] Training contemplation generator with variation: vanilla
2025-05-17 13:31:51,429 [INFO] Step 0 - total_loss: 1.5218, reason_loss: 0.9858, ans_loss: 1.7005, eval_loss: 1.2503
2025-05-17 13:34:08,501 [INFO] Step 1 - total_loss: 1.2342, reason_loss: 0.9552, ans_loss: 1.3272, eval_loss: 1.1516
2025-05-17 13:36:26,160 [INFO] Step 2 - total_loss: 1.0958, reason_loss: 0.9283, ans_loss: 1.1516, eval_loss: 1.2090
2025-05-17 13:38:40,363 [INFO] Step 3 - total_loss: 1.0074, reason_loss: 0.9057, ans_loss: 1.0414, eval_loss: 1.1199
2025-05-17 13:40:58,045 [INFO] Step 4 - total_loss: 0.9475, reason_loss: 0.8830, ans_loss: 0.9690, eval_loss: 1.1147
2025-05-17 13:41:05,442 [INFO] Loading best validation loss = 1.1147184054056802
2025-05-17 13:43:19,753 [INFO] Step 0 - total_loss: 0.8386, reason_loss: 0.8695, ans_loss: 0.8283, eval_loss: 1.1139
2025-05-17 13:45:37,389 [INFO] Step 1 - total_loss: 0.8372, reason_loss: 0.8695, ans_loss: 0.8264, eval_loss: 1.1131
2025-05-17 13:45:45,052 [INFO] Loading best validation loss = 1.1130876292785008
2025-05-17 19:16:19,832 [INFO] Logging to ./logs/effi_cot_vanilla_42_multiarith_mistral/0.25
2025-05-17 19:16:19,843 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/multiarith/0.25', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/multiarith/0.25', 'result_path': './results/effi_cot/vanilla/mistral/multiarith/0.25', 'experiment_name': 'effi_cot_vanilla_42_multiarith_mistral/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 2, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.01, 'st_linear_epochs': 1, 'st_llm_lr': 1e-07, 'st_llm_wd': 0.001, 'st_llm_epochs': 2, 'cg_linear_lr': 0.001, 'cg_linear_wd': 0.0001, 'cg_linear_epochs': 5, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/MultiArith', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-17 19:20:05,388 [INFO] Logging to ./logs/effi_cot_vanilla_42_multiarith_mistral/0.25
2025-05-17 19:20:05,389 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/multiarith/0.25', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/multiarith/0.25', 'result_path': './results/effi_cot/vanilla/mistral/multiarith/0.25', 'experiment_name': 'effi_cot_vanilla_42_multiarith_mistral/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 3, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.01, 'st_linear_epochs': 1, 'st_llm_lr': 1e-07, 'st_llm_wd': 0.001, 'st_llm_epochs': 2, 'cg_linear_lr': 0.001, 'cg_linear_wd': 0.0001, 'cg_linear_epochs': 5, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/MultiArith', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-17 19:34:06,716 [INFO] Logging to ./logs/effi_cot_vanilla_42_multiarith_mistral/0.25
2025-05-17 19:34:06,717 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/multiarith/0.25', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/multiarith/0.25', 'result_path': './results/effi_cot/vanilla/mistral/multiarith/0.25', 'experiment_name': 'effi_cot_vanilla_42_multiarith_mistral/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 4, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.01, 'st_linear_epochs': 1, 'st_llm_lr': 1e-07, 'st_llm_wd': 0.001, 'st_llm_epochs': 2, 'cg_linear_lr': 0.001, 'cg_linear_wd': 0.0001, 'cg_linear_epochs': 5, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/MultiArith', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-17 19:38:47,018 [INFO] Logging to ./logs/effi_cot_vanilla_42_multiarith_mistral/0.25
2025-05-17 19:38:47,019 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/multiarith/0.25', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/multiarith/0.25', 'result_path': './results/effi_cot/vanilla/mistral/multiarith/0.25', 'experiment_name': 'effi_cot_vanilla_42_multiarith_mistral/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 5, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.0001, 'st_linear_wd': 0.01, 'st_linear_epochs': 1, 'st_llm_lr': 1e-07, 'st_llm_wd': 0.001, 'st_llm_epochs': 2, 'cg_linear_lr': 0.001, 'cg_linear_wd': 0.0001, 'cg_linear_epochs': 5, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/MultiArith', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
