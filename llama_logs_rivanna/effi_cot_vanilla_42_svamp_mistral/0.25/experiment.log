2025-05-11 20:11:50,492 [INFO] Logging to ./logs/effi_cot_vanilla_42_svamp_mistral/0.25
2025-05-11 20:11:50,492 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/svamp/0.25', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/svamp/0.25', 'result_path': './results/effi_cot/vanilla/mistral/svamp/0.25', 'experiment_name': 'effi_cot_vanilla_42_svamp_mistral/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.01, 'st_linear_wd': 0.0001, 'st_linear_epochs': 3, 'st_llm_lr': 1e-05, 'st_llm_wd': 1e-05, 'st_llm_epochs': 2, 'cg_linear_lr': 0.0001, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-05, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 1, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/SVAMP', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-11 20:11:52,056 [INFO] Logging to ./logs/effi_cot_vanilla_42_svamp_mistral/0.25
2025-05-11 20:11:52,056 [INFO] Training sentence transformer
2025-05-11 20:14:24,405 [INFO] Step 0 - train_loss: 1.0422, val_loss: 0.9674
2025-05-11 20:14:42,369 [INFO] Step 1 - train_loss: 0.9531, val_loss: 0.9429
2025-05-11 20:15:00,582 [INFO] Step 2 - train_loss: 0.9113, val_loss: 0.8961
2025-05-11 20:15:16,725 [INFO] Loading best validation loss = 0.8960726090839931
2025-05-11 20:16:03,188 [INFO] Step 0 - train_loss: 1.0964, val_loss: 0.9125
2025-05-11 20:16:53,832 [INFO] Step 1 - train_loss: 0.9476, val_loss: 0.8541
2025-05-11 20:17:10,324 [INFO] Loading best validation loss = 0.8540690268789018
2025-05-11 20:18:30,956 [INFO] Logging to ./logs/effi_cot_vanilla_42_svamp_mistral/0.25
2025-05-11 20:18:30,956 [INFO] Training contemplation generator with variation: vanilla
2025-05-11 20:22:39,561 [INFO] Step 0 - total_loss: 1.2404, reason_loss: 0.8065, ans_loss: 1.3851, eval_loss: 1.0996
2025-05-11 20:26:14,295 [INFO] Step 1 - total_loss: 0.7849, reason_loss: 0.4377, ans_loss: 0.9006, eval_loss: 0.9356
2025-05-11 20:29:46,734 [INFO] Step 2 - total_loss: 0.6313, reason_loss: 0.2895, ans_loss: 0.7453, eval_loss: 0.8751
2025-05-11 20:29:54,988 [INFO] Loading best validation loss = 0.8750695445667952
2025-05-11 20:33:24,699 [INFO] Step 0 - total_loss: 0.4284, reason_loss: 0.1915, ans_loss: 0.5074, eval_loss: 0.9158
2025-05-11 20:33:32,873 [INFO] Loading best validation loss = 0.9157949945866131
2025-05-17 10:55:29,559 [INFO] Logging to ./logs/effi_cot_vanilla_42_svamp_mistral/0.25
2025-05-17 10:55:29,559 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/svamp/0.25', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/svamp/0.25', 'result_path': './results/effi_cot/vanilla/mistral/svamp/0.25', 'experiment_name': 'effi_cot_vanilla_42_svamp_mistral/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.01, 'st_linear_wd': 0.0001, 'st_linear_epochs': 3, 'st_llm_lr': 1e-05, 'st_llm_wd': 1e-05, 'st_llm_epochs': 2, 'cg_linear_lr': 0.0001, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-05, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 1, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/SVAMP', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-17 10:55:30,146 [INFO] Logging to ./logs/effi_cot_vanilla_42_svamp_mistral/0.25
2025-05-17 10:55:30,146 [INFO] Training sentence transformer
2025-05-17 10:57:37,018 [INFO] Step 0 - train_loss: 1.0422, val_loss: 0.9674
2025-05-17 10:57:54,675 [INFO] Step 1 - train_loss: 0.9531, val_loss: 0.9429
2025-05-17 10:58:12,540 [INFO] Step 2 - train_loss: 0.9113, val_loss: 0.8961
2025-05-17 10:58:27,834 [INFO] Loading best validation loss = 0.8960726090839931
2025-05-17 10:59:14,054 [INFO] Step 0 - train_loss: 1.0964, val_loss: 0.9125
2025-05-17 11:00:03,598 [INFO] Step 1 - train_loss: 0.9476, val_loss: 0.8541
2025-05-17 11:00:18,847 [INFO] Loading best validation loss = 0.8540690268789018
2025-05-17 11:00:30,907 [INFO] Logging to ./logs/effi_cot_vanilla_42_svamp_mistral/0.25
2025-05-17 11:00:30,907 [INFO] Training contemplation generator with variation: vanilla
2025-05-17 11:04:33,638 [INFO] Step 0 - total_loss: 1.2404, reason_loss: 0.8065, ans_loss: 1.3851, eval_loss: 1.0996
2025-05-17 11:08:06,071 [INFO] Step 1 - total_loss: 0.7849, reason_loss: 0.4377, ans_loss: 0.9006, eval_loss: 0.9356
2025-05-17 11:11:38,379 [INFO] Step 2 - total_loss: 0.6313, reason_loss: 0.2895, ans_loss: 0.7453, eval_loss: 0.8751
2025-05-17 11:11:46,419 [INFO] Loading best validation loss = 0.8750695445667952
2025-05-17 11:15:16,386 [INFO] Step 0 - total_loss: 0.4284, reason_loss: 0.1915, ans_loss: 0.5074, eval_loss: 0.9158
2025-05-17 11:15:24,475 [INFO] Loading best validation loss = 0.9157949945866131
2025-05-17 11:45:44,048 [INFO] Logging to ./logs/effi_cot_vanilla_42_svamp_mistral/0.25
2025-05-17 11:45:44,048 [INFO] Training sentence transformer
2025-05-17 11:47:48,003 [INFO] Step 0 - train_loss: 1.0216, val_loss: 0.9859
2025-05-17 11:48:05,705 [INFO] Step 1 - train_loss: 0.9157, val_loss: 0.9505
2025-05-17 11:48:23,670 [INFO] Step 2 - train_loss: 0.8732, val_loss: 0.9202
2025-05-17 11:48:38,290 [INFO] Loading best validation loss = 0.92023161309106
2025-05-17 11:49:24,530 [INFO] Step 0 - train_loss: 0.9627, val_loss: 0.9631
2025-05-17 11:50:14,123 [INFO] Step 1 - train_loss: 0.8899, val_loss: 0.8717
2025-05-17 11:50:28,735 [INFO] Loading best validation loss = 0.871707216330937
2025-05-17 11:50:42,022 [INFO] Logging to ./logs/effi_cot_vanilla_42_svamp_mistral/0.25
2025-05-17 11:50:42,022 [INFO] Training contemplation generator with variation: vanilla
2025-05-17 11:54:45,058 [INFO] Step 0 - total_loss: 1.5762, reason_loss: 0.8346, ans_loss: 1.8234, eval_loss: 1.4565
2025-05-17 11:58:17,984 [INFO] Step 1 - total_loss: 1.3036, reason_loss: 0.4672, ans_loss: 1.5824, eval_loss: 1.2138
2025-05-17 12:01:51,125 [INFO] Step 2 - total_loss: 0.9073, reason_loss: 0.3148, ans_loss: 1.1048, eval_loss: 0.9750
2025-05-17 12:01:59,101 [INFO] Loading best validation loss = 0.9750175677984952
2025-05-17 12:05:29,494 [INFO] Step 0 - total_loss: 0.6263, reason_loss: 0.2156, ans_loss: 0.7632, eval_loss: 0.9308
2025-05-17 12:05:37,559 [INFO] Loading best validation loss = 0.9307770236534998
2025-05-17 12:35:52,179 [INFO] Logging to ./logs/effi_cot_vanilla_42_svamp_mistral/0.25
2025-05-17 12:35:52,180 [INFO] Training sentence transformer
2025-05-17 12:37:55,987 [INFO] Step 0 - train_loss: 1.0299, val_loss: 1.0326
2025-05-17 12:38:13,939 [INFO] Step 1 - train_loss: 0.9307, val_loss: 0.9287
2025-05-17 12:38:32,176 [INFO] Step 2 - train_loss: 0.9072, val_loss: 0.8814
2025-05-17 12:38:46,689 [INFO] Loading best validation loss = 0.8814285687037877
2025-05-17 12:39:32,891 [INFO] Step 0 - train_loss: 1.0023, val_loss: 0.9854
2025-05-17 12:40:22,642 [INFO] Step 1 - train_loss: 0.9106, val_loss: 0.9985
2025-05-17 12:40:33,576 [INFO] Loading best validation loss = 0.985399157660348
2025-05-17 12:40:45,510 [INFO] Logging to ./logs/effi_cot_vanilla_42_svamp_mistral/0.25
2025-05-17 12:40:45,510 [INFO] Training contemplation generator with variation: vanilla
2025-05-17 12:44:48,149 [INFO] Step 0 - total_loss: 1.3572, reason_loss: 0.8367, ans_loss: 1.5307, eval_loss: 1.2154
2025-05-17 12:48:20,712 [INFO] Step 1 - total_loss: 0.9710, reason_loss: 0.7108, ans_loss: 1.0577, eval_loss: 1.0832
2025-05-17 12:51:53,493 [INFO] Step 2 - total_loss: 0.7560, reason_loss: 0.4391, ans_loss: 0.8616, eval_loss: 1.0518
2025-05-17 12:52:01,435 [INFO] Loading best validation loss = 1.051778669608757
2025-05-17 12:55:31,666 [INFO] Step 0 - total_loss: 0.5724, reason_loss: 0.3106, ans_loss: 0.6597, eval_loss: 0.9517
2025-05-17 12:55:39,794 [INFO] Loading best validation loss = 0.951721955081448
2025-05-17 18:00:24,569 [INFO] Logging to ./logs/effi_cot_vanilla_42_svamp_mistral/0.25
2025-05-17 18:00:24,580 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/svamp/0.25', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/svamp/0.25', 'result_path': './results/effi_cot/vanilla/mistral/svamp/0.25', 'experiment_name': 'effi_cot_vanilla_42_svamp_mistral/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 2, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.01, 'st_linear_wd': 0.0001, 'st_linear_epochs': 3, 'st_llm_lr': 1e-05, 'st_llm_wd': 1e-05, 'st_llm_epochs': 2, 'cg_linear_lr': 0.0001, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-05, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 1, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/SVAMP', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-17 18:29:24,144 [INFO] Logging to ./logs/effi_cot_vanilla_42_svamp_mistral/0.25
2025-05-17 18:29:24,162 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/svamp/0.25', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/svamp/0.25', 'result_path': './results/effi_cot/vanilla/mistral/svamp/0.25', 'experiment_name': 'effi_cot_vanilla_42_svamp_mistral/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 3, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.01, 'st_linear_wd': 0.0001, 'st_linear_epochs': 3, 'st_llm_lr': 1e-05, 'st_llm_wd': 1e-05, 'st_llm_epochs': 2, 'cg_linear_lr': 0.0001, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-05, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 1, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/SVAMP', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-17 18:30:07,482 [INFO] Logging to ./logs/effi_cot_vanilla_42_svamp_mistral/0.25
2025-05-17 18:30:07,483 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/svamp/0.25', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/svamp/0.25', 'result_path': './results/effi_cot/vanilla/mistral/svamp/0.25', 'experiment_name': 'effi_cot_vanilla_42_svamp_mistral/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 4, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.01, 'st_linear_wd': 0.0001, 'st_linear_epochs': 3, 'st_llm_lr': 1e-05, 'st_llm_wd': 1e-05, 'st_llm_epochs': 2, 'cg_linear_lr': 0.0001, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-05, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 1, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/SVAMP', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-17 18:31:24,145 [INFO] Logging to ./logs/effi_cot_vanilla_42_svamp_mistral/0.25
2025-05-17 18:31:24,146 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/svamp/0.25', 'checkpoint_path': './checkpoints/effi_cot/vanilla/mistral/svamp/0.25', 'result_path': './results/effi_cot/vanilla/mistral/svamp/0.25', 'experiment_name': 'effi_cot_vanilla_42_svamp_mistral/0.25', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 5, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.01, 'st_linear_wd': 0.0001, 'st_linear_epochs': 3, 'st_llm_lr': 1e-05, 'st_llm_wd': 1e-05, 'st_llm_epochs': 2, 'cg_linear_lr': 0.0001, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 3, 'cg_llm_lr': 1e-05, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 1, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/SVAMP', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
