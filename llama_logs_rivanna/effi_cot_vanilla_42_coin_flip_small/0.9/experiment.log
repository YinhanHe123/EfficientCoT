2025-05-10 05:23:31,762 [INFO] Logging to ./logs/effi_cot_vanilla_42_coin_flip_small/0.9
2025-05-10 05:23:31,762 [INFO] Hyperparameters: {'config_name': 'small', 'log_dir': './logs', 'model_save_path': '/scratch/nee7ne/effi_cot/saved_models/effi_cot/vanilla/small/coin_flip/0.9', 'checkpoint_path': './checkpoints/effi_cot/vanilla/small/coin_flip/0.9', 'result_path': './results/effi_cot/vanilla/small/coin_flip/0.9', 'experiment_name': 'effi_cot_vanilla_42_coin_flip_small/0.9', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_weight_decay': 1e-05, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'batch_size': 4, 'alpha': 0.9, 'save_interval': 1, 'max_seq_length': 512, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 800, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 1, 'start_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/sfs/gpfs/tardis/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'ccot_lr': 1e-05, 'eval_temp': 0.7, 'codi_lr': 0.0008, 'coconut_stage': None, 'st_linear_lr': 0.001, 'st_linear_wd': 0.0001, 'st_linear_epochs': 3, 'st_llm_lr': 1e-05, 'st_llm_wd': 0.001, 'st_llm_epochs': 1, 'cg_linear_lr': 0.01, 'cg_linear_wd': 0.01, 'cg_linear_epochs': 1, 'cg_llm_lr': 1e-07, 'cg_llm_wd': 0.001, 'cg_llm_epochs': 2, 'teacher_model_name': 'meta-llama/Llama-2-7b-chat-hf', 'student_model_name': 'princeton-nlp/Sheared-LLaMA-1.3B', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'skrishna/coin_flip', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
2025-05-10 05:23:31,900 [INFO] Logging to ./logs/effi_cot_vanilla_42_coin_flip_small/0.9
2025-05-10 05:23:31,900 [INFO] Training sentence transformer
2025-05-10 05:27:11,212 [INFO] Step 0 - train_loss: 1.1046, val_loss: 0.9209
2025-05-10 05:27:39,505 [INFO] Step 1 - train_loss: 0.8920, val_loss: 0.9050
2025-05-10 05:28:08,284 [INFO] Step 2 - train_loss: 0.8686, val_loss: 0.8656
2025-05-10 05:28:23,304 [INFO] Loading best validation loss = 0.8656159207224846
2025-05-10 05:29:36,178 [INFO] Step 0 - train_loss: 0.8818, val_loss: 0.8712
2025-05-10 05:29:51,177 [INFO] Loading best validation loss = 0.8711538553237915
2025-05-10 05:30:06,014 [INFO] Logging to ./logs/effi_cot_vanilla_42_coin_flip_small/0.9
2025-05-10 05:30:06,014 [INFO] Training contemplation generator with variation: vanilla
2025-05-10 05:35:09,821 [INFO] Step 0 - total_loss: 0.8481, reason_loss: 0.9092, ans_loss: 0.2982, eval_loss: 0.9490
2025-05-10 05:35:19,638 [INFO] Loading best validation loss = 0.9490254754573106
2025-05-10 05:39:14,771 [INFO] Step 0 - total_loss: 0.7601, reason_loss: 0.8438, ans_loss: 0.0060, eval_loss: 0.9492
2025-05-10 05:43:13,039 [INFO] Step 1 - total_loss: 0.7601, reason_loss: 0.8438, ans_loss: 0.0060, eval_loss: 0.9494
2025-05-10 05:43:19,391 [INFO] Loading best validation loss = 0.9492388796806336
2025-05-10 06:13:17,016 [INFO] Logging to ./logs/effi_cot_vanilla_42_coin_flip_small/0.9
2025-05-10 06:13:17,016 [INFO] Training sentence transformer
2025-05-10 06:16:39,716 [INFO] Step 0 - train_loss: 0.9557, val_loss: 0.8709
2025-05-10 06:17:07,881 [INFO] Step 1 - train_loss: 0.8652, val_loss: 0.8721
2025-05-10 06:17:33,156 [INFO] Step 2 - train_loss: 0.8565, val_loss: 0.9112
2025-05-10 06:17:43,645 [INFO] Loading best validation loss = 0.8708544254302979
2025-05-10 06:18:56,605 [INFO] Step 0 - train_loss: 0.8746, val_loss: 0.8923
2025-05-10 06:19:10,109 [INFO] Loading best validation loss = 0.8923209741711616
2025-05-10 06:19:22,785 [INFO] Logging to ./logs/effi_cot_vanilla_42_coin_flip_small/0.9
2025-05-10 06:19:22,785 [INFO] Training contemplation generator with variation: vanilla
2025-05-10 06:24:25,964 [INFO] Step 0 - total_loss: 0.7502, reason_loss: 0.7628, ans_loss: 0.6367, eval_loss: 0.6630
2025-05-10 06:24:35,620 [INFO] Loading best validation loss = 0.6629864757508039
2025-05-10 06:28:32,544 [INFO] Step 0 - total_loss: 0.6925, reason_loss: 0.7180, ans_loss: 0.4630, eval_loss: 0.6603
2025-05-10 06:32:31,292 [INFO] Step 1 - total_loss: 0.6910, reason_loss: 0.7180, ans_loss: 0.4480, eval_loss: 0.6577
2025-05-10 06:32:41,400 [INFO] Loading best validation loss = 0.6576727029681205
2025-05-10 07:02:37,339 [INFO] Logging to ./logs/effi_cot_vanilla_42_coin_flip_small/0.9
2025-05-10 07:02:37,339 [INFO] Training sentence transformer
2025-05-10 07:06:01,961 [INFO] Step 0 - train_loss: 0.9780, val_loss: 0.8871
2025-05-10 07:06:30,684 [INFO] Step 1 - train_loss: 0.8506, val_loss: 0.8733
2025-05-10 07:06:59,778 [INFO] Step 2 - train_loss: 0.8667, val_loss: 0.8735
2025-05-10 07:07:10,271 [INFO] Loading best validation loss = 0.8733059391379356
2025-05-10 07:08:23,223 [INFO] Step 0 - train_loss: 0.8806, val_loss: 0.8607
2025-05-10 07:08:37,212 [INFO] Loading best validation loss = 0.8607293233275414
2025-05-10 07:08:50,242 [INFO] Logging to ./logs/effi_cot_vanilla_42_coin_flip_small/0.9
2025-05-10 07:08:50,243 [INFO] Training contemplation generator with variation: vanilla
2025-05-10 07:13:52,941 [INFO] Step 0 - total_loss: 0.8059, reason_loss: 0.8429, ans_loss: 0.4726, eval_loss: 0.3865
2025-05-10 07:14:03,176 [INFO] Loading best validation loss = 0.3865156346745789
2025-05-10 07:17:59,459 [INFO] Step 0 - total_loss: 0.7060, reason_loss: 0.7829, ans_loss: 0.0137, eval_loss: 0.3886
2025-05-10 07:21:56,849 [INFO] Step 1 - total_loss: 0.7059, reason_loss: 0.7829, ans_loss: 0.0132, eval_loss: 0.3906
2025-05-10 07:22:02,990 [INFO] Loading best validation loss = 0.38860872434452176
