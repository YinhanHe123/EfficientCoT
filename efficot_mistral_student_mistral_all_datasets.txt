(effi_cot) nee7ne@uvavast:~/EfficientCoT$ bash run_para_search.sh
Starting evaluation runs with 45 combinations...
[1/45] Running with max_contemp_tokens=1, eval_temp=0.1, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                                                                         | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be in
ferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:14<00:00,  1.34s/it]
Average time taken for each sample: 1.3212259936332702, Average time taken for contemplation: 0.0209812593460083
Evaluation results: {'numerical_accuracy': 0.42, 'close_match_rate': 0.44, 'mean_relative_error': np.float64(0.5678593571264325), 'median_relative_error': np.float64(0.0626443931046739)}
[2/45] Running with max_contemp_tokens=1, eval_temp=0.2, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                                                                         | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be in
ferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:15<00:00,  1.35s/it]
Average time taken for each sample: 1.3321908855438231, Average time taken for contemplation: 0.020742759704589844
Evaluation results: {'numerical_accuracy': 0.42, 'close_match_rate': 0.44, 'mean_relative_error': np.float64(0.5732077719980403), 'median_relative_error': np.float64(0.07244831467330135)}
[3/45] Running with max_contemp_tokens=1, eval_temp=0.3, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                                                                         | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be in
ferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:14<00:00,  1.35s/it]
Average time taken for each sample: 1.3299633169174194, Average time taken for contemplation: 0.02075514554977417
Evaluation results: {'numerical_accuracy': 0.42, 'close_match_rate': 0.44, 'mean_relative_error': np.float64(0.5756629645627359), 'median_relative_error': np.float64(0.07244831467330135)}
[4/45] Running with max_contemp_tokens=1, eval_temp=0.4, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:07<00:00,  2.66s/it]
Running inference:   0%|                                                                                                                         | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be in
ferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:14<00:00,  1.35s/it]
Average time taken for each sample: 1.3282556271553039, Average time taken for contemplation: 0.02167559862136841
Evaluation results: {'numerical_accuracy': 0.42, 'close_match_rate': 0.44, 'mean_relative_error': np.float64(0.5747538736536449), 'median_relative_error': np.float64(0.07244831467330135)}
[5/45] Running with max_contemp_tokens=1, eval_temp=0.5, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.12s/it]
Running inference:   0%|                                                                                                                         | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be in
ferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:13<00:00,  1.34s/it]
Average time taken for each sample: 1.315820574760437, Average time taken for contemplation: 0.020481247901916504
Evaluation results: {'numerical_accuracy': 0.42, 'close_match_rate': 0.44, 'mean_relative_error': np.float64(0.5847336929307535), 'median_relative_error': np.float64(0.07244831467330135)}
[6/45] Running with max_contemp_tokens=1, eval_temp=0.6, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.08it/s]
Running inference:   0%|                                                                                                                         | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be in
ferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:14<00:00,  1.34s/it]
Average time taken for each sample: 1.3233441925048828, Average time taken for contemplation: 0.021096746921539306
Evaluation results: {'numerical_accuracy': 0.43, 'close_match_rate': 0.45, 'mean_relative_error': np.float64(0.5221406512583364), 'median_relative_error': np.float64(0.0626443931046739)}
[7/45] Running with max_contemp_tokens=1, eval_temp=0.7, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.40it/s]
Running inference:   0%|                                                                                                                         | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be in
ferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference:  83%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  84%|███████████████████████████████████████████████████████████████████████████████
█████████Running inference:  85%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  86%|██████████████████████████████████████████████████████████████████████
██████████████████Running inference:  87%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  88%|█████████████████████████████████████████████████████████████
███████████████████████████Running inference:  89%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  90%|████████████████████████████████████████████████████
████████████████████████████████████Running inference:  91%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  92%|███████████████████████████████████████████
█████████████████████████████████████████████Running inference:  93%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  94%|██████████████████████████████████
██████████████████████████████████████████████████████Running inference:  95%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  96%|█████████████████████████
███████████████████████████████████████████████████████████████Running inference:  97%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  98%|████████████████
████████████████████████████████████████████████████████████████████████Running inference:  99%|████████████████████████████████████████████████████████████████████████████████████████Running inference: 100%|███████
█████████████████████████████████████████████████████████████████████████████████Running inference: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████
█| 100/100 [02:14<00:00,  1.35s/it]
Average time taken for each sample: 1.3274980235099791, Average time taken for contemplation: 0.021348786354064942
Evaluation results: {'numerical_accuracy': 0.42, 'close_match_rate': 0.44, 'mean_relative_error': np.float64(0.6085189047259513), 'median_relative_error': np.float64(0.07352941176470588)}
[8/45] Running with max_contemp_tokens=1, eval_temp=0.8, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.40it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:01<00:00,  1.22s/it]
Average time taken for each sample: 1.201466703414917, Average time taken for contemplation: 0.020916948318481444
Evaluation results: {'numerical_accuracy': 0.44, 'close_match_rate': 0.47, 'mean_relative_error': np.float64(0.5022261324161491), 'median_relative_error': np.float64(0.0373864430468204)}
[9/45] Running with max_contemp_tokens=1, eval_temp=0.9, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:03<00:00,  1.23s/it]
Average time taken for each sample: 1.21273339509964, Average time taken for contemplation: 0.020850965976715086
Evaluation results: {'numerical_accuracy': 0.44, 'close_match_rate': 0.47, 'mean_relative_error': np.float64(0.48160516295916694), 'median_relative_error': np.float64(0.0373864430468204)}
[10/45] Running with max_contemp_tokens=2, eval_temp=0.1, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:05<00:00,  1.25s/it]
Average time taken for each sample: 1.2355485272407531, Average time taken for contemplation: 0.02108677625656128
Evaluation results: {'numerical_accuracy': 0.48, 'close_match_rate': 0.5, 'mean_relative_error': np.float64(0.3421133321916276), 'median_relative_error': np.float64(0.014482664595452142)}
[11/45] Running with max_contemp_tokens=2, eval_temp=0.2, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:08<00:00,  1.28s/it]
Average time taken for each sample: 1.2602696299552918, Average time taken for contemplation: 0.021433298587799073
Evaluation results: {'numerical_accuracy': 0.45, 'close_match_rate': 0.47, 'mean_relative_error': np.float64(0.37409797904734676), 'median_relative_error': np.float64(0.0373864430468204)}
[12/45] Running with max_contemp_tokens=2, eval_temp=0.3, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:15<00:00,  1.36s/it]
Average time taken for each sample: 1.3411907362937927, Average time taken for contemplation: 0.02059129476547241
Evaluation results: {'numerical_accuracy': 0.46, 'close_match_rate': 0.48, 'mean_relative_error': np.float64(0.3980075059419482), 'median_relative_error': np.float64(0.03367003367003367)}
[13/45] Running with max_contemp_tokens=2, eval_temp=0.4, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:20<00:00,  1.41s/it]
Average time taken for each sample: 1.3876353120803833, Average time taken for contemplation: 0.021558706760406495
Evaluation results: {'numerical_accuracy': 0.47, 'close_match_rate': 0.49, 'mean_relative_error': np.float64(0.33726706067070567), 'median_relative_error': np.float64(0.02308382609810426)}
[14/45] Running with max_contemp_tokens=2, eval_temp=0.5, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:17<00:00,  1.38s/it]
Average time taken for each sample: 1.3598338150978089, Average time taken for contemplation: 0.021439623832702637
Evaluation results: {'numerical_accuracy': 0.47, 'close_match_rate': 0.49, 'mean_relative_error': np.float64(0.39108696047453234), 'median_relative_error': np.float64(0.02308382609810426)}
[15/45] Running with max_contemp_tokens=2, eval_temp=0.6, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:18<00:00,  1.38s/it]
Average time taken for each sample: 1.3635245585441589, Average time taken for contemplation: 0.02130107641220093
Evaluation results: {'numerical_accuracy': 0.43, 'close_match_rate': 0.45, 'mean_relative_error': np.float64(0.4298382941371975), 'median_relative_error': np.float64(0.050464396284829724)}
[16/45] Running with max_contemp_tokens=2, eval_temp=0.7, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:16<00:00,  1.37s/it]
Average time taken for each sample: 1.347483468055725, Average time taken for contemplation: 0.021227793693542482
Evaluation results: {'numerical_accuracy': 0.46, 'close_match_rate': 0.49, 'mean_relative_error': np.float64(0.39906209555814726), 'median_relative_error': np.float64(0.02308382609810426)}
[17/45] Running with max_contemp_tokens=2, eval_temp=0.8, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:14<00:00,  1.34s/it]
Average time taken for each sample: 1.322971646785736, Average time taken for contemplation: 0.021486899852752685
Evaluation results: {'numerical_accuracy': 0.47, 'close_match_rate': 0.49, 'mean_relative_error': np.float64(0.39608065134258263), 'median_relative_error': np.float64(0.02043972244924122)}
[18/45] Running with max_contemp_tokens=2, eval_temp=0.9, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:16<00:00,  1.36s/it]
Average time taken for each sample: 1.338127405643463, Average time taken for contemplation: 0.021069650650024415
Evaluation results: {'numerical_accuracy': 0.45, 'close_match_rate': 0.47, 'mean_relative_error': np.float64(0.4230363517493002), 'median_relative_error': np.float64(0.03367003367003367)}
[19/45] Running with max_contemp_tokens=3, eval_temp=0.1, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.32it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:21<00:00,  1.42s/it]
Average time taken for each sample: 1.3973401045799256, Average time taken for contemplation: 0.02143965721130371
Evaluation results: {'numerical_accuracy': 0.44, 'close_match_rate': 0.46, 'mean_relative_error': np.float64(0.41742825933243854), 'median_relative_error': np.float64(0.039920556107249254)}
[20/45] Running with max_contemp_tokens=3, eval_temp=0.2, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:18<00:00,  1.38s/it]
Average time taken for each sample: 1.3650102233886718, Average time taken for contemplation: 0.021359264850616455
Evaluation results: {'numerical_accuracy': 0.43, 'close_match_rate': 0.45, 'mean_relative_error': np.float64(0.425205826163458), 'median_relative_error': np.float64(0.044490131578947364)}
[21/45] Running with max_contemp_tokens=3, eval_temp=0.3, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:19<00:00,  1.39s/it]
Average time taken for each sample: 1.371053740978241, Average time taken for contemplation: 0.02162126064300537
Evaluation results: {'numerical_accuracy': 0.45, 'close_match_rate': 0.48, 'mean_relative_error': np.float64(0.35920784650300513), 'median_relative_error': np.float64(0.03367003367003367)}
[22/45] Running with max_contemp_tokens=3, eval_temp=0.4, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:06<00:00,  1.26s/it]
Average time taken for each sample: 1.2464433598518372, Average time taken for contemplation: 0.02044545650482178
Evaluation results: {'numerical_accuracy': 0.45, 'close_match_rate': 0.47, 'mean_relative_error': np.float64(0.39884515738496), 'median_relative_error': np.float64(0.0373864430468204)}
[23/45] Running with max_contemp_tokens=3, eval_temp=0.5, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:06<00:00,  1.27s/it]
Average time taken for each sample: 1.2481941533088685, Average time taken for contemplation: 0.020995516777038575
Evaluation results: {'numerical_accuracy': 0.45, 'close_match_rate': 0.46, 'mean_relative_error': np.float64(0.37577182607445947), 'median_relative_error': np.float64(0.0373864430468204)}
[24/45] Running with max_contemp_tokens=3, eval_temp=0.6, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.42it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:07<00:00,  1.27s/it]
Average time taken for each sample: 1.2561765480041505, Average time taken for contemplation: 0.020697076320648194
Evaluation results: {'numerical_accuracy': 0.44, 'close_match_rate': 0.46, 'mean_relative_error': np.float64(0.3907819355812996), 'median_relative_error': np.float64(0.0493545183714002)}
[25/45] Running with max_contemp_tokens=3, eval_temp=0.7, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:10<00:00,  1.30s/it]
Average time taken for each sample: 1.2837676906585693, Average time taken for contemplation: 0.020365719795227052
Evaluation results: {'numerical_accuracy': 0.44, 'close_match_rate': 0.47, 'mean_relative_error': np.float64(0.33199227873587617), 'median_relative_error': np.float64(0.0373864430468204)}
[26/45] Running with max_contemp_tokens=3, eval_temp=0.8, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.20it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:12<00:00,  1.33s/it]
Average time taken for each sample: 1.3120134615898131, Average time taken for contemplation: 0.021380901336669922
Evaluation results: {'numerical_accuracy': 0.42, 'close_match_rate': 0.45, 'mean_relative_error': np.float64(0.4271635995374085), 'median_relative_error': np.float64(0.039920556107249254)}
[27/45] Running with max_contemp_tokens=3, eval_temp=0.9, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.41it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:20<00:00,  1.41s/it]
Average time taken for each sample: 1.3869138693809508, Average time taken for contemplation: 0.021002833843231202
Evaluation results: {'numerical_accuracy': 0.43, 'close_match_rate': 0.46, 'mean_relative_error': np.float64(0.4390890532708807), 'median_relative_error': np.float64(0.0373864430468204)}
[28/45] Running with max_contemp_tokens=4, eval_temp=0.1, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:26<00:00,  1.47s/it]
Average time taken for each sample: 1.4494828844070435, Average time taken for contemplation: 0.021275007724761964
Evaluation results: {'numerical_accuracy': 0.45, 'close_match_rate': 0.47, 'mean_relative_error': np.float64(0.516939932450883), 'median_relative_error': np.float64(0.0373864430468204)}
[29/45] Running with max_contemp_tokens=4, eval_temp=0.2, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:26<00:00,  1.46s/it]
Average time taken for each sample: 1.4424272322654723, Average time taken for contemplation: 0.02102163791656494
Evaluation results: {'numerical_accuracy': 0.44, 'close_match_rate': 0.45, 'mean_relative_error': np.float64(0.6197577812374839), 'median_relative_error': np.float64(0.0391390931372549)}
[30/45] Running with max_contemp_tokens=4, eval_temp=0.3, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.40it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:26<00:00,  1.46s/it]
Average time taken for each sample: 1.4418290066719055, Average time taken for contemplation: 0.021668355464935302
Evaluation results: {'numerical_accuracy': 0.48, 'close_match_rate': 0.49, 'mean_relative_error': np.float64(0.5906465279988753), 'median_relative_error': np.float64(0.02096854317901383)}
[31/45] Running with max_contemp_tokens=4, eval_temp=0.4, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.43it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:26<00:00,  1.47s/it]
Average time taken for each sample: 1.4440626215934753, Average time taken for contemplation: 0.021000428199768065
Evaluation results: {'numerical_accuracy': 0.44, 'close_match_rate': 0.46, 'mean_relative_error': np.float64(0.36880942466409566), 'median_relative_error': np.float64(0.03414351851851852)}
[32/45] Running with max_contemp_tokens=4, eval_temp=0.5, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:26<00:00,  1.47s/it]
Average time taken for each sample: 1.4460073328018188, Average time taken for contemplation: 0.02154374361038208
Evaluation results: {'numerical_accuracy': 0.42, 'close_match_rate': 0.43, 'mean_relative_error': np.float64(0.4280613312385145), 'median_relative_error': np.float64(0.0430453431372549)}
[33/45] Running with max_contemp_tokens=4, eval_temp=0.6, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.43it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:26<00:00,  1.47s/it]
Average time taken for each sample: 1.448990068435669, Average time taken for contemplation: 0.02137350082397461
Evaluation results: {'numerical_accuracy': 0.44, 'close_match_rate': 0.46, 'mean_relative_error': np.float64(0.3967948665453475), 'median_relative_error': np.float64(0.03367003367003367)}
[34/45] Running with max_contemp_tokens=4, eval_temp=0.7, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:27<00:00,  1.47s/it]
Average time taken for each sample: 1.4515626001358033, Average time taken for contemplation: 0.021588423252105714
Evaluation results: {'numerical_accuracy': 0.4, 'close_match_rate': 0.41, 'mean_relative_error': np.float64(0.49960373552695997), 'median_relative_error': np.float64(0.06082372526049695)}
[35/45] Running with max_contemp_tokens=4, eval_temp=0.8, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:26<00:00,  1.47s/it]
Average time taken for each sample: 1.4453931832313538, Average time taken for contemplation: 0.021318283081054688
Evaluation results: {'numerical_accuracy': 0.41, 'close_match_rate': 0.42, 'mean_relative_error': np.float64(0.4797840356270424), 'median_relative_error': np.float64(0.060241510392567796)}
[36/45] Running with max_contemp_tokens=4, eval_temp=0.9, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:13<00:00,  1.34s/it]
Average time taken for each sample: 1.3186201643943787, Average time taken for contemplation: 0.021067988872528077
Evaluation results: {'numerical_accuracy': 0.42, 'close_match_rate': 0.42, 'mean_relative_error': np.float64(0.38987337300179703), 'median_relative_error': np.float64(0.04225852272727273)}
[37/45] Running with max_contemp_tokens=5, eval_temp=0.1, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:20<00:00,  1.40s/it]
Average time taken for each sample: 1.382747073173523, Average time taken for contemplation: 0.021051342487335204
Evaluation results: {'numerical_accuracy': 0.39, 'close_match_rate': 0.41, 'mean_relative_error': np.float64(0.28546598515132937), 'median_relative_error': np.float64(0.05211141060197664)}
[38/45] Running with max_contemp_tokens=5, eval_temp=0.2, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:16<00:00,  1.37s/it]
Average time taken for each sample: 1.3531719589233397, Average time taken for contemplation: 0.020555448532104493
Evaluation results: {'numerical_accuracy': 0.37, 'close_match_rate': 0.39, 'mean_relative_error': np.float64(0.34011439409117034), 'median_relative_error': np.float64(0.04134697357203751)}
[39/45] Running with max_contemp_tokens=5, eval_temp=0.3, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:16<00:00,  1.37s/it]
Average time taken for each sample: 1.3523405838012694, Average time taken for contemplation: 0.02039179563522339
Evaluation results: {'numerical_accuracy': 0.34, 'close_match_rate': 0.36, 'mean_relative_error': np.float64(0.42225521995225196), 'median_relative_error': np.float64(0.06907378335949765)}
[40/45] Running with max_contemp_tokens=5, eval_temp=0.4, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.40it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:33<00:00,  1.54s/it]
Average time taken for each sample: 1.51391343832016, Average time taken for contemplation: 0.021310956478118898
Evaluation results: {'numerical_accuracy': 0.39, 'close_match_rate': 0.41, 'mean_relative_error': np.float64(0.3158241802530902), 'median_relative_error': np.float64(0.06091527520098949)}
[41/45] Running with max_contemp_tokens=5, eval_temp=0.5, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.43it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:31<00:00,  1.51s/it]
Average time taken for each sample: 1.4911260938644408, Average time taken for contemplation: 0.021195483207702637
Evaluation results: {'numerical_accuracy': 0.38, 'close_match_rate': 0.4, 'mean_relative_error': np.float64(0.3261691494436295), 'median_relative_error': np.float64(0.05115089514066496)}
[42/45] Running with max_contemp_tokens=5, eval_temp=0.6, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.42it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:30<00:00,  1.51s/it]
Average time taken for each sample: 1.4864738488197327, Average time taken for contemplation: 0.02099402904510498
Evaluation results: {'numerical_accuracy': 0.37, 'close_match_rate': 0.38, 'mean_relative_error': np.float64(0.43079441190807727), 'median_relative_error': np.float64(0.059714795008912656)}
[43/45] Running with max_contemp_tokens=5, eval_temp=0.7, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.40it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:32<00:00,  1.52s/it]
Average time taken for each sample: 1.5012890052795411, Average time taken for contemplation: 0.021883525848388673
Evaluation results: {'numerical_accuracy': 0.39, 'close_match_rate': 0.4, 'mean_relative_error': np.float64(0.3185625578818771), 'median_relative_error': np.float64(0.05115089514066496)}
[44/45] Running with max_contemp_tokens=5, eval_temp=0.8, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:31<00:00,  1.52s/it]
Average time taken for each sample: 1.4957045125961304, Average time taken for contemplation: 0.02100806713104248
Evaluation results: {'numerical_accuracy': 0.39, 'close_match_rate': 0.4, 'mean_relative_error': np.float64(0.3098751489098487), 'median_relative_error': np.float64(0.04060705496308449)}
[45/45] Running with max_contemp_tokens=5, eval_temp=0.9, dataset=svamp, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:32<00:00,  1.53s/it]
Average time taken for each sample: 1.5066587042808532, Average time taken for contemplation: 0.021543488502502442
Evaluation results: {'numerical_accuracy': 0.4, 'close_match_rate': 0.41, 'mean_relative_error': np.float64(0.2942890941802922), 'median_relative_error': np.float64(0.037027244060559766)}
[46/45] Running with max_contemp_tokens=1, eval_temp=0.1, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:17<00:00,  1.37s/it]
Average time taken for each sample: 1.3512493419647216, Average time taken for contemplation: 0.020707757472991944
Evaluation results: {'numerical_accuracy': 0.12, 'close_match_rate': 0.13, 'mean_relative_error': np.float64(0.7820608857129795), 'median_relative_error': np.float64(0.3333333333333333)}
[47/45] Running with max_contemp_tokens=1, eval_temp=0.2, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:17<00:00,  1.38s/it]
Average time taken for each sample: 1.3528937077522278, Average time taken for contemplation: 0.020951194763183592
Evaluation results: {'numerical_accuracy': 0.12, 'close_match_rate': 0.13, 'mean_relative_error': np.float64(0.7811021077967033), 'median_relative_error': np.float64(0.3333333333333333)}
[48/45] Running with max_contemp_tokens=1, eval_temp=0.3, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:17<00:00,  1.37s/it]
Average time taken for each sample: 1.3512481212615968, Average time taken for contemplation: 0.0207995867729187
Evaluation results: {'numerical_accuracy': 0.12, 'close_match_rate': 0.12, 'mean_relative_error': np.float64(1.471544121236009), 'median_relative_error': np.float64(0.3625)}
[49/45] Running with max_contemp_tokens=1, eval_temp=0.4, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:09<00:00,  1.29s/it]
Average time taken for each sample: 1.2707193732261657, Average time taken for contemplation: 0.020554678440093996
Evaluation results: {'numerical_accuracy': 0.1, 'close_match_rate': 0.1, 'mean_relative_error': np.float64(2.5252525252525256e+25), 'median_relative_error': np.float64(0.3605769230769231)}
[50/45] Running with max_contemp_tokens=1, eval_temp=0.5, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.42it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:03<00:00,  1.24s/it]
Average time taken for each sample: 1.2209851598739625, Average time taken for contemplation: 0.020625362396240233
Evaluation results: {'numerical_accuracy': 0.1, 'close_match_rate': 0.1, 'mean_relative_error': np.float64(1.3641618632625898), 'median_relative_error': np.float64(0.35222130810366103)}
[51/45] Running with max_contemp_tokens=1, eval_temp=0.6, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:06<00:00,  1.27s/it]
Average time taken for each sample: 1.246162748336792, Average time taken for contemplation: 0.02059685230255127
Evaluation results: {'numerical_accuracy': 0.09, 'close_match_rate': 0.09, 'mean_relative_error': np.float64(1.2557948919885076), 'median_relative_error': np.float64(0.3730769230769231)}
[52/45] Running with max_contemp_tokens=1, eval_temp=0.7, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:03<00:00,  1.24s/it]
Average time taken for each sample: 1.2175002217292785, Average time taken for contemplation: 0.0219775652885437
Evaluation results: {'numerical_accuracy': 0.09, 'close_match_rate': 0.09, 'mean_relative_error': np.float64(1.7331340054143842), 'median_relative_error': np.float64(0.375)}
[53/45] Running with max_contemp_tokens=1, eval_temp=0.8, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:56<00:00,  1.16s/it]
Average time taken for each sample: 1.147939383983612, Average time taken for contemplation: 0.020769596099853516
Evaluation results: {'numerical_accuracy': 0.1, 'close_match_rate': 0.1, 'mean_relative_error': np.float64(1.4879622234536387), 'median_relative_error': np.float64(0.4)}
[54/45] Running with max_contemp_tokens=1, eval_temp=0.9, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:15<00:00,  1.36s/it]
Average time taken for each sample: 1.335927014350891, Average time taken for contemplation: 0.021689867973327635
Evaluation results: {'numerical_accuracy': 0.11, 'close_match_rate': 0.11, 'mean_relative_error': np.float64(1.4809663273616431), 'median_relative_error': np.float64(0.4)}
[55/45] Running with max_contemp_tokens=2, eval_temp=0.1, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:20<00:00,  1.41s/it]
Average time taken for each sample: 1.3841940689086913, Average time taken for contemplation: 0.021206076145172118
Evaluation results: {'numerical_accuracy': 0.11, 'close_match_rate': 0.11, 'mean_relative_error': np.float64(2.525252525252525e+26), 'median_relative_error': np.float64(0.3333333333333333)}
[56/45] Running with max_contemp_tokens=2, eval_temp=0.2, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:14<00:00,  1.35s/it]
Average time taken for each sample: 1.3250686144828796, Average time taken for contemplation: 0.021280438899993898
Evaluation results: {'numerical_accuracy': 0.12, 'close_match_rate': 0.12, 'mean_relative_error': np.float64(2.525252525252525e+26), 'median_relative_error': np.float64(0.3405797101449275)}
[57/45] Running with max_contemp_tokens=2, eval_temp=0.3, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.43it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:14<00:00,  1.34s/it]
Average time taken for each sample: 1.3230515575408937, Average time taken for contemplation: 0.020761613845825196
Evaluation results: {'numerical_accuracy': 0.11, 'close_match_rate': 0.11, 'mean_relative_error': np.float64(2.5252499999999998e+26), 'median_relative_error': np.float64(0.3405797101449275)}
[58/45] Running with max_contemp_tokens=2, eval_temp=0.4, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.50it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:16<00:00,  1.37s/it]
Average time taken for each sample: 1.347320520877838, Average time taken for contemplation: 0.020366618633270262
Evaluation results: {'numerical_accuracy': 0.11, 'close_match_rate': 0.11, 'mean_relative_error': np.float64(2.525252272727273e+26), 'median_relative_error': np.float64(0.3333333333333333)}
[59/45] Running with max_contemp_tokens=2, eval_temp=0.5, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:15<00:00,  1.35s/it]
Average time taken for each sample: 1.3296881985664368, Average time taken for contemplation: 0.021675848960876466
Evaluation results: {'numerical_accuracy': 0.12, 'close_match_rate': 0.12, 'mean_relative_error': np.float64(2.5252499999999998e+26), 'median_relative_error': np.float64(0.34782608695652173)}
[60/45] Running with max_contemp_tokens=2, eval_temp=0.6, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:15<00:00,  1.35s/it]
Average time taken for each sample: 1.3277720189094544, Average time taken for contemplation: 0.02119778871536255
Evaluation results: {'numerical_accuracy': 0.09, 'close_match_rate': 0.09, 'mean_relative_error': np.float64(0.855293216056594), 'median_relative_error': np.float64(0.3333333333333333)}
[61/45] Running with max_contemp_tokens=2, eval_temp=0.7, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.50it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:17<00:00,  1.38s/it]
Average time taken for each sample: 1.355737998485565, Average time taken for contemplation: 0.021065266132354737
Evaluation results: {'numerical_accuracy': 0.11, 'close_match_rate': 0.13, 'mean_relative_error': np.float64(1.184879278786284), 'median_relative_error': np.float64(0.3333333333333333)}
[62/45] Running with max_contemp_tokens=2, eval_temp=0.8, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:15<00:00,  1.36s/it]
Average time taken for each sample: 1.3360451889038085, Average time taken for contemplation: 0.020876471996307374
Evaluation results: {'numerical_accuracy': 0.09, 'close_match_rate': 0.1, 'mean_relative_error': np.float64(0.7092770391069567), 'median_relative_error': np.float64(0.40454545454545454)}
[63/45] Running with max_contemp_tokens=2, eval_temp=0.9, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:15<00:00,  1.36s/it]
Average time taken for each sample: 1.3348667621612549, Average time taken for contemplation: 0.021014714241027833
Evaluation results: {'numerical_accuracy': 0.14, 'close_match_rate': 0.15, 'mean_relative_error': np.float64(0.731181438795778), 'median_relative_error': np.float64(0.3570290004113533)}
[64/45] Running with max_contemp_tokens=3, eval_temp=0.1, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:08<00:00,  1.29s/it]
Average time taken for each sample: 1.270400722026825, Average time taken for contemplation: 0.02035435676574707
Evaluation results: {'numerical_accuracy': 0.12, 'close_match_rate': 0.12, 'mean_relative_error': np.float64(3.4435570685479546e+27), 'median_relative_error': np.float64(0.4240384615384616)}
[65/45] Running with max_contemp_tokens=3, eval_temp=0.2, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.42it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:11<00:00,  1.32s/it]
Average time taken for each sample: 1.2971459364891051, Average time taken for contemplation: 0.021135385036468505
Evaluation results: {'numerical_accuracy': 0.14, 'close_match_rate': 0.14, 'mean_relative_error': np.float64(3.443560454581818e+27), 'median_relative_error': np.float64(0.39230769230769236)}
[66/45] Running with max_contemp_tokens=3, eval_temp=0.3, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:08<00:00,  1.28s/it]
Average time taken for each sample: 1.2635075545310974, Average time taken for contemplation: 0.02040663719177246
Evaluation results: {'numerical_accuracy': 0.12, 'close_match_rate': 0.13, 'mean_relative_error': np.float64(3.4125363636363636e+27), 'median_relative_error': np.float64(0.3405797101449275)}
[67/45] Running with max_contemp_tokens=3, eval_temp=0.4, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.43it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:07<00:00,  1.28s/it]
Average time taken for each sample: 1.261452705860138, Average time taken for contemplation: 0.02053846836090088
Evaluation results: {'numerical_accuracy': 0.1, 'close_match_rate': 0.11, 'mean_relative_error': np.float64(3.7537875250365913e+27), 'median_relative_error': np.float64(0.4647727272727272)}
[68/45] Running with max_contemp_tokens=3, eval_temp=0.5, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:21<00:00,  1.41s/it]
Average time taken for each sample: 1.389297890663147, Average time taken for contemplation: 0.02077955484390259
Evaluation results: {'numerical_accuracy': 0.11, 'close_match_rate': 0.11, 'mean_relative_error': np.float64(26.167022586607406), 'median_relative_error': np.float64(0.45178571428571423)}
[69/45] Running with max_contemp_tokens=3, eval_temp=0.6, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:23<00:00,  1.44s/it]
Average time taken for each sample: 1.4134098529815673, Average time taken for contemplation: 0.020938727855682373
Evaluation results: {'numerical_accuracy': 0.13, 'close_match_rate': 0.13, 'mean_relative_error': np.float64(55004.40111912734), 'median_relative_error': np.float64(0.36397058823529416)}
[70/45] Running with max_contemp_tokens=3, eval_temp=0.7, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:23<00:00,  1.43s/it]
Average time taken for each sample: 1.4064965415000916, Average time taken for contemplation: 0.021568610668182372
Evaluation results: {'numerical_accuracy': 0.12, 'close_match_rate': 0.12, 'mean_relative_error': np.float64(1.030165541374547), 'median_relative_error': np.float64(0.3333333333333333)}
[71/45] Running with max_contemp_tokens=3, eval_temp=0.8, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:23<00:00,  1.43s/it]
Average time taken for each sample: 1.4110840845108032, Average time taken for contemplation: 0.020843439102172852
Evaluation results: {'numerical_accuracy': 0.11, 'close_match_rate': 0.11, 'mean_relative_error': np.float64(50.68459454273446), 'median_relative_error': np.float64(0.3038461538461539)}
[72/45] Running with max_contemp_tokens=3, eval_temp=0.9, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:25<00:00,  1.45s/it]
Average time taken for each sample: 1.4307462763786316, Average time taken for contemplation: 0.020577917098999022
Evaluation results: {'numerical_accuracy': 0.09, 'close_match_rate': 0.09, 'mean_relative_error': np.float64(0.5227123205353972), 'median_relative_error': np.float64(0.3543552036199095)}
[73/45] Running with max_contemp_tokens=4, eval_temp=0.1, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:30<00:00,  1.51s/it]
Average time taken for each sample: 1.4812085461616515, Average time taken for contemplation: 0.021099355220794678
Evaluation results: {'numerical_accuracy': 0.09, 'close_match_rate': 0.11, 'mean_relative_error': np.float64(6.146262225246728e+28), 'median_relative_error': np.float64(0.3333333333333333)}
[74/45] Running with max_contemp_tokens=4, eval_temp=0.2, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.43it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:29<00:00,  1.50s/it]
Average time taken for each sample: 1.4776797413825988, Average time taken for contemplation: 0.021661927700042726
Evaluation results: {'numerical_accuracy': 0.12, 'close_match_rate': 0.12, 'mean_relative_error': np.float64(6.146247207824331e+28), 'median_relative_error': np.float64(0.3764705882352941)}
[75/45] Running with max_contemp_tokens=4, eval_temp=0.3, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:29<00:00,  1.49s/it]
Average time taken for each sample: 1.4700201272964477, Average time taken for contemplation: 0.020537750720977785
Evaluation results: {'numerical_accuracy': 0.08, 'close_match_rate': 0.08, 'mean_relative_error': np.float64(5.140268927891639e+28), 'median_relative_error': np.float64(0.44143356643356646)}
[76/45] Running with max_contemp_tokens=4, eval_temp=0.4, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:32<00:00,  1.52s/it]
Average time taken for each sample: 1.4974347138404847, Average time taken for contemplation: 0.021444852352142333
Evaluation results: {'numerical_accuracy': 0.14, 'close_match_rate': 0.15, 'mean_relative_error': np.float64(5.2539393822197175e+28), 'median_relative_error': np.float64(0.3333333333333333)}
[77/45] Running with max_contemp_tokens=4, eval_temp=0.5, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:23<00:00,  1.44s/it]
Average time taken for each sample: 1.4179001832008362, Average time taken for contemplation: 0.0327906346321106
Evaluation results: {'numerical_accuracy': 0.05, 'close_match_rate': 0.08, 'mean_relative_error': np.float64(5.000000000005e+28), 'median_relative_error': np.float64(0.3333333333333333)}
[78/45] Running with max_contemp_tokens=4, eval_temp=0.6, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:17<00:00,  1.37s/it]
Average time taken for each sample: 1.3541922378540039, Average time taken for contemplation: 0.02075505495071411
Evaluation results: {'numerical_accuracy': 0.09, 'close_match_rate': 0.1, 'mean_relative_error': np.float64(5e+23), 'median_relative_error': np.float64(0.35544330091587195)}
[79/45] Running with max_contemp_tokens=4, eval_temp=0.7, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference:   1%|█▌                                                                                      Running inference:   2%|███▏
         Running inference:   3%|████▊                                                                                   Running inference:   4%|██████▎
                  Running inference:   5%|███████▉                                                                                Running inference:   6%|█████████▌
                           Running inference:   7%|███████████▏                                                                            Running inference:   8%|████████████▋
                                    Running inference:   9%|██████████████▎                                                                         Running inference:  10%|███████████████▊
                                             Running inference:  11%|█████████████████▍                                                                      Running inference:  12%|██████████████████▉
                                                      Running inference:  13%|████████████████████▌                                                                   Running inference:  14%|██████████████████████
                                                               Running inference:  15%|███████████████████████▋                                                                Running inference:  16%|████████████████
█████████▎                                                              Running inference:  17%|██████████████████████████▊                                                             Running inference:  18%|███████
█████████████████████▍                                                           Running inference:  19%|██████████████████████████████                                                          Running inference:  20
%|███████████████████████████████▌                                                        Running inference:  21%|█████████████████████████████████▏                                                      Running infer
ence:  22%|██████████████████████████████████▊                                                     Running inference:  23%|████████████████████████████████████▎                                                   Runn
ing inference:  24%|█████████████████████████████████████▉                                                  Running inference:  25%|███████████████████████████████████████▌
     Running inference:  26%|█████████████████████████████████████████                                               Running inference:  27%|██████████████████████████████████████████▋
              Running inference:  28%|████████████████████████████████████████████▏                                           Running inference:  29%|█████████████████████████████████████████████▊
                       Running inference:  30%|███████████████████████████████████████████████▍                                        Running inference:  31%|████████████████████████████████████████████████▉
                                Running inference:  32%|██████████████████████████████████████████████████▌                                     Running inference:  33%|███████████████████████████████████████████████
█████▏                                   Running inference:  34%|█████████████████████████████████████████████████████▋                                  Running inference:  35%|██████████████████████████████████████
█████████████████▎                                Running inference:  36%|████████████████████████████████████████████████████████▉                               Running inference:  37%|█████████████████████████████
█████████████████████████████▍                             Running inference:  38%|████████████████████████████████████████████████████████████                            Running inference:  39%|████████████████████
█████████████████████████████████████████▌                          Running inference:  40%|███████████████████████████████████████████████████████████████▏                        Running inference:  41%|███████████
█████████████████████████████████████████████████████▊                       Running inference:  42%|██████████████████████████████████████████████████████████████████▎                     Running inference:  43%|██
█████████████████████████████████████████████████████████████████▉                    Running inference:  44%|█████████████████████████████████████████████████████████████████████▌                  Running inference
:  45%|███████████████████████████████████████████████████████████████████████                 Running inference:  46%|████████████████████████████████████████████████████████████████████████▋               Running
inference:  47%|██████████████████████████████████████████████████████████████████████████▎             Running inference:  48%|███████████████████████████████████████████████████████████████████████████▊
 Running inference:  49%|█████████████████████████████████████████████████████████████████████████████▍          Running inference:  50%|██████████████████████████████████████████████████████████████████████████████
█         Running inference:  51%|████████████████████████████████████████████████████████████████████████████████▌       Running inference:  52%|█████████████████████████████████████████████████████████████████████
█████████████▏     Running inference:  53%|███████████████████████████████████████████████████████████████████████████████████▋    Running inference:  54%|████████████████████████████████████████████████████████████
█████████████████████████▎  Running inference:  55%|██████████████████████████████████████████████████████████████████████████████████████▉ Running inference:  56%|███████████████████████████████████████████████████
█████████████████████████████████████Running inference:  57%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  58%|██████████████████████████████████████████
██████████████████████████████████████████████Running inference:  59%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  60%|█████████████████████████████████
███████████████████████████████████████████████████████Running inference:  61%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  62%|████████████████████████
████████████████████████████████████████████████████████████████Running inference:  63%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  64%|███████████████
█████████████████████████████████████████████████████████████████████████Running inference:  65%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  66%|██████
██████████████████████████████████████████████████████████████████████████████████Running inference:  67%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  6
8%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  69%|████████████████████████████████████████████████████████████████████████████████████████Running infe
rence:  70%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  71%|████████████████████████████████████████████████████████████████████████████████████████Run
ning inference:  72%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  73%|██████████████████████████████████████████████████████████████████████████████████
██████Running inference:  74%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  75%|█████████████████████████████████████████████████████████████████████████
███████████████Running inference:  76%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  77%|████████████████████████████████████████████████████████████████
████████████████████████Running inference:  78%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  79%|███████████████████████████████████████████████████████
█████████████████████████████████Running inference:  80%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  81%|██████████████████████████████████████████████
██████████████████████████████████████████Running inference:  82%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  83%|█████████████████████████████████████
███████████████████████████████████████████████████Running inference:  84%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  85%|████████████████████████████
████████████████████████████████████████████████████████████Running inference:  86%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  87%|███████████████████
█████████████████████████████████████████████████████████████████████Running inference:  88%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  89%|██████████
██████████████████████████████████████████████████████████████████████████████Running inference:  90%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  91%|█
███████████████████████████████████████████████████████████████████████████████████████Running inference:  92%|████████████████████████████████████████████████████████████████████████████████████████Running inferenc
e:  93%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  94%|████████████████████████████████████████████████████████████████████████████████████████Running
 inference:  95%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  96%|██████████████████████████████████████████████████████████████████████████████████████
██Running inference:  97%|████████████████████████████████████████████████████████████████████████████████████████Running inference:  98%|█████████████████████████████████████████████████████████████████████████████
███████████Running inference:  99%|████████████████████████████████████████████████████████████████████████████████████████Running inference: 100%|████████████████████████████████████████████████████████████████████
████████████████████Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:
16<00:00,  1.37s/it]
Average time taken for each sample: 1.3489267492294312, Average time taken for contemplation: 0.021024293899536133
Evaluation results: {'numerical_accuracy': 0.06, 'close_match_rate': 0.07, 'mean_relative_error': np.float64(5e+23), 'median_relative_error': np.float64(0.3431372549019608)}
[80/45] Running with max_contemp_tokens=4, eval_temp=0.8, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:18<00:00,  1.38s/it]
Average time taken for each sample: 1.3647169971466064, Average time taken for contemplation: 0.020828661918640138
Evaluation results: {'numerical_accuracy': 0.11, 'close_match_rate': 0.13, 'mean_relative_error': np.float64(5.0500000000055496e+23), 'median_relative_error': np.float64(0.3333333333333333)}
[81/45] Running with max_contemp_tokens=4, eval_temp=0.9, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:17<00:00,  1.37s/it]
Average time taken for each sample: 1.3538740515708922, Average time taken for contemplation: 0.02078620910644531
Evaluation results: {'numerical_accuracy': 0.06, 'close_match_rate': 0.07, 'mean_relative_error': np.float64(900212.8219990412), 'median_relative_error': np.float64(0.38580246913580246)}
[82/45] Running with max_contemp_tokens=5, eval_temp=0.1, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:34<00:00,  1.55s/it]
Average time taken for each sample: 1.5235298562049866, Average time taken for contemplation: 0.020620746612548826
Evaluation results: {'numerical_accuracy': 0.09, 'close_match_rate': 0.11, 'mean_relative_error': np.float64(4.43982671911422e+30), 'median_relative_error': np.float64(0.6458333333333333)}
[83/45] Running with max_contemp_tokens=5, eval_temp=0.2, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:32<00:00,  1.53s/it]
Average time taken for each sample: 1.501785273551941, Average time taken for contemplation: 0.020850217342376708
Evaluation results: {'numerical_accuracy': 0.08, 'close_match_rate': 0.1, 'mean_relative_error': np.float64(4.440336402600458e+30), 'median_relative_error': np.float64(0.6666666666666666)}
[84/45] Running with max_contemp_tokens=5, eval_temp=0.3, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.43it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:33<00:00,  1.53s/it]
Average time taken for each sample: 1.5087774896621704, Average time taken for contemplation: 0.020602977275848387
Evaluation results: {'numerical_accuracy': 0.12, 'close_match_rate': 0.14, 'mean_relative_error': np.float64(6.666788811316849e+30), 'median_relative_error': np.float64(0.5416127989657402)}
[85/45] Running with max_contemp_tokens=5, eval_temp=0.4, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:35<00:00,  1.56s/it]
Average time taken for each sample: 1.5326383209228516, Average time taken for contemplation: 0.020563602447509766
Evaluation results: {'numerical_accuracy': 0.11, 'close_match_rate': 0.12, 'mean_relative_error': np.float64(2.702531347036947e+30), 'median_relative_error': np.float64(0.49866310160427807)}
[86/45] Running with max_contemp_tokens=5, eval_temp=0.5, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:32<00:00,  1.53s/it]
Average time taken for each sample: 1.506613245010376, Average time taken for contemplation: 0.02140615463256836
Evaluation results: {'numerical_accuracy': 0.1, 'close_match_rate': 0.11, 'mean_relative_error': np.float64(3.1170667005807557e+30), 'median_relative_error': np.float64(0.4733909702209414)}
[87/45] Running with max_contemp_tokens=5, eval_temp=0.6, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:35<00:00,  1.55s/it]
Average time taken for each sample: 1.5271042275428772, Average time taken for contemplation: 0.021578183174133302
Evaluation results: {'numerical_accuracy': 0.07, 'close_match_rate': 0.08, 'mean_relative_error': np.float64(9.076695707350399e+29), 'median_relative_error': np.float64(0.4118155619596542)}
[88/45] Running with max_contemp_tokens=5, eval_temp=0.7, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:34<00:00,  1.55s/it]
Average time taken for each sample: 1.5260974955558777, Average time taken for contemplation: 0.020870778560638428
Evaluation results: {'numerical_accuracy': 0.07, 'close_match_rate': 0.08, 'mean_relative_error': np.float64(9.076695707350399e+29), 'median_relative_error': np.float64(0.48333333333333334)}
[89/45] Running with max_contemp_tokens=5, eval_temp=0.8, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:34<00:00,  1.54s/it]
Average time taken for each sample: 1.5220723128318787, Average time taken for contemplation: 0.02077146053314209
Evaluation results: {'numerical_accuracy': 0.06, 'close_match_rate': 0.06, 'mean_relative_error': np.float64(4.9105218461543456e+29), 'median_relative_error': np.float64(0.4527972027972028)}
[90/45] Running with max_contemp_tokens=5, eval_temp=0.9, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:30<00:00,  1.51s/it]
Average time taken for each sample: 1.4844652009010315, Average time taken for contemplation: 0.021279280185699464
Evaluation results: {'numerical_accuracy': 0.07, 'close_match_rate': 0.07, 'mean_relative_error': np.float64(3.647521346153846e+29), 'median_relative_error': np.float64(0.3795612633683158)}
[91/45] Running with max_contemp_tokens=1, eval_temp=0.1, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:57<00:00,  1.17s/it]
Average time taken for each sample: 1.154264748096466, Average time taken for contemplation: 0.020890357494354247
Evaluation results: {'numerical_accuracy': 0.17, 'close_match_rate': 0.17, 'mean_relative_error': np.float64(0.3602620327417214), 'median_relative_error': np.float64(0.2857142857142857)}
[92/45] Running with max_contemp_tokens=1, eval_temp=0.2, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:01<00:00,  1.22s/it]
Average time taken for each sample: 1.2006803226470948, Average time taken for contemplation: 0.021469271183013915
Evaluation results: {'numerical_accuracy': 0.19, 'close_match_rate': 0.19, 'mean_relative_error': np.float64(0.33001779128772385), 'median_relative_error': np.float64(0.25227272727272726)}
[93/45] Running with max_contemp_tokens=1, eval_temp=0.3, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.43it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:56<00:00,  1.17s/it]
Average time taken for each sample: 1.1498370265960693, Average time taken for contemplation: 0.020263850688934326
Evaluation results: {'numerical_accuracy': 0.19, 'close_match_rate': 0.19, 'mean_relative_error': np.float64(0.33759987260314617), 'median_relative_error': np.float64(0.2596256684491979)}
[94/45] Running with max_contemp_tokens=1, eval_temp=0.4, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:52<00:00,  1.13s/it]
Average time taken for each sample: 1.109138479232788, Average time taken for contemplation: 0.020175747871398926
Evaluation results: {'numerical_accuracy': 0.17, 'close_match_rate': 0.17, 'mean_relative_error': np.float64(0.33731788076233776), 'median_relative_error': np.float64(0.27297794117647056)}
[95/45] Running with max_contemp_tokens=1, eval_temp=0.5, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:09<00:00,  1.30s/it]
Average time taken for each sample: 1.275490882396698, Average time taken for contemplation: 0.02134188175201416
Evaluation results: {'numerical_accuracy': 0.17, 'close_match_rate': 0.17, 'mean_relative_error': np.float64(0.33467899187344885), 'median_relative_error': np.float64(0.25227272727272726)}
[96/45] Running with max_contemp_tokens=1, eval_temp=0.6, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.43it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:08<00:00,  1.28s/it]
Average time taken for each sample: 1.2603392243385314, Average time taken for contemplation: 0.020438654422760008
Evaluation results: {'numerical_accuracy': 0.16, 'close_match_rate': 0.16, 'mean_relative_error': np.float64(0.35552338637455966), 'median_relative_error': np.float64(0.27297794117647056)}
[97/45] Running with max_contemp_tokens=1, eval_temp=0.7, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.43it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:10<00:00,  1.31s/it]
Average time taken for each sample: 1.2870580077171325, Average time taken for contemplation: 0.02121790647506714
Evaluation results: {'numerical_accuracy': 0.16, 'close_match_rate': 0.16, 'mean_relative_error': np.float64(0.33932052670342705), 'median_relative_error': np.float64(0.25)}
[98/45] Running with max_contemp_tokens=1, eval_temp=0.8, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.43it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:07<00:00,  1.28s/it]
Average time taken for each sample: 1.2543848371505737, Average time taken for contemplation: 0.020995903015136718
Evaluation results: {'numerical_accuracy': 0.17, 'close_match_rate': 0.17, 'mean_relative_error': np.float64(0.432876363379637), 'median_relative_error': np.float64(0.2596256684491979)}
[99/45] Running with max_contemp_tokens=1, eval_temp=0.9, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:07<00:00,  1.27s/it]
Average time taken for each sample: 1.2553706097602844, Average time taken for contemplation: 0.020722367763519288
Evaluation results: {'numerical_accuracy': 0.16, 'close_match_rate': 0.16, 'mean_relative_error': np.float64(0.3663267469106762), 'median_relative_error': np.float64(0.25227272727272726)}
[100/45] Running with max_contemp_tokens=2, eval_temp=0.1, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:15<00:00,  1.35s/it]
Average time taken for each sample: 1.329297640323639, Average time taken for contemplation: 0.02129737854003906
Evaluation results: {'numerical_accuracy': 0.17, 'close_match_rate': 0.17, 'mean_relative_error': np.float64(0.3738961176248783), 'median_relative_error': np.float64(0.2596256684491979)}
[101/45] Running with max_contemp_tokens=2, eval_temp=0.2, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:14<00:00,  1.35s/it]
Average time taken for each sample: 1.3257405591011047, Average time taken for contemplation: 0.02071537494659424
Evaluation results: {'numerical_accuracy': 0.16, 'close_match_rate': 0.16, 'mean_relative_error': np.float64(0.38443976841852906), 'median_relative_error': np.float64(0.2596256684491979)}
[102/45] Running with max_contemp_tokens=2, eval_temp=0.3, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:16<00:00,  1.36s/it]
Average time taken for each sample: 1.34104380607605, Average time taken for contemplation: 0.02086979866027832
Evaluation results: {'numerical_accuracy': 0.17, 'close_match_rate': 0.17, 'mean_relative_error': np.float64(0.41378317501193573), 'median_relative_error': np.float64(0.25)}
[103/45] Running with max_contemp_tokens=2, eval_temp=0.4, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:14<00:00,  1.34s/it]
Average time taken for each sample: 1.322381191253662, Average time taken for contemplation: 0.021226425170898438
Evaluation results: {'numerical_accuracy': 0.15, 'close_match_rate': 0.15, 'mean_relative_error': np.float64(0.4562133796663195), 'median_relative_error': np.float64(0.2656862745098039)}
[104/45] Running with max_contemp_tokens=2, eval_temp=0.5, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:13<00:00,  1.34s/it]
Average time taken for each sample: 1.3164978075027465, Average time taken for contemplation: 0.02035160541534424
Evaluation results: {'numerical_accuracy': 0.17, 'close_match_rate': 0.17, 'mean_relative_error': np.float64(0.3949629523158922), 'median_relative_error': np.float64(0.25227272727272726)}
[105/45] Running with max_contemp_tokens=2, eval_temp=0.6, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.42it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:01<00:00,  1.21s/it]
Average time taken for each sample: 1.1940651416778565, Average time taken for contemplation: 0.02061680793762207
Evaluation results: {'numerical_accuracy': 0.16, 'close_match_rate': 0.16, 'mean_relative_error': np.float64(0.4509887754911211), 'median_relative_error': np.float64(0.25)}
[106/45] Running with max_contemp_tokens=2, eval_temp=0.7, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.43it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:01<00:00,  1.22s/it]
Average time taken for each sample: 1.2030757665634155, Average time taken for contemplation: 0.02037670135498047
Evaluation results: {'numerical_accuracy': 0.14, 'close_match_rate': 0.14, 'mean_relative_error': np.float64(0.4592551599080997), 'median_relative_error': np.float64(0.25227272727272726)}
[107/45] Running with max_contemp_tokens=2, eval_temp=0.8, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:03<00:00,  1.24s/it]
Average time taken for each sample: 1.2199743866920472, Average time taken for contemplation: 0.021339142322540285
Evaluation results: {'numerical_accuracy': 0.15, 'close_match_rate': 0.15, 'mean_relative_error': np.float64(0.415107435656951), 'median_relative_error': np.float64(0.25)}
[108/45] Running with max_contemp_tokens=2, eval_temp=0.9, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [02:06<00:00,  1.26s/it]
Average time taken for each sample: 1.2429579925537109, Average time taken for contemplation: 0.021215574741363527
Evaluation results: {'numerical_accuracy': 0.15, 'close_match_rate': 0.15, 'mean_relative_error': np.float64(0.3970144591423989), 'median_relative_error': np.float64(0.25227272727272726)}
[109/45] Running with max_contemp_tokens=3, eval_temp=0.1, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:30<00:00,  1.10it/s]
Average time taken for each sample: 0.902034432888031, Average time taken for contemplation: 0.019421315193176268
Evaluation results: {'numerical_accuracy': 0.16, 'close_match_rate': 0.16, 'mean_relative_error': np.float64(0.5325817734896381), 'median_relative_error': np.float64(0.25227272727272726)}
[110/45] Running with max_contemp_tokens=3, eval_temp=0.2, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:26<00:00,  1.15it/s]
Average time taken for each sample: 0.8615239644050598, Average time taken for contemplation: 0.020075254440307617
Evaluation results: {'numerical_accuracy': 0.17, 'close_match_rate': 0.17, 'mean_relative_error': np.float64(0.48522651111105475), 'median_relative_error': np.float64(0.25)}
[111/45] Running with max_contemp_tokens=3, eval_temp=0.3, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:26<00:00,  1.16it/s]
Average time taken for each sample: 0.8608950471878052, Average time taken for contemplation: 0.020469911098480224
Evaluation results: {'numerical_accuracy': 0.19, 'close_match_rate': 0.19, 'mean_relative_error': np.float64(0.46727573060464783), 'median_relative_error': np.float64(0.23861480075901328)}
[112/45] Running with max_contemp_tokens=3, eval_temp=0.4, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:27<00:00,  1.14it/s]
Average time taken for each sample: 0.8697658991813659, Average time taken for contemplation: 0.019998352527618408
Evaluation results: {'numerical_accuracy': 0.18, 'close_match_rate': 0.18, 'mean_relative_error': np.float64(0.5122841544420189), 'median_relative_error': np.float64(0.25)}
[113/45] Running with max_contemp_tokens=3, eval_temp=0.5, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:26<00:00,  1.16it/s]
Average time taken for each sample: 0.8608048677444458, Average time taken for contemplation: 0.01956059217453003
Evaluation results: {'numerical_accuracy': 0.19, 'close_match_rate': 0.19, 'mean_relative_error': np.float64(0.42141202698407065), 'median_relative_error': np.float64(0.21764705882352942)}
[114/45] Running with max_contemp_tokens=3, eval_temp=0.6, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:26<00:00,  1.16it/s]
Average time taken for each sample: 0.857539894580841, Average time taken for contemplation: 0.019665024280548095
Evaluation results: {'numerical_accuracy': 0.22, 'close_match_rate': 0.22, 'mean_relative_error': np.float64(0.36652096678640667), 'median_relative_error': np.float64(0.2)}
[115/45] Running with max_contemp_tokens=3, eval_temp=0.7, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:26<00:00,  1.16it/s]
Average time taken for each sample: 0.8596163320541382, Average time taken for contemplation: 0.01932227611541748
Evaluation results: {'numerical_accuracy': 0.18, 'close_match_rate': 0.18, 'mean_relative_error': np.float64(0.3996297302662851), 'median_relative_error': np.float64(0.2)}
[116/45] Running with max_contemp_tokens=3, eval_temp=0.8, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:26<00:00,  1.16it/s]
Average time taken for each sample: 0.8581750273704529, Average time taken for contemplation: 0.020718977451324463
Evaluation results: {'numerical_accuracy': 0.17, 'close_match_rate': 0.17, 'mean_relative_error': np.float64(0.32491941758228826), 'median_relative_error': np.float64(0.23861480075901328)}
[117/45] Running with max_contemp_tokens=3, eval_temp=0.9, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:27<00:00,  1.15it/s]
Average time taken for each sample: 0.8684198594093323, Average time taken for contemplation: 0.02064589023590088
Evaluation results: {'numerical_accuracy': 0.21, 'close_match_rate': 0.21, 'mean_relative_error': np.float64(0.3555122955229173), 'median_relative_error': np.float64(0.2596256684491979)}
[118/45] Running with max_contemp_tokens=4, eval_temp=0.1, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:30<00:00,  1.11it/s]
Average time taken for each sample: 0.8966866874694824, Average time taken for contemplation: 0.019943907260894775
Evaluation results: {'numerical_accuracy': 0.17, 'close_match_rate': 0.17, 'mean_relative_error': np.float64(0.34537432185936234), 'median_relative_error': np.float64(0.25)}
[119/45] Running with max_contemp_tokens=4, eval_temp=0.2, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:29<00:00,  1.11it/s]
Average time taken for each sample: 0.8939916706085205, Average time taken for contemplation: 0.01985274314880371
Evaluation results: {'numerical_accuracy': 0.21, 'close_match_rate': 0.21, 'mean_relative_error': np.float64(0.3233429722125853), 'median_relative_error': np.float64(0.2148846960167715)}
[120/45] Running with max_contemp_tokens=4, eval_temp=0.3, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:28<00:00,  1.13it/s]
Average time taken for each sample: 0.8825359177589417, Average time taken for contemplation: 0.020197336673736573
Evaluation results: {'numerical_accuracy': 0.16, 'close_match_rate': 0.16, 'mean_relative_error': np.float64(0.4417176457849907), 'median_relative_error': np.float64(0.25)}
[121/45] Running with max_contemp_tokens=4, eval_temp=0.4, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:29<00:00,  1.12it/s]
Average time taken for each sample: 0.8894155883789062, Average time taken for contemplation: 0.020526623725891112
Evaluation results: {'numerical_accuracy': 0.18, 'close_match_rate': 0.18, 'mean_relative_error': np.float64(0.4248653920332485), 'median_relative_error': np.float64(0.25)}
[122/45] Running with max_contemp_tokens=4, eval_temp=0.5, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.42it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:30<00:00,  1.11it/s]
Average time taken for each sample: 0.8971973896026612, Average time taken for contemplation: 0.02054201126098633
Evaluation results: {'numerical_accuracy': 0.22, 'close_match_rate': 0.22, 'mean_relative_error': np.float64(0.398891988863831), 'median_relative_error': np.float64(0.2)}
[123/45] Running with max_contemp_tokens=4, eval_temp=0.6, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.40it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:28<00:00,  1.13it/s]
Average time taken for each sample: 0.881094069480896, Average time taken for contemplation: 0.020696141719818116
Evaluation results: {'numerical_accuracy': 0.18, 'close_match_rate': 0.18, 'mean_relative_error': np.float64(0.41873087578986623), 'median_relative_error': np.float64(0.22474132684114426)}
[124/45] Running with max_contemp_tokens=4, eval_temp=0.7, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.40it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:29<00:00,  1.12it/s]
Average time taken for each sample: 0.8861539316177368, Average time taken for contemplation: 0.020041654109954832
Evaluation results: {'numerical_accuracy': 0.19, 'close_match_rate': 0.19, 'mean_relative_error': np.float64(0.3452888279873211), 'median_relative_error': np.float64(0.2)}
[125/45] Running with max_contemp_tokens=4, eval_temp=0.8, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.42it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:30<00:00,  1.11it/s]
Average time taken for each sample: 0.8987919497489929, Average time taken for contemplation: 0.020076684951782227
Evaluation results: {'numerical_accuracy': 0.22, 'close_match_rate': 0.22, 'mean_relative_error': np.float64(0.3485071769778948), 'median_relative_error': np.float64(0.2)}
[126/45] Running with max_contemp_tokens=4, eval_temp=0.9, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.42it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:30<00:00,  1.10it/s]
Average time taken for each sample: 0.9007653665542602, Average time taken for contemplation: 0.020157384872436523
Evaluation results: {'numerical_accuracy': 0.19, 'close_match_rate': 0.19, 'mean_relative_error': np.float64(0.42746938861482564), 'median_relative_error': np.float64(0.2596256684491979)}
[127/45] Running with max_contemp_tokens=5, eval_temp=0.1, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:31<00:00,  1.09it/s]
Average time taken for each sample: 0.9085436415672302, Average time taken for contemplation: 0.019629161357879638
Evaluation results: {'numerical_accuracy': 0.17, 'close_match_rate': 0.17, 'mean_relative_error': np.float64(0.482868428081799), 'median_relative_error': np.float64(0.22875816993464052)}
[128/45] Running with max_contemp_tokens=5, eval_temp=0.2, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:31<00:00,  1.09it/s]
Average time taken for each sample: 0.9088297891616821, Average time taken for contemplation: 0.019673149585723877
Evaluation results: {'numerical_accuracy': 0.15, 'close_match_rate': 0.15, 'mean_relative_error': np.float64(0.5387458520914811), 'median_relative_error': np.float64(0.25)}
[129/45] Running with max_contemp_tokens=5, eval_temp=0.3, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.41it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:32<00:00,  1.09it/s]
Average time taken for each sample: 0.9152440309524537, Average time taken for contemplation: 0.019539566040039064
Evaluation results: {'numerical_accuracy': 0.15, 'close_match_rate': 0.15, 'mean_relative_error': np.float64(0.5102194140106591), 'median_relative_error': np.float64(0.25)}
[130/45] Running with max_contemp_tokens=5, eval_temp=0.4, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:31<00:00,  1.09it/s]
Average time taken for each sample: 0.9127468729019165, Average time taken for contemplation: 0.0195298171043396
Evaluation results: {'numerical_accuracy': 0.17, 'close_match_rate': 0.17, 'mean_relative_error': np.float64(0.4910950467862918), 'median_relative_error': np.float64(0.22431865828092243)}
[131/45] Running with max_contemp_tokens=5, eval_temp=0.5, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:32<00:00,  1.08it/s]
Average time taken for each sample: 0.9229354953765869, Average time taken for contemplation: 0.02014610290527344
Evaluation results: {'numerical_accuracy': 0.2, 'close_match_rate': 0.2, 'mean_relative_error': np.float64(0.35894755673154366), 'median_relative_error': np.float64(0.2)}
[132/45] Running with max_contemp_tokens=5, eval_temp=0.6, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:32<00:00,  1.08it/s]
Average time taken for each sample: 0.9240819573402405, Average time taken for contemplation: 0.02006988525390625
Evaluation results: {'numerical_accuracy': 0.19, 'close_match_rate': 0.19, 'mean_relative_error': np.float64(0.5309507849933399), 'median_relative_error': np.float64(0.18974358974358974)}
[133/45] Running with max_contemp_tokens=5, eval_temp=0.7, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:31<00:00,  1.10it/s]
Average time taken for each sample: 0.9081109809875488, Average time taken for contemplation: 0.019580638408660887
Evaluation results: {'numerical_accuracy': 0.13, 'close_match_rate': 0.13, 'mean_relative_error': np.float64(0.5760353328411941), 'median_relative_error': np.float64(0.2701298701298701)}
[134/45] Running with max_contemp_tokens=5, eval_temp=0.8, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:32<00:00,  1.08it/s]
Average time taken for each sample: 0.9178078293800354, Average time taken for contemplation: 0.019621779918670656
Evaluation results: {'numerical_accuracy': 0.16, 'close_match_rate': 0.16, 'mean_relative_error': np.float64(0.67642873554402), 'median_relative_error': np.float64(0.23861480075901328)}
[135/45] Running with max_contemp_tokens=5, eval_temp=0.9, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.42it/s]
Running inference:   0%|                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token.
 As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████| 100/100 [01:32<00:00,  1.08it/s]
Average time taken for each sample: 0.9189108300209046, Average time taken for contemplation: 0.019393863677978514
Evaluation results: {'numerical_accuracy': 0.19, 'close_match_rate': 0.19, 'mean_relative_error': np.float64(0.5827312176695721), 'median_relative_error': np.float64(0.20526315789473684)}
All evaluation runs completed!
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ tmux capture-pane -p -S - > efficot_mistral_student_mistral_all_datasets.txt

