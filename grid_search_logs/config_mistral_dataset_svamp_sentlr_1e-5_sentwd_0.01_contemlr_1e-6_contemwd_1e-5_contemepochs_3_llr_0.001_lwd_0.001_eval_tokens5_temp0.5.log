Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.95it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:01<00:00,  1.58it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.53it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.57it/s]
Running inference:   0%|          | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference:   1%|          | 1/100 [00:01<02:29,  1.51s/it]Running inference:   2%|▏         | 2/100 [00:02<01:54,  1.17s/it]Running inference:   3%|▎         | 3/100 [00:03<01:44,  1.07s/it]Running inference:   4%|▍         | 4/100 [00:04<01:37,  1.02s/it]Running inference:   5%|▌         | 5/100 [00:05<01:33,  1.01it/s]Running inference:   6%|▌         | 6/100 [00:06<01:31,  1.03it/s]Running inference:   7%|▋         | 7/100 [00:07<01:29,  1.04it/s]Running inference:   8%|▊         | 8/100 [00:08<01:27,  1.05it/s]Running inference:   9%|▉         | 9/100 [00:09<01:25,  1.06it/s]Running inference:  10%|█         | 10/100 [00:09<01:24,  1.06it/s]Running inference:  11%|█         | 11/100 [00:10<01:23,  1.07it/s]Running inference:  12%|█▏        | 12/100 [00:11<01:22,  1.07it/s]Running inference:  13%|█▎        | 13/100 [00:12<01:20,  1.08it/s]Running inference:  14%|█▍        | 14/100 [00:13<01:19,  1.08it/s]Running inference:  15%|█▌        | 15/100 [00:14<01:18,  1.08it/s]Running inference:  16%|█▌        | 16/100 [00:15<01:18,  1.08it/s]Running inference:  17%|█▋        | 17/100 [00:16<01:17,  1.08it/s]Running inference:  18%|█▊        | 18/100 [00:17<01:16,  1.08it/s]Running inference:  19%|█▉        | 19/100 [00:18<01:15,  1.07it/s]Running inference:  20%|██        | 20/100 [00:19<01:14,  1.07it/s]Running inference:  21%|██        | 21/100 [00:20<01:13,  1.07it/s]Running inference:  22%|██▏       | 22/100 [00:21<01:12,  1.07it/s]Running inference:  23%|██▎       | 23/100 [00:22<01:11,  1.07it/s]Running inference:  24%|██▍       | 24/100 [00:22<01:09,  1.09it/s]Running inference:  25%|██▌       | 25/100 [00:23<01:08,  1.10it/s]Running inference:  26%|██▌       | 26/100 [00:24<01:07,  1.09it/s]Running inference:  27%|██▋       | 27/100 [00:25<01:07,  1.08it/s]Running inference:  28%|██▊       | 28/100 [00:26<01:06,  1.08it/s]Running inference:  29%|██▉       | 29/100 [00:27<01:05,  1.08it/s]Running inference:  30%|███       | 30/100 [00:28<01:05,  1.08it/s]Running inference:  31%|███       | 31/100 [00:29<01:04,  1.08it/s]Running inference:  32%|███▏      | 32/100 [00:30<01:03,  1.07it/s]Running inference:  33%|███▎      | 33/100 [00:31<01:02,  1.07it/s]Running inference:  34%|███▍      | 34/100 [00:32<01:01,  1.07it/s]Running inference:  35%|███▌      | 35/100 [00:33<01:00,  1.07it/s]Running inference:  36%|███▌      | 36/100 [00:34<00:58,  1.09it/s]Running inference:  37%|███▋      | 37/100 [00:34<00:58,  1.08it/s]Running inference:  38%|███▊      | 38/100 [00:35<00:57,  1.08it/s]Running inference:  39%|███▉      | 39/100 [00:36<00:56,  1.08it/s]Running inference:  40%|████      | 40/100 [00:37<00:55,  1.08it/s]Running inference:  41%|████      | 41/100 [00:38<00:54,  1.07it/s]Running inference:  42%|████▏     | 42/100 [00:39<00:53,  1.07it/s]Running inference:  43%|████▎     | 43/100 [00:40<00:53,  1.07it/s]Running inference:  44%|████▍     | 44/100 [00:41<00:52,  1.07it/s]Running inference:  45%|████▌     | 45/100 [00:42<00:51,  1.07it/s]Running inference:  46%|████▌     | 46/100 [00:43<00:50,  1.06it/s]Running inference:  47%|████▋     | 47/100 [00:44<00:49,  1.07it/s]Running inference:  48%|████▊     | 48/100 [00:45<00:48,  1.07it/s]Running inference:  49%|████▉     | 49/100 [00:46<00:47,  1.07it/s]Running inference:  50%|█████     | 50/100 [00:47<00:46,  1.07it/s]Running inference:  51%|█████     | 51/100 [00:48<00:46,  1.06it/s]Running inference:  52%|█████▏    | 52/100 [00:48<00:45,  1.07it/s]Running inference:  53%|█████▎    | 53/100 [00:49<00:44,  1.07it/s]Running inference:  54%|█████▍    | 54/100 [00:50<00:42,  1.08it/s]Running inference:  55%|█████▌    | 55/100 [00:51<00:41,  1.08it/s]Running inference:  56%|█████▌    | 56/100 [00:52<00:40,  1.08it/s]Running inference:  57%|█████▋    | 57/100 [00:53<00:40,  1.07it/s]Running inference:  58%|█████▊    | 58/100 [00:54<00:39,  1.07it/s]Running inference:  59%|█████▉    | 59/100 [00:55<00:38,  1.07it/s]Running inference:  60%|██████    | 60/100 [00:56<00:37,  1.07it/s]Running inference:  61%|██████    | 61/100 [00:57<00:36,  1.07it/s]Running inference:  62%|██████▏   | 62/100 [00:58<00:35,  1.07it/s]Running inference:  63%|██████▎   | 63/100 [00:59<00:34,  1.07it/s]Running inference:  64%|██████▍   | 64/100 [01:00<00:33,  1.07it/s]Running inference:  65%|██████▌   | 65/100 [01:01<00:32,  1.06it/s]Running inference:  66%|██████▌   | 66/100 [01:02<00:31,  1.06it/s]Running inference:  67%|██████▋   | 67/100 [01:02<00:30,  1.08it/s]Running inference:  68%|██████▊   | 68/100 [01:03<00:29,  1.08it/s]Running inference:  69%|██████▉   | 69/100 [01:04<00:28,  1.08it/s]Running inference:  70%|███████   | 70/100 [01:05<00:27,  1.08it/s]Running inference:  71%|███████   | 71/100 [01:06<00:26,  1.08it/s]Running inference:  72%|███████▏  | 72/100 [01:07<00:26,  1.07it/s]Running inference:  73%|███████▎  | 73/100 [01:08<00:25,  1.07it/s]Running inference:  74%|███████▍  | 74/100 [01:09<00:24,  1.06it/s]Running inference:  75%|███████▌  | 75/100 [01:10<00:23,  1.06it/s]Running inference:  76%|███████▌  | 76/100 [01:11<00:22,  1.05it/s]Running inference:  77%|███████▋  | 77/100 [01:12<00:21,  1.06it/s]Running inference:  78%|███████▊  | 78/100 [01:13<00:20,  1.06it/s]Running inference:  79%|███████▉  | 79/100 [01:14<00:19,  1.06it/s]Running inference:  80%|████████  | 80/100 [01:15<00:18,  1.08it/s]Running inference:  81%|████████  | 81/100 [01:16<00:17,  1.07it/s]Running inference:  82%|████████▏ | 82/100 [01:16<00:16,  1.07it/s]Running inference:  83%|████████▎ | 83/100 [01:17<00:15,  1.07it/s]Running inference:  84%|████████▍ | 84/100 [01:18<00:14,  1.08it/s]Running inference:  85%|████████▌ | 85/100 [01:19<00:13,  1.07it/s]Running inference:  86%|████████▌ | 86/100 [01:20<00:13,  1.07it/s]Running inference:  87%|████████▋ | 87/100 [01:21<00:12,  1.07it/s]Running inference:  88%|████████▊ | 88/100 [01:22<00:11,  1.07it/s]Running inference:  89%|████████▉ | 89/100 [01:23<00:10,  1.07it/s]Running inference:  90%|█████████ | 90/100 [01:24<00:09,  1.06it/s]Running inference:  91%|█████████ | 91/100 [01:25<00:08,  1.06it/s]Running inference:  92%|█████████▏| 92/100 [01:26<00:07,  1.06it/s]Running inference:  93%|█████████▎| 93/100 [01:27<00:06,  1.06it/s]Running inference:  94%|█████████▍| 94/100 [01:28<00:05,  1.07it/s]Running inference:  95%|█████████▌| 95/100 [01:29<00:04,  1.07it/s]Running inference:  96%|█████████▌| 96/100 [01:30<00:03,  1.07it/s]Running inference:  97%|█████████▋| 97/100 [01:31<00:02,  1.07it/s]Running inference:  98%|█████████▊| 98/100 [01:31<00:01,  1.07it/s]Running inference:  99%|█████████▉| 99/100 [01:32<00:00,  1.07it/s]Running inference: 100%|██████████| 100/100 [01:33<00:00,  1.07it/s]Running inference: 100%|██████████| 100/100 [01:33<00:00,  1.07it/s]
Average time taken for each sample: 0.9336901760101318, Average time taken for contemplation: 0.019855098724365236
Evaluation results: {'numerical_accuracy': 0.37, 'close_match_rate': 0.37, 'mean_relative_error': np.float64(1.1111111111111112e+31), 'median_relative_error': np.float64(0.0437833162546704), 'dataset': 'svamp', 'student': 'optimum/mistral-1.1b-testing', 'teacher': 'mistralai/Mistral-7B-Instruct-v0.2', 'sent_trans_lr': 1e-05, 'sent_trans_weight_decay': 0.01, 'sent_trans_epochs': 15, 'contemp_gen_lr': 1e-07, 'contemp_gen_epochs': 2, 'contemp_gen_lin_layer_lr': 0.001, 'contemp_gen_lin_layer_epochs': 10, 'contemp_gen_lin_layer_weight_decay': 0.001, 'contemp_gen_weight_decay': 1e-05, 'eval_temp': 0.5, 'train_max_contemp_tokens': 5, 'eval_max_contemp_tokens': 5}
