Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.08it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.23s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.05s/it]
Running inference:   0%|          | 0/100 [00:00<?, ?it/s]> /home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py(545)forward()
-> position_embeddings = self.rotary_emb(hidden_states, position_ids)
(Pdb) The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
> /home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py(545)forward()
-> position_embeddings = self.rotary_emb(hidden_states, position_ids)
(Pdb) > /home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py(545)forward()
-> position_embeddings = self.rotary_emb(hidden_states, position_ids)
(Pdb) Clear all breaks? (Pdb) > /home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py(545)forward()
-> position_embeddings = self.rotary_emb(hidden_states, position_ids)
(Pdb) > /home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py(545)forward()
-> position_embeddings = self.rotary_emb(hidden_states, position_ids)
(Pdb) (Pdb) (Pdb) > /home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py(545)forward()
-> position_embeddings = self.rotary_emb(hidden_states, position_ids)
(Pdb) > /home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py(545)forward()
-> position_embeddings = self.rotary_emb(hidden_states, position_ids)
(Pdb) > /home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py(545)forward()
-> position_embeddings = self.rotary_emb(hidden_states, position_ids)
(Pdb) > /home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py(545)forward()
-> position_embeddings = self.rotary_emb(hidden_states, position_ids)
(Pdb) > /home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py(545)forward()
-> position_embeddings = self.rotary_emb(hidden_states, position_ids)
(Pdb) > /home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py(545)forward()
-> position_embeddings = self.rotary_emb(hidden_states, position_ids)
(Pdb) (Pdb) > /home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py(545)forward()
-> position_embeddings = self.rotary_emb(hidden_states, position_ids)
(Pdb) > /home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py(545)forward()
-> position_embeddings = self.rotary_emb(hidden_states, position_ids)
(Pdb) > /home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py(545)forward()
-> position_embeddings = self.rotary_emb(hidden_states, position_ids)
(Pdb) > /home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py(545)forward()
-> position_embeddings = self.rotary_emb(hidden_states, position_ids)
(Pdb) > /home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py(545)forward()
-> position_embeddings = self.rotary_emb(hidden_states, position_ids)
(Pdb) > /home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py(545)forward()
-> position_embeddings = self.rotary_emb(hidden_states, position_ids)
(Pdb) > /home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py(545)forward()
-> position_embeddings = self.rotary_emb(hidden_states, position_ids)
(Pdb) > /home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py(545)forward()
-> position_embeddings = self.rotary_emb(hidden_states, position_ids)
(Pdb) Running inference:   0%|          | 0/100 [56:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/nee7ne/EfficientCoT/main.py", line 272, in <module>
    main()
  File "/home/nee7ne/EfficientCoT/main.py", line 218, in main
    results = run_inference(
  File "/home/nee7ne/EfficientCoT/inference/inference.py", line 152, in run_inference
    outputs = teacher_model.generate(
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/generation/utils.py", line 2223, in generate
    result = self._sample(
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/generation/utils.py", line 3214, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 845, in forward
    outputs = self.model(
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 545, in forward
    position_embeddings = self.rotary_emb(hidden_states, position_ids)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 545, in forward
    position_embeddings = self.rotary_emb(hidden_states, position_ids)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
