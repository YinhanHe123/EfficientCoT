(effi_cot) nee7ne@uvavast:~/EfficientCoT$ bash run_efficot_on_datasets.sh
Running: python main.py --config mistral --mode train_sentence_transformer --dataset gsm8k  --device 0
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.50it/s]
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.50it/s]
2025-04-08 23:00:07,166 [INFO] Logging to ./logs/sentence_transformer_16_to_20
2025-04-08 23:00:07,166 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/data/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/gsm8k', 'checkpoint_path': './checkpoin
ts/effi_cot/vanilla/mistral/gsm8k', 'result_path': './results/effi_cot/vanilla/mistral/gsm8k', 'experiment_name': 'effi_cot_vanilla_42_gsm8k_mistral', 'learning_rate': 1e-05, 'weight_decay': 0.01, 'num_epochs': 12,
'train_sen_trans_epochs': 15, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 150, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 400, 'max_contemp_tokens': 5, 'start_layer_idx': 16,
'end_layer_idx': 20, 'reasoning_pairs_path': '/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'eval_temp': 0.3}
Epoch 1/15 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [01:33<00:00,  1.17s/it]
Epoch 1/12 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:19<00:00,  1.04it/s]
2025-04-08 23:01:59,706 [INFO] Step 0 - train_loss: 0.9087, val_loss: 0.9124
Epoch 1/15 - Train Loss: 0.9087, Val Loss: 0.9124
Saved best model with validation loss: 0.9124
Epoch 2/15 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [01:32<00:00,  1.15s/it]
Epoch 2/12 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:18<00:00,  1.09it/s]
2025-04-08 23:03:57,878 [INFO] Step 1 - train_loss: 0.8342, val_loss: 0.8587
Epoch 2/15 - Train Loss: 0.8342, Val Loss: 0.8587
Saved best model with validation loss: 0.8587
Epoch 3/15 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [01:33<00:00,  1.17s/it]
Epoch 3/12 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:18<00:00,  1.08it/s]
2025-04-08 23:05:57,270 [INFO] Step 2 - train_loss: 0.8540, val_loss: 0.9109
Epoch 3/15 - Train Loss: 0.8540, Val Loss: 0.9109
Epoch 4/15 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [01:34<00:00,  1.18s/it]
Epoch 4/12 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:18<00:00,  1.08it/s]
2025-04-08 23:07:49,761 [INFO] Step 3 - train_loss: 0.8279, val_loss: 0.8655
Epoch 4/15 - Train Loss: 0.8279, Val Loss: 0.8655
Epoch 5/15 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [01:33<00:00,  1.16s/it]
Epoch 5/12 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:18<00:00,  1.08it/s]
2025-04-08 23:09:41,358 [INFO] Step 4 - train_loss: 0.8084, val_loss: 0.8673
Epoch 5/15 - Train Loss: 0.8084, Val Loss: 0.8673
Epoch 6/15 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [01:33<00:00,  1.16s/it]
Epoch 6/12 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:18<00:00,  1.08it/s]
2025-04-08 23:11:32,847 [INFO] Step 5 - train_loss: 0.8393, val_loss: 0.8391
Epoch 6/15 - Train Loss: 0.8393, Val Loss: 0.8391
Saved best model with validation loss: 0.8391
Epoch 7/15 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [01:33<00:00,  1.16s/it]
Epoch 7/12 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:18<00:00,  1.08it/s]
2025-04-08 23:13:31,732 [INFO] Step 6 - train_loss: 0.8068, val_loss: 0.8500
Epoch 7/15 - Train Loss: 0.8068, Val Loss: 0.8500
Epoch 8/15 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [01:32<00:00,  1.15s/it]
Epoch 8/12 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:18<00:00,  1.08it/s]
2025-04-08 23:15:22,560 [INFO] Step 7 - train_loss: 0.8194, val_loss: 0.8995
Epoch 8/15 - Train Loss: 0.8194, Val Loss: 0.8995
Epoch 9/15 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [01:33<00:00,  1.17s/it]
Epoch 9/12 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:18<00:00,  1.08it/s]
2025-04-08 23:17:14,483 [INFO] Step 8 - train_loss: 0.8187, val_loss: 0.9057
Epoch 9/15 - Train Loss: 0.8187, Val Loss: 0.9057
Epoch 10/15 - Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [01:34<00:00,  1.18s/it]
Epoch 10/12 - Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:18<00:00,  1.08it/s]
2025-04-08 23:19:07,102 [INFO] Step 9 - train_loss: 0.7938, val_loss: 0.8625
Epoch 10/15 - Train Loss: 0.7938, Val Loss: 0.8625
Epoch 11/15 - Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [01:33<00:00,  1.17s/it]
Epoch 11/12 - Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:18<00:00,  1.08it/s]
2025-04-08 23:20:59,402 [INFO] Step 10 - train_loss: 0.8272, val_loss: 0.9285
Epoch 11/15 - Train Loss: 0.8272, Val Loss: 0.9285
Epoch 12/15 - Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [01:33<00:00,  1.16s/it]
Epoch 12/12 - Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:18<00:00,  1.08it/s]
2025-04-08 23:22:51,011 [INFO] Step 11 - train_loss: 0.8495, val_loss: 0.8695
Epoch 12/15 - Train Loss: 0.8495, Val Loss: 0.8695
Epoch 13/15 - Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [01:32<00:00,  1.16s/it]
Epoch 13/12 - Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:18<00:00,  1.08it/s]
2025-04-08 23:24:42,187 [INFO] Step 12 - train_loss: 0.8175, val_loss: 0.9012
Epoch 13/15 - Train Loss: 0.8175, Val Loss: 0.9012
Epoch 14/15 - Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [01:33<00:00,  1.16s/it]
Epoch 14/12 - Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:18<00:00,  1.08it/s]
2025-04-08 23:26:33,827 [INFO] Step 13 - train_loss: 0.8323, val_loss: 0.8837
Epoch 14/15 - Train Loss: 0.8323, Val Loss: 0.8837
Epoch 15/15 - Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [01:33<00:00,  1.17s/it]
Epoch 15/12 - Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:18<00:00,  1.08it/s]
2025-04-08 23:28:25,831 [INFO] Step 14 - train_loss: 0.8113, val_loss: 0.8460
Epoch 15/15 - Train Loss: 0.8113, Val Loss: 0.8460
----------------------------------------
Running: python main.py --config mistral --mode train_contemp_generator --dataset gsm8k  --device 0
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.52it/s]
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
2025-04-08 23:28:50,489 [INFO] Logging to ./logs/contemp_generator
2025-04-08 23:28:50,489 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/data/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/gsm8k', 'checkpoint_path': './checkpoin
ts/effi_cot/vanilla/mistral/gsm8k', 'result_path': './results/effi_cot/vanilla/mistral/gsm8k', 'experiment_name': 'effi_cot_vanilla_42_gsm8k_mistral', 'learning_rate': 1e-05, 'weight_decay': 0.01, 'num_epochs': 12,
'train_sen_trans_epochs': 15, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 150, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 400, 'max_contemp_tokens': 5, 'start_layer_idx': 16,
'end_layer_idx': 20, 'reasoning_pairs_path': '/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'eval_temp': 0.3, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_mode
l_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'openai/gsm8k', 'teacher_hidden_dim
': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
Starting training contemplation generator...
Epoch 1/12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [04:07<00:00,  1.62it/s]
2025-04-08 23:32:57,620 [INFO] Step 0 - total_loss: 1.8022, reason_loss: 0.9886, ans_loss: 2.0734
Epoch 1 - Loss: 1.8022 (Reason: 0.9886, Ans: 2.0734)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.14it/s]
Evaluation - Total Loss: 1.6370, Reason Loss: 0.9708, Answer Loss: 1.8590
2025-04-08 23:33:29,485 [INFO] Step 0 - eval_loss: 1.6370
Validation Loss: 1.6370
Epoch 2/12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [04:07<00:00,  1.62it/s]
2025-04-08 23:37:36,494 [INFO] Step 1 - total_loss: 1.5717, reason_loss: 0.9700, ans_loss: 1.7722
Epoch 2 - Loss: 1.5717 (Reason: 0.9700, Ans: 1.7722)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:32<00:00,  3.09it/s]
Evaluation - Total Loss: 1.6186, Reason Loss: 0.9503, Answer Loss: 1.8414
2025-04-08 23:38:08,894 [INFO] Step 1 - eval_loss: 1.6186
Validation Loss: 1.6186
Epoch 3/12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [04:07<00:00,  1.62it/s]
2025-04-08 23:42:16,042 [INFO] Step 2 - total_loss: 1.5371, reason_loss: 0.9527, ans_loss: 1.7319
Epoch 3 - Loss: 1.5371 (Reason: 0.9527, Ans: 1.7319)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.16it/s]
Evaluation - Total Loss: 1.6285, Reason Loss: 0.9369, Answer Loss: 1.8590
2025-04-08 23:42:47,714 [INFO] Step 2 - eval_loss: 1.6285
Validation Loss: 1.6285
Epoch 4/12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [04:07<00:00,  1.62it/s]
2025-04-08 23:46:54,892 [INFO] Step 3 - total_loss: 1.5010, reason_loss: 0.9401, ans_loss: 1.6880
Epoch 4 - Loss: 1.5010 (Reason: 0.9401, Ans: 1.6880)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:32<00:00,  3.09it/s]
Evaluation - Total Loss: 1.6232, Reason Loss: 0.9263, Answer Loss: 1.8556
2025-04-08 23:47:27,266 [INFO] Step 3 - eval_loss: 1.6232
Validation Loss: 1.6232
Epoch 5/12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [04:07<00:00,  1.62it/s]
2025-04-08 23:51:34,507 [INFO] Step 4 - total_loss: 1.4606, reason_loss: 0.9329, ans_loss: 1.6365
Epoch 5 - Loss: 1.4606 (Reason: 0.9329, Ans: 1.6365)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.14it/s]
Evaluation - Total Loss: 1.5647, Reason Loss: 0.9209, Answer Loss: 1.7793
2025-04-08 23:52:06,366 [INFO] Step 4 - eval_loss: 1.5647
Validation Loss: 1.5647
Epoch 6/12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [04:05<00:00,  1.63it/s]
2025-04-08 23:56:12,341 [INFO] Step 5 - total_loss: 1.4123, reason_loss: 0.9292, ans_loss: 1.5734
Epoch 6 - Loss: 1.4123 (Reason: 0.9292, Ans: 1.5734)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.14it/s]
Evaluation - Total Loss: 1.5942, Reason Loss: 0.9222, Answer Loss: 1.8182
2025-04-08 23:56:44,193 [INFO] Step 5 - eval_loss: 1.5942
Validation Loss: 1.5942
Epoch 7/12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [04:06<00:00,  1.63it/s]
2025-04-09 00:00:50,348 [INFO] Step 6 - total_loss: 1.3810, reason_loss: 0.9277, ans_loss: 1.5322
Epoch 7 - Loss: 1.3810 (Reason: 0.9277, Ans: 1.5322)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.17it/s]
Evaluation - Total Loss: 1.5709, Reason Loss: 0.9206, Answer Loss: 1.7877
2025-04-09 00:01:21,861 [INFO] Step 6 - eval_loss: 1.5709
Validation Loss: 1.5709
Epoch 8/12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [04:05<00:00,  1.63it/s]
2025-04-09 00:05:27,810 [INFO] Step 7 - total_loss: 1.3257, reason_loss: 0.9272, ans_loss: 1.4585
Epoch 8 - Loss: 1.3257 (Reason: 0.9272, Ans: 1.4585)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.14it/s]
Evaluation - Total Loss: 1.6659, Reason Loss: 0.9212, Answer Loss: 1.9141
2025-04-09 00:05:59,682 [INFO] Step 7 - eval_loss: 1.6659
Validation Loss: 1.6659
Epoch 9/12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [04:06<00:00,  1.62it/s]
2025-04-09 00:10:06,555 [INFO] Step 8 - total_loss: 1.2980, reason_loss: 0.9287, ans_loss: 1.4211
Epoch 9 - Loss: 1.2980 (Reason: 0.9287, Ans: 1.4211)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.14it/s]
Evaluation - Total Loss: 1.6134, Reason Loss: 0.9234, Answer Loss: 1.8434
2025-04-09 00:10:38,358 [INFO] Step 8 - eval_loss: 1.6134
Validation Loss: 1.6134
Epoch 10/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [04:05<00:00,  1.63it/s]
2025-04-09 00:14:43,778 [INFO] Step 9 - total_loss: 1.2778, reason_loss: 0.9298, ans_loss: 1.3938
Epoch 10 - Loss: 1.2778 (Reason: 0.9298, Ans: 1.3938)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.16it/s]
Evaluation - Total Loss: 1.6649, Reason Loss: 0.9255, Answer Loss: 1.9114
2025-04-09 00:15:15,417 [INFO] Step 9 - eval_loss: 1.6649
Validation Loss: 1.6649
Epoch 11/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [04:06<00:00,  1.63it/s]
2025-04-09 00:19:21,569 [INFO] Step 10 - total_loss: 1.1853, reason_loss: 0.9298, ans_loss: 1.2705
Epoch 11 - Loss: 1.1853 (Reason: 0.9298, Ans: 1.2705)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.14it/s]
Evaluation - Total Loss: 1.7034, Reason Loss: 0.9255, Answer Loss: 1.9627
2025-04-09 00:19:53,418 [INFO] Step 10 - eval_loss: 1.7034
Validation Loss: 1.7034
Epoch 12/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [05:06<00:00,  1.31it/s]
2025-04-09 00:24:59,429 [INFO] Step 11 - total_loss: 1.2061, reason_loss: 0.9288, ans_loss: 1.2985
Epoch 12 - Loss: 1.2061 (Reason: 0.9288, Ans: 1.2985)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.21it/s]
Evaluation - Total Loss: 1.6899, Reason Loss: 0.9222, Answer Loss: 1.9458
2025-04-09 00:25:30,570 [INFO] Step 11 - eval_loss: 1.6899
Validation Loss: 1.6899
Saved best model with validation loss: 1.5647
----------------------------------------
Running: python main.py --config mistral --mode evaluate --dataset gsm8k  --device 0
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:21<00:00,  1.22it/s]
Average time taken for each sample: 0.8131069779396057, Average time taken for contemplation: 0.019420905113220213
Evaluation results: {'numerical_accuracy': 0.07, 'close_match_rate': 0.07, 'mean_relative_error': np.float64(0.7969226701140045), 'median_relative_error': np.float64(0.463884430176565)}
----------------------------------------
Running: python main.py --config mistral --mode train_sentence_transformer --dataset multiarith  --device 0
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.50it/s]
2025-04-09 00:27:40,885 [INFO] Logging to ./logs/sentence_transformer_16_to_20
2025-04-09 00:27:40,885 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/data/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/multiarith', 'checkpoint_path': './chec
kpoints/effi_cot/vanilla/mistral/multiarith', 'result_path': './results/effi_cot/vanilla/mistral/multiarith', 'experiment_name': 'effi_cot_vanilla_42_multiarith_mistral', 'learning_rate': 1e-05, 'weight_decay': 0.01
, 'num_epochs': 12, 'train_sen_trans_epochs': 15, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 150, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 400, 'max_contemp_tokens': 5, 'st
art_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'eval_temp': 0.3}
Epoch 1/15 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:30<00:00,  2.59it/s]
Epoch 1/12 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.09it/s]
2025-04-09 00:28:16,679 [INFO] Step 0 - train_loss: 1.2222, val_loss: 1.0620
Epoch 1/15 - Train Loss: 1.2222, Val Loss: 1.0620
Saved best model with validation loss: 1.0620
Epoch 2/15 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:30<00:00,  2.64it/s]
Epoch 2/12 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.08it/s]
2025-04-09 00:28:58,793 [INFO] Step 1 - train_loss: 1.0257, val_loss: 1.0300
Epoch 2/15 - Train Loss: 1.0257, Val Loss: 1.0300
Saved best model with validation loss: 1.0300
Epoch 3/15 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:30<00:00,  2.59it/s]
Epoch 3/12 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.08it/s]
2025-04-09 00:29:41,828 [INFO] Step 2 - train_loss: 0.9413, val_loss: 0.8995
Epoch 3/15 - Train Loss: 0.9413, Val Loss: 0.8995
Saved best model with validation loss: 0.8995
Epoch 4/15 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:30<00:00,  2.64it/s]
Epoch 4/12 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.08it/s]
2025-04-09 00:30:24,328 [INFO] Step 3 - train_loss: 0.9045, val_loss: 0.8538
Epoch 4/15 - Train Loss: 0.9045, Val Loss: 0.8538
Saved best model with validation loss: 0.8538
Epoch 5/15 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:31<00:00,  2.56it/s]
Epoch 5/12 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.08it/s]
2025-04-09 00:31:07,807 [INFO] Step 4 - train_loss: 0.8640, val_loss: 0.8526
Epoch 5/15 - Train Loss: 0.8640, Val Loss: 0.8526
Saved best model with validation loss: 0.8526
Epoch 6/15 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:30<00:00,  2.64it/s]
Epoch 6/12 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.08it/s]
2025-04-09 00:31:50,249 [INFO] Step 5 - train_loss: 0.8751, val_loss: 0.8465
Epoch 6/15 - Train Loss: 0.8751, Val Loss: 0.8465
Saved best model with validation loss: 0.8465
Epoch 7/15 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:30<00:00,  2.63it/s]
Epoch 7/12 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.07it/s]
2025-04-09 00:32:32,936 [INFO] Step 6 - train_loss: 0.8488, val_loss: 0.8587
Epoch 7/15 - Train Loss: 0.8488, Val Loss: 0.8587
Epoch 8/15 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:30<00:00,  2.61it/s]
Epoch 8/12 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.07it/s]
2025-04-09 00:33:08,463 [INFO] Step 7 - train_loss: 0.8495, val_loss: 0.9074
Epoch 8/15 - Train Loss: 0.8495, Val Loss: 0.9074
Epoch 9/15 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:30<00:00,  2.61it/s]
Epoch 9/12 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.06it/s]
2025-04-09 00:33:44,047 [INFO] Step 8 - train_loss: 0.8611, val_loss: 0.8966
Epoch 9/15 - Train Loss: 0.8611, Val Loss: 0.8966
Epoch 10/15 - Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:30<00:00,  2.60it/s]
Epoch 10/12 - Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.06it/s]
2025-04-09 00:34:19,756 [INFO] Step 9 - train_loss: 0.8520, val_loss: 0.8704
Epoch 10/15 - Train Loss: 0.8520, Val Loss: 0.8704
Epoch 11/15 - Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:30<00:00,  2.61it/s]
Epoch 11/12 - Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.06it/s]
2025-04-09 00:34:55,373 [INFO] Step 10 - train_loss: 0.8986, val_loss: 0.8941
Epoch 11/15 - Train Loss: 0.8986, Val Loss: 0.8941
Epoch 12/15 - Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:30<00:00,  2.60it/s]
Epoch 12/12 - Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.05it/s]
2025-04-09 00:35:31,076 [INFO] Step 11 - train_loss: 0.8338, val_loss: 0.8699
Epoch 12/15 - Train Loss: 0.8338, Val Loss: 0.8699
Epoch 13/15 - Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:30<00:00,  2.61it/s]
Epoch 13/12 - Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.05it/s]
2025-04-09 00:36:06,681 [INFO] Step 12 - train_loss: 0.8669, val_loss: 0.8704
Epoch 13/15 - Train Loss: 0.8669, Val Loss: 0.8704
Epoch 14/15 - Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:30<00:00,  2.62it/s]
Epoch 14/12 - Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.05it/s]
2025-04-09 00:36:42,154 [INFO] Step 13 - train_loss: 0.8108, val_loss: 0.8541
Epoch 14/15 - Train Loss: 0.8108, Val Loss: 0.8541
Epoch 15/15 - Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:30<00:00,  2.61it/s]
Epoch 15/12 - Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.05it/s]
2025-04-09 00:37:17,755 [INFO] Step 14 - train_loss: 0.8040, val_loss: 0.8310
Epoch 15/15 - Train Loss: 0.8040, Val Loss: 0.8310
Saved best model with validation loss: 0.8310
----------------------------------------
Running: python main.py --config mistral --mode train_contemp_generator --dataset multiarith  --device 0
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.53it/s]
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.50it/s]
2025-04-09 00:37:49,453 [INFO] Logging to ./logs/contemp_generator
2025-04-09 00:37:49,454 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/data/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/multiarith', 'checkpoint_path': './chec
kpoints/effi_cot/vanilla/mistral/multiarith', 'result_path': './results/effi_cot/vanilla/mistral/multiarith', 'experiment_name': 'effi_cot_vanilla_42_multiarith_mistral', 'learning_rate': 1e-05, 'weight_decay': 0.01
, 'num_epochs': 12, 'train_sen_trans_epochs': 15, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 150, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 400, 'max_contemp_tokens': 5, 'st
art_layer_idx': 16, 'end_layer_idx': 20, 'reasoning_pairs_path': '/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'eval_temp': 0.3, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-
v0.2', 'student_model_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/MultiA
rith', 'teacher_hidden_dim': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
Starting training contemplation generator...
Epoch 1/12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:32<00:00,  2.61it/s]
2025-04-09 00:40:22,428 [INFO] Step 0 - total_loss: 1.4097, reason_loss: 0.5287, ans_loss: 1.7033
Epoch 1 - Loss: 1.4097 (Reason: 0.5287, Ans: 1.7033)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:16<00:00,  6.15it/s]
Evaluation - Total Loss: 1.1962, Reason Loss: 0.2015, Answer Loss: 1.5278
2025-04-09 00:40:38,699 [INFO] Step 0 - eval_loss: 1.1962
Validation Loss: 1.1962
Epoch 2/12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:32<00:00,  2.62it/s]
2025-04-09 00:43:11,595 [INFO] Step 1 - total_loss: 1.1329, reason_loss: 0.1132, ans_loss: 1.4727
Epoch 2 - Loss: 1.1329 (Reason: 0.1132, Ans: 1.4727)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:17<00:00,  5.70it/s]
Evaluation - Total Loss: 1.0952, Reason Loss: 0.0709, Answer Loss: 1.4367
2025-04-09 00:43:29,139 [INFO] Step 1 - eval_loss: 1.0952
Validation Loss: 1.0952
Epoch 3/12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:31<00:00,  2.65it/s]
2025-04-09 00:46:00,209 [INFO] Step 2 - total_loss: 1.0576, reason_loss: 0.0461, ans_loss: 1.3948
Epoch 3 - Loss: 1.0576 (Reason: 0.0461, Ans: 1.3948)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:16<00:00,  6.02it/s]
Evaluation - Total Loss: 1.0517, Reason Loss: 0.0407, Answer Loss: 1.3887
2025-04-09 00:46:16,821 [INFO] Step 2 - eval_loss: 1.0517
Validation Loss: 1.0517
Epoch 4/12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:32<00:00,  2.63it/s]
2025-04-09 00:48:49,133 [INFO] Step 3 - total_loss: 1.0105, reason_loss: 0.0343, ans_loss: 1.3359
Epoch 4 - Loss: 1.0105 (Reason: 0.0343, Ans: 1.3359)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:17<00:00,  5.70it/s]
Evaluation - Total Loss: 1.0150, Reason Loss: 0.0330, Answer Loss: 1.3423
2025-04-09 00:49:06,677 [INFO] Step 3 - eval_loss: 1.0150
Validation Loss: 1.0150
Epoch 5/12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:34<00:00,  2.60it/s]
2025-04-09 00:51:40,770 [INFO] Step 4 - total_loss: 0.9776, reason_loss: 0.0361, ans_loss: 1.2914
Epoch 5 - Loss: 0.9776 (Reason: 0.0361, Ans: 1.2914)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:17<00:00,  5.70it/s]
Evaluation - Total Loss: 1.0039, Reason Loss: 0.0342, Answer Loss: 1.3271
2025-04-09 00:51:58,316 [INFO] Step 4 - eval_loss: 1.0039
Validation Loss: 1.0039
Epoch 6/12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:32<00:00,  2.62it/s]
2025-04-09 00:54:30,832 [INFO] Step 5 - total_loss: 0.9396, reason_loss: 0.0376, ans_loss: 1.2403
Epoch 6 - Loss: 0.9396 (Reason: 0.0376, Ans: 1.2403)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:16<00:00,  6.08it/s]
Evaluation - Total Loss: 0.9904, Reason Loss: 0.0405, Answer Loss: 1.3071
2025-04-09 00:54:47,283 [INFO] Step 5 - eval_loss: 0.9904
Validation Loss: 0.9904
Epoch 7/12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:32<00:00,  2.62it/s]
2025-04-09 00:57:19,996 [INFO] Step 6 - total_loss: 0.9044, reason_loss: 0.0382, ans_loss: 1.1931
Epoch 7 - Loss: 0.9044 (Reason: 0.0382, Ans: 1.1931)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:16<00:00,  6.10it/s]
Evaluation - Total Loss: 0.9954, Reason Loss: 0.0489, Answer Loss: 1.3109
2025-04-09 00:57:36,400 [INFO] Step 6 - eval_loss: 0.9954
Validation Loss: 0.9954
Epoch 8/12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:33<00:00,  2.60it/s]
2025-04-09 01:00:10,231 [INFO] Step 7 - total_loss: 0.8852, reason_loss: 0.0478, ans_loss: 1.1643
Epoch 8 - Loss: 0.8852 (Reason: 0.0478, Ans: 1.1643)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:16<00:00,  6.15it/s]
Evaluation - Total Loss: 0.9653, Reason Loss: 0.0538, Answer Loss: 1.2691
2025-04-09 01:00:26,506 [INFO] Step 7 - eval_loss: 0.9653
Validation Loss: 0.9653
Epoch 9/12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:32<00:00,  2.62it/s]
2025-04-09 01:02:59,082 [INFO] Step 8 - total_loss: 0.8667, reason_loss: 0.0437, ans_loss: 1.1410
Epoch 9 - Loss: 0.8667 (Reason: 0.0437, Ans: 1.1410)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:16<00:00,  6.16it/s]
Evaluation - Total Loss: 0.9454, Reason Loss: 0.0392, Answer Loss: 1.2475
2025-04-09 01:03:15,322 [INFO] Step 8 - eval_loss: 0.9454
Validation Loss: 0.9454
Epoch 10/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:32<00:00,  2.62it/s]
2025-04-09 01:05:47,968 [INFO] Step 9 - total_loss: 0.8382, reason_loss: 0.0434, ans_loss: 1.1031
Epoch 10 - Loss: 0.8382 (Reason: 0.0434, Ans: 1.1031)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:16<00:00,  6.15it/s]
Evaluation - Total Loss: 0.9288, Reason Loss: 0.0457, Answer Loss: 1.2232
2025-04-09 01:06:04,228 [INFO] Step 9 - eval_loss: 0.9288
Validation Loss: 0.9288
Epoch 11/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:31<00:00,  2.64it/s]
2025-04-09 01:08:35,545 [INFO] Step 10 - total_loss: 0.8067, reason_loss: 0.0495, ans_loss: 1.0591
Epoch 11 - Loss: 0.8067 (Reason: 0.0495, Ans: 1.0591)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:16<00:00,  6.07it/s]
Evaluation - Total Loss: 0.9554, Reason Loss: 0.0499, Answer Loss: 1.2572
2025-04-09 01:08:52,029 [INFO] Step 10 - eval_loss: 0.9554
Validation Loss: 0.9554
Epoch 12/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [03:52<00:00,  1.72it/s]
2025-04-09 01:12:44,950 [INFO] Step 11 - total_loss: 0.7409, reason_loss: 0.0524, ans_loss: 0.9704
Epoch 12 - Loss: 0.7409 (Reason: 0.0524, Ans: 0.9704)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:16<00:00,  6.03it/s]
Evaluation - Total Loss: 0.9427, Reason Loss: 0.0487, Answer Loss: 1.2407
2025-04-09 01:13:01,523 [INFO] Step 11 - eval_loss: 0.9427
Validation Loss: 0.9427
Saved best model with validation loss: 0.9288
----------------------------------------
Running: python main.py --config mistral --mode evaluate --dataset multiarith  --device 0
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.24it/s]
Average time taken for each sample: 0.803415949344635, Average time taken for contemplation: 0.027741334438323974
Evaluation results: {'numerical_accuracy': 0.16, 'close_match_rate': 0.16, 'mean_relative_error': np.float64(0.3656063809889079), 'median_relative_error': np.float64(0.28348214285714285)}
----------------------------------------
Starting evaluation runs with 45 combinations...
[1/45] Running with max_contemp_tokens=1, eval_temp=0.1, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:22<00:00,  1.22it/s]
Average time taken for each sample: 0.814949369430542, Average time taken for contemplation: 0.020103189945220947
Evaluation results: {'numerical_accuracy': 0.09, 'close_match_rate': 0.09, 'mean_relative_error': np.float64(0.7530564224896713), 'median_relative_error': np.float64(0.43354735152487966)}
[2/45] Running with max_contemp_tokens=1, eval_temp=0.2, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:21<00:00,  1.22it/s]
Average time taken for each sample: 0.8139849734306336, Average time taken for contemplation: 0.019741792678833008
Evaluation results: {'numerical_accuracy': 0.09, 'close_match_rate': 0.09, 'mean_relative_error': np.float64(0.7680341670255918), 'median_relative_error': np.float64(0.39375000000000004)}
[3/45] Running with max_contemp_tokens=1, eval_temp=0.3, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:21<00:00,  1.23it/s]
Average time taken for each sample: 0.8070242619514465, Average time taken for contemplation: 0.019719905853271484
Evaluation results: {'numerical_accuracy': 0.07, 'close_match_rate': 0.07, 'mean_relative_error': np.float64(0.7969226701140045), 'median_relative_error': np.float64(0.463884430176565)}
[4/45] Running with max_contemp_tokens=1, eval_temp=0.4, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.50it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:21<00:00,  1.22it/s]
Average time taken for each sample: 0.8111783838272095, Average time taken for contemplation: 0.028085799217224122
Evaluation results: {'numerical_accuracy': 0.07, 'close_match_rate': 0.07, 'mean_relative_error': np.float64(0.8575511658322968), 'median_relative_error': np.float64(0.40588235294117647)}
[5/45] Running with max_contemp_tokens=1, eval_temp=0.5, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:21<00:00,  1.23it/s]
Average time taken for each sample: 0.808885281085968, Average time taken for contemplation: 0.019682826995849608
Evaluation results: {'numerical_accuracy': 0.1, 'close_match_rate': 0.1, 'mean_relative_error': np.float64(0.6904969160064306), 'median_relative_error': np.float64(0.39375000000000004)}
[6/45] Running with max_contemp_tokens=1, eval_temp=0.6, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.50it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:21<00:00,  1.23it/s]
Average time taken for each sample: 0.8107679486274719, Average time taken for contemplation: 0.019373905658721925
Evaluation results: {'numerical_accuracy': 0.09, 'close_match_rate': 0.09, 'mean_relative_error': np.float64(0.6670921551707026), 'median_relative_error': np.float64(0.3653846153846154)}
[7/45] Running with max_contemp_tokens=1, eval_temp=0.7, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.51it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:21<00:00,  1.23it/s]
Average time taken for each sample: 0.8076353263854981, Average time taken for contemplation: 0.025863373279571535
Evaluation results: {'numerical_accuracy': 0.08, 'close_match_rate': 0.08, 'mean_relative_error': np.float64(0.7293986552833361), 'median_relative_error': np.float64(0.3860576923076923)}
[8/45] Running with max_contemp_tokens=1, eval_temp=0.8, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.24it/s]
Average time taken for each sample: 0.7984095549583435, Average time taken for contemplation: 0.01960111141204834
Evaluation results: {'numerical_accuracy': 0.08, 'close_match_rate': 0.08, 'mean_relative_error': np.float64(0.6768537459200231), 'median_relative_error': np.float64(0.375)}
[9/45] Running with max_contemp_tokens=1, eval_temp=0.9, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.50it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:21<00:00,  1.23it/s]
Average time taken for each sample: 0.8112447452545166, Average time taken for contemplation: 0.02951826572418213
Evaluation results: {'numerical_accuracy': 0.08, 'close_match_rate': 0.08, 'mean_relative_error': np.float64(0.7122291133013944), 'median_relative_error': np.float64(0.3875)}
[10/45] Running with max_contemp_tokens=2, eval_temp=0.1, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:21<00:00,  1.23it/s]
Average time taken for each sample: 0.8090494465827942, Average time taken for contemplation: 0.019875247478485108
Evaluation results: {'numerical_accuracy': 0.09, 'close_match_rate': 0.09, 'mean_relative_error': np.float64(0.7530564224896713), 'median_relative_error': np.float64(0.43354735152487966)}
[11/45] Running with max_contemp_tokens=2, eval_temp=0.2, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:21<00:00,  1.23it/s]
Average time taken for each sample: 0.8110687375068665, Average time taken for contemplation: 0.029736526012420654
Evaluation results: {'numerical_accuracy': 0.09, 'close_match_rate': 0.09, 'mean_relative_error': np.float64(0.7680341670255918), 'median_relative_error': np.float64(0.39375000000000004)}
[12/45] Running with max_contemp_tokens=2, eval_temp=0.3, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.50it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:22<00:00,  1.21it/s]
Average time taken for each sample: 0.8207740211486816, Average time taken for contemplation: 0.02052032470703125
Evaluation results: {'numerical_accuracy': 0.07, 'close_match_rate': 0.07, 'mean_relative_error': np.float64(0.7969226701140045), 'median_relative_error': np.float64(0.463884430176565)}
[13/45] Running with max_contemp_tokens=2, eval_temp=0.4, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:23<00:00,  1.19it/s]
Average time taken for each sample: 0.8336753296852112, Average time taken for contemplation: 0.020494842529296876
Evaluation results: {'numerical_accuracy': 0.07, 'close_match_rate': 0.07, 'mean_relative_error': np.float64(0.8575511658322968), 'median_relative_error': np.float64(0.40588235294117647)}
[14/45] Running with max_contemp_tokens=2, eval_temp=0.5, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:22<00:00,  1.22it/s]
Average time taken for each sample: 0.8166451072692871, Average time taken for contemplation: 0.03001387119293213
Evaluation results: {'numerical_accuracy': 0.1, 'close_match_rate': 0.1, 'mean_relative_error': np.float64(0.6904969160064306), 'median_relative_error': np.float64(0.39375000000000004)}
[15/45] Running with max_contemp_tokens=2, eval_temp=0.6, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:21<00:00,  1.22it/s]
Average time taken for each sample: 0.8138532662391662, Average time taken for contemplation: 0.01916412830352783
Evaluation results: {'numerical_accuracy': 0.09, 'close_match_rate': 0.09, 'mean_relative_error': np.float64(0.6670921551707026), 'median_relative_error': np.float64(0.3653846153846154)}
[16/45] Running with max_contemp_tokens=2, eval_temp=0.7, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.33it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:24<00:00,  1.18it/s]
Average time taken for each sample: 0.8405420756340027, Average time taken for contemplation: 0.03881441116333008
Evaluation results: {'numerical_accuracy': 0.08, 'close_match_rate': 0.08, 'mean_relative_error': np.float64(0.7293986552833361), 'median_relative_error': np.float64(0.3860576923076923)}
[17/45] Running with max_contemp_tokens=2, eval_temp=0.8, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.30s/it]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:23<00:00,  1.19it/s]
Average time taken for each sample: 0.8334691476821899, Average time taken for contemplation: 0.028917574882507326
Evaluation results: {'numerical_accuracy': 0.08, 'close_match_rate': 0.08, 'mean_relative_error': np.float64(0.6768537459200231), 'median_relative_error': np.float64(0.375)}
[18/45] Running with max_contemp_tokens=2, eval_temp=0.9, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.16it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:25<00:00,  1.17it/s]
Average time taken for each sample: 0.8465804028511047, Average time taken for contemplation: 0.02755049467086792
Evaluation results: {'numerical_accuracy': 0.08, 'close_match_rate': 0.08, 'mean_relative_error': np.float64(0.7122291133013944), 'median_relative_error': np.float64(0.3875)}
[19/45] Running with max_contemp_tokens=3, eval_temp=0.1, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:28<00:00,  1.12it/s]
Average time taken for each sample: 0.8850171065330505, Average time taken for contemplation: 0.05015869379043579
Evaluation results: {'numerical_accuracy': 0.09, 'close_match_rate': 0.09, 'mean_relative_error': np.float64(0.7530564224896713), 'median_relative_error': np.float64(0.43354735152487966)}
[20/45] Running with max_contemp_tokens=3, eval_temp=0.2, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:26<00:00,  1.15it/s]
Average time taken for each sample: 0.864823043346405, Average time taken for contemplation: 0.0380488133430481
Evaluation results: {'numerical_accuracy': 0.09, 'close_match_rate': 0.09, 'mean_relative_error': np.float64(0.7680341670255918), 'median_relative_error': np.float64(0.39375000000000004)}
[21/45] Running with max_contemp_tokens=3, eval_temp=0.3, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:26<00:00,  1.16it/s]
Average time taken for each sample: 0.8560115122795104, Average time taken for contemplation: 0.028554141521453857
Evaluation results: {'numerical_accuracy': 0.07, 'close_match_rate': 0.07, 'mean_relative_error': np.float64(0.7969226701140045), 'median_relative_error': np.float64(0.463884430176565)}
[22/45] Running with max_contemp_tokens=3, eval_temp=0.4, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:24<00:00,  1.19it/s]
Average time taken for each sample: 0.8375134682655334, Average time taken for contemplation: 0.028447833061218262
Evaluation results: {'numerical_accuracy': 0.07, 'close_match_rate': 0.07, 'mean_relative_error': np.float64(0.8575511658322968), 'median_relative_error': np.float64(0.40588235294117647)}
[23/45] Running with max_contemp_tokens=3, eval_temp=0.5, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:25<00:00,  1.18it/s]
Average time taken for each sample: 0.8450929141044616, Average time taken for contemplation: 0.028124165534973145
Evaluation results: {'numerical_accuracy': 0.1, 'close_match_rate': 0.1, 'mean_relative_error': np.float64(0.6904969160064306), 'median_relative_error': np.float64(0.39375000000000004)}
[24/45] Running with max_contemp_tokens=3, eval_temp=0.6, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:24<00:00,  1.19it/s]
Average time taken for each sample: 0.8378781366348267, Average time taken for contemplation: 0.02612373113632202
Evaluation results: {'numerical_accuracy': 0.09, 'close_match_rate': 0.09, 'mean_relative_error': np.float64(0.6670921551707026), 'median_relative_error': np.float64(0.3653846153846154)}
[25/45] Running with max_contemp_tokens=3, eval_temp=0.7, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.43it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:25<00:00,  1.16it/s]
Average time taken for each sample: 0.8549533987045288, Average time taken for contemplation: 0.03770916223526001
Evaluation results: {'numerical_accuracy': 0.08, 'close_match_rate': 0.08, 'mean_relative_error': np.float64(0.7293986552833361), 'median_relative_error': np.float64(0.3860576923076923)}
[26/45] Running with max_contemp_tokens=3, eval_temp=0.8, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.43it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:24<00:00,  1.18it/s]
Average time taken for each sample: 0.8444484519958496, Average time taken for contemplation: 0.029117746353149412
Evaluation results: {'numerical_accuracy': 0.08, 'close_match_rate': 0.08, 'mean_relative_error': np.float64(0.6768537459200231), 'median_relative_error': np.float64(0.375)}
[27/45] Running with max_contemp_tokens=3, eval_temp=0.9, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:23<00:00,  1.19it/s]
Average time taken for each sample: 0.8332311534881591, Average time taken for contemplation: 0.027496261596679686
Evaluation results: {'numerical_accuracy': 0.08, 'close_match_rate': 0.08, 'mean_relative_error': np.float64(0.7122291133013944), 'median_relative_error': np.float64(0.3875)}
[28/45] Running with max_contemp_tokens=4, eval_temp=0.1, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.24it/s]
Average time taken for each sample: 0.8025169014930725, Average time taken for contemplation: 0.019886662960052492
Evaluation results: {'numerical_accuracy': 0.09, 'close_match_rate': 0.09, 'mean_relative_error': np.float64(0.7530564224896713), 'median_relative_error': np.float64(0.43354735152487966)}
[29/45] Running with max_contemp_tokens=4, eval_temp=0.2, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.36it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:21<00:00,  1.23it/s]
Average time taken for each sample: 0.8088649606704712, Average time taken for contemplation: 0.020215418338775635
Evaluation results: {'numerical_accuracy': 0.09, 'close_match_rate': 0.09, 'mean_relative_error': np.float64(0.7680341670255918), 'median_relative_error': np.float64(0.39375000000000004)}
[30/45] Running with max_contemp_tokens=4, eval_temp=0.3, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:21<00:00,  1.23it/s]
Average time taken for each sample: 0.8090363955497741, Average time taken for contemplation: 0.019956240653991698
Evaluation results: {'numerical_accuracy': 0.07, 'close_match_rate': 0.07, 'mean_relative_error': np.float64(0.7969226701140045), 'median_relative_error': np.float64(0.463884430176565)}
[31/45] Running with max_contemp_tokens=4, eval_temp=0.4, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:21<00:00,  1.23it/s]
Average time taken for each sample: 0.8092054438591003, Average time taken for contemplation: 0.02020768165588379
Evaluation results: {'numerical_accuracy': 0.07, 'close_match_rate': 0.07, 'mean_relative_error': np.float64(0.8575511658322968), 'median_relative_error': np.float64(0.40588235294117647)}
[32/45] Running with max_contemp_tokens=4, eval_temp=0.5, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:22<00:00,  1.22it/s]
Average time taken for each sample: 0.8165404415130615, Average time taken for contemplation: 0.02034373760223389
Evaluation results: {'numerical_accuracy': 0.1, 'close_match_rate': 0.1, 'mean_relative_error': np.float64(0.6904969160064306), 'median_relative_error': np.float64(0.39375000000000004)}
[33/45] Running with max_contemp_tokens=4, eval_temp=0.6, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:24<00:00,  1.18it/s]
Average time taken for each sample: 0.8418367409706116, Average time taken for contemplation: 0.02083704710006714
Evaluation results: {'numerical_accuracy': 0.09, 'close_match_rate': 0.09, 'mean_relative_error': np.float64(0.6670921551707026), 'median_relative_error': np.float64(0.3653846153846154)}
[34/45] Running with max_contemp_tokens=4, eval_temp=0.7, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.24it/s]
Average time taken for each sample: 0.799643075466156, Average time taken for contemplation: 0.020357933044433594
Evaluation results: {'numerical_accuracy': 0.08, 'close_match_rate': 0.08, 'mean_relative_error': np.float64(0.7293986552833361), 'median_relative_error': np.float64(0.3860576923076923)}
[35/45] Running with max_contemp_tokens=4, eval_temp=0.8, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.16s/it]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.24it/s]
Average time taken for each sample: 0.8043833613395691, Average time taken for contemplation: 0.020647573471069335
Evaluation results: {'numerical_accuracy': 0.08, 'close_match_rate': 0.08, 'mean_relative_error': np.float64(0.6768537459200231), 'median_relative_error': np.float64(0.375)}
[36/45] Running with max_contemp_tokens=4, eval_temp=0.9, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.24it/s]
Average time taken for each sample: 0.8011258435249329, Average time taken for contemplation: 0.020056304931640626
Evaluation results: {'numerical_accuracy': 0.08, 'close_match_rate': 0.08, 'mean_relative_error': np.float64(0.7122291133013944), 'median_relative_error': np.float64(0.3875)}
[37/45] Running with max_contemp_tokens=5, eval_temp=0.1, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.23it/s]
Average time taken for each sample: 0.8049992895126343, Average time taken for contemplation: 0.019448256492614745
Evaluation results: {'numerical_accuracy': 0.09, 'close_match_rate': 0.09, 'mean_relative_error': np.float64(0.7530564224896713), 'median_relative_error': np.float64(0.43354735152487966)}
[38/45] Running with max_contemp_tokens=5, eval_temp=0.2, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.24it/s]
Average time taken for each sample: 0.8029933929443359, Average time taken for contemplation: 0.021001553535461424
Evaluation results: {'numerical_accuracy': 0.09, 'close_match_rate': 0.09, 'mean_relative_error': np.float64(0.7680341670255918), 'median_relative_error': np.float64(0.39375000000000004)}
[39/45] Running with max_contemp_tokens=5, eval_temp=0.3, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.41it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.24it/s]
Average time taken for each sample: 0.8021525692939758, Average time taken for contemplation: 0.019656839370727538
Evaluation results: {'numerical_accuracy': 0.07, 'close_match_rate': 0.07, 'mean_relative_error': np.float64(0.7969226701140045), 'median_relative_error': np.float64(0.463884430176565)}
[40/45] Running with max_contemp_tokens=5, eval_temp=0.4, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.43it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.24it/s]
Average time taken for each sample: 0.801820514202118, Average time taken for contemplation: 0.0194427227973938
Evaluation results: {'numerical_accuracy': 0.07, 'close_match_rate': 0.07, 'mean_relative_error': np.float64(0.8575511658322968), 'median_relative_error': np.float64(0.40588235294117647)}
[41/45] Running with max_contemp_tokens=5, eval_temp=0.5, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.24it/s]
Average time taken for each sample: 0.8031487464904785, Average time taken for contemplation: 0.01945493936538696
Evaluation results: {'numerical_accuracy': 0.1, 'close_match_rate': 0.1, 'mean_relative_error': np.float64(0.6904969160064306), 'median_relative_error': np.float64(0.39375000000000004)}
[42/45] Running with max_contemp_tokens=5, eval_temp=0.6, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.51it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:21<00:00,  1.23it/s]
Average time taken for each sample: 0.8051105642318725, Average time taken for contemplation: 0.019405779838562013
Evaluation results: {'numerical_accuracy': 0.09, 'close_match_rate': 0.09, 'mean_relative_error': np.float64(0.6670921551707026), 'median_relative_error': np.float64(0.3653846153846154)}
[43/45] Running with max_contemp_tokens=5, eval_temp=0.7, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.25it/s]
Average time taken for each sample: 0.7973114037513733, Average time taken for contemplation: 0.020076889991760254
Evaluation results: {'numerical_accuracy': 0.08, 'close_match_rate': 0.08, 'mean_relative_error': np.float64(0.7293986552833361), 'median_relative_error': np.float64(0.3860576923076923)}
[44/45] Running with max_contemp_tokens=5, eval_temp=0.8, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.43it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.24it/s]
Average time taken for each sample: 0.8019796848297119, Average time taken for contemplation: 0.01991746425628662
Evaluation results: {'numerical_accuracy': 0.08, 'close_match_rate': 0.08, 'mean_relative_error': np.float64(0.6768537459200231), 'median_relative_error': np.float64(0.375)}
[45/45] Running with max_contemp_tokens=5, eval_temp=0.9, dataset=gsm8k, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:21<00:00,  1.22it/s]
Average time taken for each sample: 0.8140589952468872, Average time taken for contemplation: 0.01939199209213257
Evaluation results: {'numerical_accuracy': 0.08, 'close_match_rate': 0.08, 'mean_relative_error': np.float64(0.7122291133013944), 'median_relative_error': np.float64(0.3875)}
[46/45] Running with max_contemp_tokens=1, eval_temp=0.1, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:19<00:00,  1.25it/s]
Average time taken for each sample: 0.795030767917633, Average time taken for contemplation: 0.020276098251342772
Evaluation results: {'numerical_accuracy': 0.15, 'close_match_rate': 0.15, 'mean_relative_error': np.float64(0.37320839743436834), 'median_relative_error': np.float64(0.2857142857142857)}
[47/45] Running with max_contemp_tokens=1, eval_temp=0.2, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:19<00:00,  1.26it/s]
Average time taken for each sample: 0.7898926448822021, Average time taken for contemplation: 0.0279176926612854
Evaluation results: {'numerical_accuracy': 0.15, 'close_match_rate': 0.15, 'mean_relative_error': np.float64(0.38180958791055886), 'median_relative_error': np.float64(0.3009433962264151)}
[48/45] Running with max_contemp_tokens=1, eval_temp=0.3, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.43it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.24it/s]
Average time taken for each sample: 0.7998682546615601, Average time taken for contemplation: 0.029481050968170167
Evaluation results: {'numerical_accuracy': 0.16, 'close_match_rate': 0.16, 'mean_relative_error': np.float64(0.3656063809889079), 'median_relative_error': np.float64(0.28348214285714285)}
[49/45] Running with max_contemp_tokens=1, eval_temp=0.4, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.52it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:18<00:00,  1.28it/s]
Average time taken for each sample: 0.7799876618385315, Average time taken for contemplation: 0.028394365310668947
Evaluation results: {'numerical_accuracy': 0.16, 'close_match_rate': 0.16, 'mean_relative_error': np.float64(0.3998910770056795), 'median_relative_error': np.float64(0.3125)}
[50/45] Running with max_contemp_tokens=1, eval_temp=0.5, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.50it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:21<00:00,  1.22it/s]
Average time taken for each sample: 0.815873589515686, Average time taken for contemplation: 0.019367222785949708
Evaluation results: {'numerical_accuracy': 0.14, 'close_match_rate': 0.14, 'mean_relative_error': np.float64(0.38487580621408723), 'median_relative_error': np.float64(0.27521008403361347)}
[51/45] Running with max_contemp_tokens=1, eval_temp=0.6, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.24it/s]
Average time taken for each sample: 0.8026274013519287, Average time taken for contemplation: 0.020364999771118164
Evaluation results: {'numerical_accuracy': 0.17, 'close_match_rate': 0.17, 'mean_relative_error': np.float64(0.38302322979785586), 'median_relative_error': np.float64(0.3009433962264151)}
[52/45] Running with max_contemp_tokens=1, eval_temp=0.7, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:21<00:00,  1.23it/s]
Average time taken for each sample: 0.8125644564628601, Average time taken for contemplation: 0.019564192295074462
Evaluation results: {'numerical_accuracy': 0.15, 'close_match_rate': 0.15, 'mean_relative_error': np.float64(0.38970062245131326), 'median_relative_error': np.float64(0.2928571428571428)}
[53/45] Running with max_contemp_tokens=1, eval_temp=0.8, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:23<00:00,  1.19it/s]
Average time taken for each sample: 0.8340193939208984, Average time taken for contemplation: 0.03855162382125855
Evaluation results: {'numerical_accuracy': 0.13, 'close_match_rate': 0.13, 'mean_relative_error': np.float64(0.41531091716110735), 'median_relative_error': np.float64(0.3333333333333333)}
[54/45] Running with max_contemp_tokens=1, eval_temp=0.9, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.14it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.25it/s]
Average time taken for each sample: 0.7983444690704345, Average time taken for contemplation: 0.02003152370452881
Evaluation results: {'numerical_accuracy': 0.13, 'close_match_rate': 0.13, 'mean_relative_error': np.float64(0.48951230604999624), 'median_relative_error': np.float64(0.3333333333333333)}
[55/45] Running with max_contemp_tokens=2, eval_temp=0.1, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.12s/it]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:19<00:00,  1.26it/s]
Average time taken for each sample: 0.793131685256958, Average time taken for contemplation: 0.027455503940582274
Evaluation results: {'numerical_accuracy': 0.15, 'close_match_rate': 0.15, 'mean_relative_error': np.float64(0.37320839743436834), 'median_relative_error': np.float64(0.2857142857142857)}
[56/45] Running with max_contemp_tokens=2, eval_temp=0.2, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.05s/it]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:19<00:00,  1.26it/s]
Average time taken for each sample: 0.7885640406608582, Average time taken for contemplation: 0.019989564418792724
Evaluation results: {'numerical_accuracy': 0.15, 'close_match_rate': 0.15, 'mean_relative_error': np.float64(0.38180958791055886), 'median_relative_error': np.float64(0.3009433962264151)}
[57/45] Running with max_contemp_tokens=2, eval_temp=0.3, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:05<00:00,  1.67s/it]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:19<00:00,  1.26it/s]
Average time taken for each sample: 0.7912032890319824, Average time taken for contemplation: 0.020175135135650633
Evaluation results: {'numerical_accuracy': 0.16, 'close_match_rate': 0.16, 'mean_relative_error': np.float64(0.3656063809889079), 'median_relative_error': np.float64(0.28348214285714285)}
[58/45] Running with max_contemp_tokens=2, eval_temp=0.4, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.37it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:17<00:00,  1.30it/s]
Average time taken for each sample: 0.7671138834953308, Average time taken for contemplation: 0.019499306678771974
Evaluation results: {'numerical_accuracy': 0.16, 'close_match_rate': 0.16, 'mean_relative_error': np.float64(0.3998910770056795), 'median_relative_error': np.float64(0.3125)}
[59/45] Running with max_contemp_tokens=2, eval_temp=0.5, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.42it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:21<00:00,  1.22it/s]
Average time taken for each sample: 0.8138696718215942, Average time taken for contemplation: 0.019601945877075196
Evaluation results: {'numerical_accuracy': 0.14, 'close_match_rate': 0.14, 'mean_relative_error': np.float64(0.38487580621408723), 'median_relative_error': np.float64(0.27521008403361347)}
[60/45] Running with max_contemp_tokens=2, eval_temp=0.6, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:21<00:00,  1.23it/s]
Average time taken for each sample: 0.8086290884017945, Average time taken for contemplation: 0.019666218757629396
Evaluation results: {'numerical_accuracy': 0.17, 'close_match_rate': 0.17, 'mean_relative_error': np.float64(0.38302322979785586), 'median_relative_error': np.float64(0.3009433962264151)}
[61/45] Running with max_contemp_tokens=2, eval_temp=0.7, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.05it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.24it/s]
Average time taken for each sample: 0.8002869367599488, Average time taken for contemplation: 0.01954488515853882
Evaluation results: {'numerical_accuracy': 0.15, 'close_match_rate': 0.15, 'mean_relative_error': np.float64(0.38970062245131326), 'median_relative_error': np.float64(0.2928571428571428)}
[62/45] Running with max_contemp_tokens=2, eval_temp=0.8, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.40it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.25it/s]
Average time taken for each sample: 0.798442406654358, Average time taken for contemplation: 0.020273663997650147
Evaluation results: {'numerical_accuracy': 0.13, 'close_match_rate': 0.13, 'mean_relative_error': np.float64(0.41531091716110735), 'median_relative_error': np.float64(0.3333333333333333)}
[63/45] Running with max_contemp_tokens=2, eval_temp=0.9, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.43it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:21<00:00,  1.23it/s]
Average time taken for each sample: 0.8091727662086486, Average time taken for contemplation: 0.020118565559387208
Evaluation results: {'numerical_accuracy': 0.13, 'close_match_rate': 0.13, 'mean_relative_error': np.float64(0.48951230604999624), 'median_relative_error': np.float64(0.3333333333333333)}
[64/45] Running with max_contemp_tokens=3, eval_temp=0.1, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.25it/s]
Average time taken for each sample: 0.7974960184097291, Average time taken for contemplation: 0.02034393548965454
Evaluation results: {'numerical_accuracy': 0.15, 'close_match_rate': 0.15, 'mean_relative_error': np.float64(0.37320839743436834), 'median_relative_error': np.float64(0.2857142857142857)}
[65/45] Running with max_contemp_tokens=3, eval_temp=0.2, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:19<00:00,  1.26it/s]
Average time taken for each sample: 0.7912362694740296, Average time taken for contemplation: 0.02048571586608887
Evaluation results: {'numerical_accuracy': 0.15, 'close_match_rate': 0.15, 'mean_relative_error': np.float64(0.38180958791055886), 'median_relative_error': np.float64(0.3009433962264151)}
[66/45] Running with max_contemp_tokens=3, eval_temp=0.3, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.25it/s]
Average time taken for each sample: 0.7984266996383667, Average time taken for contemplation: 0.020143582820892333
Evaluation results: {'numerical_accuracy': 0.16, 'close_match_rate': 0.16, 'mean_relative_error': np.float64(0.3656063809889079), 'median_relative_error': np.float64(0.28348214285714285)}
[67/45] Running with max_contemp_tokens=3, eval_temp=0.4, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.02it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:19<00:00,  1.26it/s]
Average time taken for each sample: 0.7926953768730164, Average time taken for contemplation: 0.01954223871231079
Evaluation results: {'numerical_accuracy': 0.16, 'close_match_rate': 0.16, 'mean_relative_error': np.float64(0.3998910770056795), 'median_relative_error': np.float64(0.3125)}
[68/45] Running with max_contemp_tokens=3, eval_temp=0.5, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:07<00:00,  2.65s/it]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:22<00:00,  1.22it/s]
Average time taken for each sample: 0.8175917100906372, Average time taken for contemplation: 0.019845330715179445
Evaluation results: {'numerical_accuracy': 0.14, 'close_match_rate': 0.14, 'mean_relative_error': np.float64(0.38487580621408723), 'median_relative_error': np.float64(0.27521008403361347)}
[69/45] Running with max_contemp_tokens=3, eval_temp=0.6, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:04<00:00,  1.39s/it]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.25it/s]
Average time taken for each sample: 0.797668182849884, Average time taken for contemplation: 0.020532727241516113
Evaluation results: {'numerical_accuracy': 0.17, 'close_match_rate': 0.17, 'mean_relative_error': np.float64(0.38302322979785586), 'median_relative_error': np.float64(0.3009433962264151)}
[70/45] Running with max_contemp_tokens=3, eval_temp=0.7, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.25it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:19<00:00,  1.26it/s]
Average time taken for each sample: 0.7881706833839417, Average time taken for contemplation: 0.019903812408447265
Evaluation results: {'numerical_accuracy': 0.15, 'close_match_rate': 0.15, 'mean_relative_error': np.float64(0.38970062245131326), 'median_relative_error': np.float64(0.2928571428571428)}
[71/45] Running with max_contemp_tokens=3, eval_temp=0.8, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.41it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.24it/s]
Average time taken for each sample: 0.8010527992248535, Average time taken for contemplation: 0.01984447717666626
Evaluation results: {'numerical_accuracy': 0.13, 'close_match_rate': 0.13, 'mean_relative_error': np.float64(0.41531091716110735), 'median_relative_error': np.float64(0.3333333333333333)}
[72/45] Running with max_contemp_tokens=3, eval_temp=0.9, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.41it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:18<00:00,  1.27it/s]
Average time taken for each sample: 0.7836247086524963, Average time taken for contemplation: 0.019771873950958252
Evaluation results: {'numerical_accuracy': 0.13, 'close_match_rate': 0.13, 'mean_relative_error': np.float64(0.48951230604999624), 'median_relative_error': np.float64(0.3333333333333333)}
[73/45] Running with max_contemp_tokens=4, eval_temp=0.1, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.43it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:18<00:00,  1.27it/s]
Average time taken for each sample: 0.781761863231659, Average time taken for contemplation: 0.01974886417388916
Evaluation results: {'numerical_accuracy': 0.15, 'close_match_rate': 0.15, 'mean_relative_error': np.float64(0.37320839743436834), 'median_relative_error': np.float64(0.2857142857142857)}
[74/45] Running with max_contemp_tokens=4, eval_temp=0.2, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.24it/s]
Average time taken for each sample: 0.800462076663971, Average time taken for contemplation: 0.021023468971252443
Evaluation results: {'numerical_accuracy': 0.15, 'close_match_rate': 0.15, 'mean_relative_error': np.float64(0.38180958791055886), 'median_relative_error': np.float64(0.3009433962264151)}
[75/45] Running with max_contemp_tokens=4, eval_temp=0.3, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.25it/s]
Average time taken for each sample: 0.7987629008293152, Average time taken for contemplation: 0.019483749866485597
Evaluation results: {'numerical_accuracy': 0.16, 'close_match_rate': 0.16, 'mean_relative_error': np.float64(0.3656063809889079), 'median_relative_error': np.float64(0.28348214285714285)}
[76/45] Running with max_contemp_tokens=4, eval_temp=0.4, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:04<00:00,  1.57s/it]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:18<00:00,  1.27it/s]
Average time taken for each sample: 0.7818107390403748, Average time taken for contemplation: 0.02041727304458618
Evaluation results: {'numerical_accuracy': 0.16, 'close_match_rate': 0.16, 'mean_relative_error': np.float64(0.3998910770056795), 'median_relative_error': np.float64(0.3125)}
[77/45] Running with max_contemp_tokens=4, eval_temp=0.5, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:22<00:00,  1.21it/s]
Average time taken for each sample: 0.8201733756065369, Average time taken for contemplation: 0.020471889972686768
Evaluation results: {'numerical_accuracy': 0.14, 'close_match_rate': 0.14, 'mean_relative_error': np.float64(0.38487580621408723), 'median_relative_error': np.float64(0.27521008403361347)}
[78/45] Running with max_contemp_tokens=4, eval_temp=0.6, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.25it/s]
Average time taken for each sample: 0.7977380228042602, Average time taken for contemplation: 0.020043091773986818
Evaluation results: {'numerical_accuracy': 0.17, 'close_match_rate': 0.17, 'mean_relative_error': np.float64(0.38302322979785586), 'median_relative_error': np.float64(0.3009433962264151)}
[79/45] Running with max_contemp_tokens=4, eval_temp=0.7, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.24s/it]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.25it/s]
Average time taken for each sample: 0.7987926459312439, Average time taken for contemplation: 0.019697825908660888
Evaluation results: {'numerical_accuracy': 0.15, 'close_match_rate': 0.15, 'mean_relative_error': np.float64(0.38970062245131326), 'median_relative_error': np.float64(0.2928571428571428)}
[80/45] Running with max_contemp_tokens=4, eval_temp=0.8, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:21<00:00,  1.23it/s]
Average time taken for each sample: 0.8120613169670104, Average time taken for contemplation: 0.02046902894973755
Evaluation results: {'numerical_accuracy': 0.13, 'close_match_rate': 0.13, 'mean_relative_error': np.float64(0.41531091716110735), 'median_relative_error': np.float64(0.3333333333333333)}
[81/45] Running with max_contemp_tokens=4, eval_temp=0.9, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.24it/s]
Average time taken for each sample: 0.8050468444824219, Average time taken for contemplation: 0.020503621101379394
Evaluation results: {'numerical_accuracy': 0.13, 'close_match_rate': 0.13, 'mean_relative_error': np.float64(0.48951230604999624), 'median_relative_error': np.float64(0.3333333333333333)}
[82/45] Running with max_contemp_tokens=5, eval_temp=0.1, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:18<00:00,  1.27it/s]
Average time taken for each sample: 0.7818838620185852, Average time taken for contemplation: 0.01948608160018921
Evaluation results: {'numerical_accuracy': 0.15, 'close_match_rate': 0.15, 'mean_relative_error': np.float64(0.37320839743436834), 'median_relative_error': np.float64(0.2857142857142857)}
[83/45] Running with max_contemp_tokens=5, eval_temp=0.2, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:19<00:00,  1.26it/s]
Average time taken for each sample: 0.7896474504470825, Average time taken for contemplation: 0.019522626399993897
Evaluation results: {'numerical_accuracy': 0.15, 'close_match_rate': 0.15, 'mean_relative_error': np.float64(0.38180958791055886), 'median_relative_error': np.float64(0.3009433962264151)}
[84/45] Running with max_contemp_tokens=5, eval_temp=0.3, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.51it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.25it/s]
Average time taken for each sample: 0.7968649411201477, Average time taken for contemplation: 0.01945434808731079
Evaluation results: {'numerical_accuracy': 0.16, 'close_match_rate': 0.16, 'mean_relative_error': np.float64(0.3656063809889079), 'median_relative_error': np.float64(0.28348214285714285)}
[85/45] Running with max_contemp_tokens=5, eval_temp=0.4, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.31it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:17<00:00,  1.29it/s]
Average time taken for each sample: 0.7709899950027466, Average time taken for contemplation: 0.019467523097991945
Evaluation results: {'numerical_accuracy': 0.16, 'close_match_rate': 0.16, 'mean_relative_error': np.float64(0.3998910770056795), 'median_relative_error': np.float64(0.3125)}
[86/45] Running with max_contemp_tokens=5, eval_temp=0.5, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:04<00:00,  1.49s/it]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:22<00:00,  1.21it/s]
Average time taken for each sample: 0.8243379592895508, Average time taken for contemplation: 0.01983429431915283
Evaluation results: {'numerical_accuracy': 0.14, 'close_match_rate': 0.14, 'mean_relative_error': np.float64(0.38487580621408723), 'median_relative_error': np.float64(0.27521008403361347)}
[87/45] Running with max_contemp_tokens=5, eval_temp=0.6, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.03s/it]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.24it/s]
Average time taken for each sample: 0.803117573261261, Average time taken for contemplation: 0.02038931369781494
Evaluation results: {'numerical_accuracy': 0.17, 'close_match_rate': 0.17, 'mean_relative_error': np.float64(0.38302322979785586), 'median_relative_error': np.float64(0.3009433962264151)}
[88/45] Running with max_contemp_tokens=5, eval_temp=0.7, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.34it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.24it/s]
Average time taken for each sample: 0.8016114854812622, Average time taken for contemplation: 0.01964724063873291
Evaluation results: {'numerical_accuracy': 0.15, 'close_match_rate': 0.15, 'mean_relative_error': np.float64(0.38970062245131326), 'median_relative_error': np.float64(0.2928571428571428)}
[89/45] Running with max_contemp_tokens=5, eval_temp=0.8, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.24it/s]
Average time taken for each sample: 0.8023200631141663, Average time taken for contemplation: 0.02004749536514282
Evaluation results: {'numerical_accuracy': 0.13, 'close_match_rate': 0.13, 'mean_relative_error': np.float64(0.41531091716110735), 'median_relative_error': np.float64(0.3333333333333333)}
[90/45] Running with max_contemp_tokens=5, eval_temp=0.9, dataset=multiarith, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.42it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:20<00:00,  1.24it/s]
Average time taken for each sample: 0.8052522730827332, Average time taken for contemplation: 0.03121434450149536
Evaluation results: {'numerical_accuracy': 0.13, 'close_match_rate': 0.13, 'mean_relative_error': np.float64(0.48951230604999624), 'median_relative_error': np.float64(0.3333333333333333)}
All combinations completed.
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ tmux capture-pane -p -S - > no_cot_mistral_gsm8k_multiarith.txt

