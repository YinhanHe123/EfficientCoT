[69/45] Running with max_contemp_tokens=3, eval_temp=0.6, dataset=svamp
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 2/2 [00:01<00:00,  1.07it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you m
ay observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:06<00:00,  1.50it/s]
Average time taken for each sample: 0.6607211136817932, Average time taken for contemplation: 0.02076689004898071
Evaluation results: {'numerical_accuracy': 0.32, 'close_match_rate': 0.32, 'mean_relative_error': np.float64(1.6554565120615472), 'median_relative_error': np.float64(0.26883468834688345)}
[70/45] Running with max_contemp_tokens=3, eval_temp=0.7, dataset=svamp
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 2/2 [00:01<00:00,  1.07it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you m
ay observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:02<00:00,  1.59it/s]
Average time taken for each sample: 0.622669427394867, Average time taken for contemplation: 0.019328107833862306
Evaluation results: {'numerical_accuracy': 0.31, 'close_match_rate': 0.31, 'mean_relative_error': np.float64(5.387210857578347e+27), 'median_relative_error': np.float64(0.2540322580645161)}
[71/45] Running with max_contemp_tokens=3, eval_temp=0.8, dataset=svamp
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 2/2 [00:01<00:00,  1.09it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you m
ay observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:03<00:00,  1.58it/s]
Average time taken for each sample: 0.6272953343391419, Average time taken for contemplation: 0.019654762744903565
Evaluation results: {'numerical_accuracy': 0.3, 'close_match_rate': 0.3, 'mean_relative_error': np.float64(4.923907969160288e+26), 'median_relative_error': np.float64(0.2727272727272727)}
[72/45] Running with max_contemp_tokens=3, eval_temp=0.9, dataset=svamp
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 2/2 [00:01<00:00,  1.05it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you m
ay observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:01<00:00,  1.63it/s]
Average time taken for each sample: 0.6080379891395569, Average time taken for contemplation: 0.020896477699279783
Evaluation results: {'numerical_accuracy': 0.34, 'close_match_rate': 0.34, 'mean_relative_error': np.float64(1.0059260346309617e+27), 'median_relative_error': np.float64(0.13392857142857142)}
[73/45] Running with max_contemp_tokens=4, eval_temp=0.1, dataset=svamp
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 2/2 [00:01<00:00,  1.09it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you m
ay observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:16<00:00,  1.31it/s]
Average time taken for each sample: 0.7581847953796387, Average time taken for contemplation: 0.02066140413284302
Evaluation results: {'numerical_accuracy': 0.26, 'close_match_rate': 0.26, 'mean_relative_error': np.float64(1.3040365497165427e+29), 'median_relative_error': np.float64(0.40492978435865795)}
[74/45] Running with max_contemp_tokens=4, eval_temp=0.2, dataset=svamp
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 2/2 [00:01<00:00,  1.06it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you m
ay observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:15<00:00,  1.32it/s]
Average time taken for each sample: 0.7513775062561036, Average time taken for contemplation: 0.020548737049102782
Evaluation results: {'numerical_accuracy': 0.27, 'close_match_rate': 0.27, 'mean_relative_error': np.float64(2.977941389933387e+28), 'median_relative_error': np.float64(0.3693181818181818)}
[75/45] Running with max_contemp_tokens=4, eval_temp=0.3, dataset=svamp
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 2/2 [00:02<00:00,  1.11s/it]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you m
ay observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:14<00:00,  1.35it/s]
Average time taken for each sample: 0.7356679105758667, Average time taken for contemplation: 0.020950267314910887
Evaluation results: {'numerical_accuracy': 0.27, 'close_match_rate': 0.27, 'mean_relative_error': np.float64(2.3711108016898052e+30), 'median_relative_error': np.float64(0.25462962962962965)}
[76/45] Running with max_contemp_tokens=4, eval_temp=0.4, dataset=svamp
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 2/2 [00:01<00:00,  1.06it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you m
ay observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:13<00:00,  1.36it/s]
Average time taken for each sample: 0.7289392805099487, Average time taken for contemplation: 0.020241868495941163
Evaluation results: {'numerical_accuracy': 0.27, 'close_match_rate': 0.27, 'mean_relative_error': np.float64(1.0805230611354175e+29), 'median_relative_error': np.float64(0.25)}
[77/45] Running with max_contemp_tokens=4, eval_temp=0.5, dataset=svamp
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 2/2 [00:01<00:00,  1.05it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you m
ay observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:13<00:00,  1.36it/s]
Average time taken for each sample: 0.7295404291152954, Average time taken for contemplation: 0.02195350408554077
Evaluation results: {'numerical_accuracy': 0.28, 'close_match_rate': 0.28, 'mean_relative_error': np.float64(3.9111657984012317e+27), 'median_relative_error': np.float64(0.25765306122448983)}
[78/45] Running with max_contemp_tokens=4, eval_temp=0.6, dataset=svamp
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 2/2 [00:01<00:00,  1.02it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you m
ay observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:15<00:00,  1.33it/s]
Average time taken for each sample: 0.7460060048103333, Average time taken for contemplation: 0.020824756622314453
Evaluation results: {'numerical_accuracy': 0.31, 'close_match_rate': 0.32, 'mean_relative_error': np.float64(6.744645268878411e+28), 'median_relative_error': np.float64(0.30292792792792794)}
[79/45] Running with max_contemp_tokens=4, eval_temp=0.7, dataset=svamp
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 2/2 [00:01<00:00,  1.06it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you m
ay observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:13<00:00,  1.35it/s]
Average time taken for each sample: 0.732804000377655, Average time taken for contemplation: 0.02029575824737549
Evaluation results: {'numerical_accuracy': 0.29, 'close_match_rate': 0.29, 'mean_relative_error': np.float64(2.374649342427572e+30), 'median_relative_error': np.float64(0.3277777777777778)}
[80/45] Running with max_contemp_tokens=4, eval_temp=0.8, dataset=svamp
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 2/2 [00:01<00:00,  1.04it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you m
ay observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:13<00:00,  1.35it/s]
Average time taken for each sample: 0.7324228596687317, Average time taken for contemplation: 0.020241551399230957
Evaluation results: {'numerical_accuracy': 0.25, 'close_match_rate': 0.25, 'mean_relative_error': np.float64(2.2758932861311953e+30), 'median_relative_error': np.float64(0.3693181818181818)}
[81/45] Running with max_contemp_tokens=4, eval_temp=0.9, dataset=svamp
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 2/2 [00:01<00:00,  1.07it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you m
ay observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:14<00:00,  1.35it/s]
Average time taken for each sample: 0.73383629322052, Average time taken for contemplation: 0.020485706329345703
Evaluation results: {'numerical_accuracy': 0.26, 'close_match_rate': 0.26, 'mean_relative_error': np.float64(2.2848806582401935e+30), 'median_relative_error': np.float64(0.33835341365461846)}
[82/45] Running with max_contemp_tokens=5, eval_temp=0.1, dataset=svamp
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 2/2 [00:01<00:00,  1.08it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you m
ay observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:22<00:00,  1.21it/s]
Average time taken for each sample: 0.8191100764274597, Average time taken for contemplation: 0.020358514785766602
Evaluation results: {'numerical_accuracy': 0.19, 'close_match_rate': 0.19, 'mean_relative_error': np.float64(1.4144239706626699e+29), 'median_relative_error': np.float64(0.6411007025761124)}
[83/45] Running with max_contemp_tokens=5, eval_temp=0.2, dataset=svamp
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 2/2 [00:01<00:00,  1.07it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you m
ay observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:20<00:00,  1.24it/s]
Average time taken for each sample: 0.7960949921607972, Average time taken for contemplation: 0.02034379482269287
Evaluation results: {'numerical_accuracy': 0.18, 'close_match_rate': 0.18, 'mean_relative_error': np.float64(1.4068202238786587e+29), 'median_relative_error': np.float64(0.7008928571428572)}
[84/45] Running with max_contemp_tokens=5, eval_temp=0.3, dataset=svamp
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 2/2 [00:01<00:00,  1.05it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you m
ay observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:20<00:00,  1.24it/s]
Average time taken for each sample: 0.8019426965713501, Average time taken for contemplation: 0.019463770389556885
Evaluation results: {'numerical_accuracy': 0.19, 'close_match_rate': 0.19, 'mean_relative_error': np.float64(2.4129275831263264e+29), 'median_relative_error': np.float64(0.609375)}
[85/45] Running with max_contemp_tokens=5, eval_temp=0.4, dataset=svamp
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 2/2 [00:01<00:00,  1.05it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you m
ay observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:21<00:00,  1.23it/s]
Average time taken for each sample: 0.8082806420326233, Average time taken for contemplation: 0.02038451433181763
Evaluation results: {'numerical_accuracy': 0.2, 'close_match_rate': 0.2, 'mean_relative_error': np.float64(1.1251145910321175e+31), 'median_relative_error': np.float64(0.682118580060423)}
[86/45] Running with max_contemp_tokens=5, eval_temp=0.5, dataset=svamp
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 2/2 [00:01<00:00,  1.09it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you m
ay observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:21<00:00,  1.22it/s]
Average time taken for each sample: 0.8101697373390198, Average time taken for contemplation: 0.019614248275756835
Evaluation results: {'numerical_accuracy': 0.17, 'close_match_rate': 0.17, 'mean_relative_error': np.float64(1.6496201277449518e+29), 'median_relative_error': np.float64(0.8084239130434783)}
[87/45] Running with max_contemp_tokens=5, eval_temp=0.6, dataset=svamp
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 2/2 [00:01<00:00,  1.07it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you m
ay observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:22<00:00,  1.22it/s]
Average time taken for each sample: 0.8159197497367859, Average time taken for contemplation: 0.020281057357788086
Evaluation results: {'numerical_accuracy': 0.19, 'close_match_rate': 0.19, 'mean_relative_error': np.float64(2.2195548568261716e+31), 'median_relative_error': np.float64(0.9897787259762008)}
[88/45] Running with max_contemp_tokens=5, eval_temp=0.7, dataset=svamp
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 2/2 [00:01<00:00,  1.08it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you m
ay observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:22<00:00,  1.21it/s]
Average time taken for each sample: 0.8171100974082947, Average time taken for contemplation: 0.01965470552444458
Evaluation results: {'numerical_accuracy': 0.2, 'close_match_rate': 0.2, 'mean_relative_error': np.float64(1.3724894266000164e+31), 'median_relative_error': np.float64(0.7100840336134454)}
[89/45] Running with max_contemp_tokens=5, eval_temp=0.8, dataset=svamp
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 2/2 [00:01<00:00,  1.06it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you m
ay observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:22<00:00,  1.22it/s]
Average time taken for each sample: 0.8140828204154968, Average time taken for contemplation: 0.020745999813079834
Evaluation results: {'numerical_accuracy': 0.14, 'close_match_rate': 0.15, 'mean_relative_error': np.float64(8.937209164437085e+31), 'median_relative_error': np.float64(0.8352796052631579)}
[90/45] Running with max_contemp_tokens=5, eval_temp=0.9, dataset=svamp
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 2/2 [00:01<00:00,  1.08it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you m
ay observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:22<00:00,  1.21it/s]
Average time taken for each sample: 0.8176174187660217, Average time taken for contemplation: 0.020362460613250734
Evaluation results: {'numerical_accuracy': 0.17, 'close_match_rate': 0.17, 'mean_relative_error': np.float64(9.044472456776464e+31), 'median_relative_error': np.float64(0.6433157099697886)}
All evaluation runs completed!
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ tmux capture-pane -p -S - > /home/nee7ne/EfficientCoT/tmux_output_2.txt
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ python main.py --mode evaluate --max_contemp_tokens 1 --eval_temp 0.8
Traceback (most recent call last):
  File "/home/nee7ne/EfficientCoT/main.py", line 210, in <module>
    main()
  File "/home/nee7ne/EfficientCoT/main.py", line 178, in main
    contemp_generator = ContemplationGenerator.from_pretrained(
  File "/home/nee7ne/EfficientCoT/models/contemp_generator.py", line 78, in from_pretrained
    config_dict = torch.load(f"{path}/config.pt")
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/serialization.py", line 1425, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/serialization.py", line 751, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/serialization.py", line 732, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/data/nee7ne/effi_cot/saved_models/effi_cot/vanilla/small/gsm8k/contemp_generator/config.pt'
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ cd
(effi_cot) nee7ne@uvavast:~$ cd /data/nee7ne/
(effi_cot) nee7ne@uvavast:/data/nee7ne$ ls
effi_cot  huggingface  ModCirc
(effi_cot) nee7ne@uvavast:/data/nee7ne$ cd effi_cot
(effi_cot) nee7ne@uvavast:/data/nee7ne/effi_cot$ ls
no_l_reason  no_sentence_transformer  saved_models  saved_models2
(effi_cot) nee7ne@uvavast:/data/nee7ne/effi_cot$ cd saved_models
(effi_cot) nee7ne@uvavast:/data/nee7ne/effi_cot/saved_models$ ls
ccot  effi_cot
(effi_cot) nee7ne@uvavast:/data/nee7ne/effi_cot/saved_models$ cd effi_cot
(effi_cot) nee7ne@uvavast:/data/nee7ne/effi_cot/saved_models/effi_cot$ ls
no_l_reason  no_sentence_transformer  old_vanilla  vanilla
(effi_cot) nee7ne@uvavast:/data/nee7ne/effi_cot/saved_models/effi_cot$ cd vanilla
(effi_cot) nee7ne@uvavast:/data/nee7ne/effi_cot/saved_models/effi_cot/vanilla$ ls
gsm8k  mistral  multiarith  small  svamp
(effi_cot) nee7ne@uvavast:/data/nee7ne/effi_cot/saved_models/effi_cot/vanilla$ cd small
(effi_cot) nee7ne@uvavast:/data/nee7ne/effi_cot/saved_models/effi_cot/vanilla/small$ ls
gsm8k  multiarith  svamp
(effi_cot) nee7ne@uvavast:/data/nee7ne/effi_cot/saved_models/effi_cot/vanilla/small$ cd
(effi_cot) nee7ne@uvavast:~$ cd EfficientCoT
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ python main.py --mode evaluate --max_contemp_tokens 1 --eval_temp 0.8 --dataset SVAMP
usage: main.py [-h] [--mode {train_sentence_transformer,train_contemp_generator,evaluate,baseline,run_experiments,train_ccot}] [--config CONFIG]
               [--dataset {gsm8k,svamp,multiarith}] [--baseline {cot,ccot,pause,implicit_cot,zero_shot_cot,effi_cot}]
               [--ccot_stage {encode,decode,prepare_decode_data,evaluate,cotrain_encode_decode}] [--experiment_file EXPERIMENT_FILE] [--device DEVICE] [--seed SEED]
               [--variation {vanilla,no_sentence_transformer,no_l_reason}] [--compression_ratio COMPRESSION_RATIO] [--max_contemp_tokens MAX_CONTEMP_TOKENS]
               [--autoregressive_layer AUTOREGRESSIVE_LAYER] [--cot_bsl_shot COT_BSL_SHOT] [--eval_temp EVAL_TEMP]
main.py: error: argument --dataset: invalid choice: 'SVAMP' (choose from 'gsm8k', 'svamp', 'multiarith')
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ python main.py --mode evaluate --max_contemp_tokens 1 --eval_temp 0.8 --dataset svamp
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.78s/it]
Traceback (most recent call last):
  File "/home/nee7ne/EfficientCoT/main.py", line 210, in <module>
    main()
  File "/home/nee7ne/EfficientCoT/main.py", line 181, in main
    results = run_inference(
  File "/home/nee7ne/EfficientCoT/inference/inference.py", line 30, in run_inference
    teacher_model = teacher_model.to(device)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3162, in to
    return super().to(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1343, in to
    return self._apply(convert)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 903, in _apply
    module._apply(fn)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 903, in _apply
    module._apply(fn)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 903, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    param_applied = fn(param)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1329, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 13.00 MiB is free. Process 1820723 has 796.00 MiB memory in use. Process 1451878 has 49.10 G
iB memory in use. Process 1454581 has 448.00 MiB memory in use. Including non-PyTorch memory, this process has 28.80 GiB memory in use. Of the allocated memory 28.39 GiB is allocated by PyTorch, and 679.50 KiB is re
served by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://
pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ python main.py --mode evaluate --max_contemp_tokens 1 --eval_temp 0.8 --dataset svamp
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.05it/s]
Traceback (most recent call last):
  File "/home/nee7ne/EfficientCoT/main.py", line 210, in <module>
    main()
  File "/home/nee7ne/EfficientCoT/main.py", line 181, in main
    results = run_inference(
  File "/home/nee7ne/EfficientCoT/inference/inference.py", line 30, in run_inference
    teacher_model = teacher_model.to(device)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3162, in to
    return super().to(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1343, in to
    return self._apply(convert)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 903, in _apply
    module._apply(fn)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 903, in _apply
    module._apply(fn)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 903, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    param_applied = fn(param)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1329, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 13.00 MiB is free. Process 1820723 has 796.00 MiB memory in use. Process 1451878 has 49.10 G
iB memory in use. Process 1454581 has 448.00 MiB memory in use. Including non-PyTorch memory, this process has 28.80 GiB memory in use. Of the allocated memory 28.39 GiB is allocated by PyTorch, and 679.50 KiB is re
served by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://
pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ nvidia-smi
Mon Apr  7 15:55:21 2025
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100 80GB PCIe          Off | 00000000:01:00.0 Off |                    0 |
| N/A   63C    P0             235W / 300W |  51542MiB / 81920MiB |     81%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A100 80GB PCIe          Off | 00000000:41:00.0 Off |                    0 |
| N/A   32C    P0              54W / 300W |      3MiB / 81920MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA A100 80GB PCIe          Off | 00000000:81:00.0 Off |                    0 |
| N/A   68C    P0             255W / 300W |  10210MiB / 81920MiB |    100%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA A100 80GB PCIe          Off | 00000000:C1:00.0 Off |                    0 |
| N/A   64C    P0             253W / 300W |  61960MiB / 81920MiB |     90%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A   1451878      C   python                                    50278MiB |
|    0   N/A  N/A   1454581      C   python                                      448MiB |
|    0   N/A  N/A   1820723      C   python                                      796MiB |
|    2   N/A  N/A   1454581      C   python                                    10202MiB |
|    3   N/A  N/A    586997      C   python                                    15974MiB |
|    3   N/A  N/A   1454097      C   python                                    45972MiB |
+---------------------------------------------------------------------------------------+
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ export CUDA_VISIBLE_DEVICES=1,2
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ python main.py --mode evaluate --max_contemp_tokens 1 --eval_temp 0.8 --dataset svamp
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.04it/s]
Running inference:   0%|                                                                                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and canno
t be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:05<00:00,  1.52it/s]
Average time taken for each sample: 0.6498723053932189, Average time taken for contemplation: 0.02088909864425659
Evaluation results: {'numerical_accuracy': 0.42, 'close_match_rate': 0.45, 'mean_relative_error': np.float64(0.4267466576506837), 'median_relative_error': np.float64(0.07366013071895425)}
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ python main.py --mode evaluate --max_contemp_tokens 1 --eval_temp 0.8 --dataset svamp --config mistral
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and canno
t be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:24<00:00,  1.18it/s]
Average time taken for each sample: 0.8434113359451294, Average time taken for contemplation: 0.020750350952148437
Evaluation results: {'numerical_accuracy': 0.24, 'close_match_rate': 0.25, 'mean_relative_error': np.float64(4.218712260550807e+24), 'median_relative_error': np.float64(0.18181818181818182)}
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ for dataset in svamp gsm8k multiarith; do python main.py --mode evaluate --config mistral --dataset $dataset; done
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
^CTraceback (most recent call last):
  File "/home/nee7ne/EfficientCoT/main.py", line 210, in <module>
    main()
  File "/home/nee7ne/EfficientCoT/main.py", line 178, in main
    contemp_generator = ContemplationGenerator.from_pretrained(
  File "/home/nee7ne/EfficientCoT/models/contemp_generator.py", line 88, in from_pretrained
    model.load_state_dict(torch.load(f"{path}/model.pt"))
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/serialization.py", line 1462, in load
    return _load(
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/serialization.py", line 1964, in _load
    result = unpickler.load()
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/_weights_only_unpickler.py", line 512, in load
    self.append(self.persistent_load(pid))
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/serialization.py", line 1928, in persistent_load
    typed_storage = load_tensor(
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/serialization.py", line 1888, in load_tensor
    zip_file.get_storage_from_record(name, numel, torch.UntypedStorage)
KeyboardInterrupt

(effi_cot) nee7ne@uvavast:~/EfficientCoT$ for dataset in svamp gsm8k multiarith; do python main.py --mode evaluate --config mistral --dataset $dataset; done
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.21s/it]
Running inference:   0%|                                                                                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and canno
t be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference:   1%|█▏                                                                                                                      | 1/100 [00:13<23:00, 13.95s/it]Running inference:   1%|█▏
                                                                                                         | 1/100 [00:27<44:39, 27.07s/it]
Traceback (most recent call last):
  File "/home/nee7ne/EfficientCoT/main.py", line 210, in <module>
    main()
  File "/home/nee7ne/EfficientCoT/main.py", line 181, in main
    results = run_inference(
  File "/home/nee7ne/EfficientCoT/inference/inference.py", line 152, in run_inference
    outputs = teacher_model.generate(
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/generation/utils.py", line 2223, in generate
    result = self._sample(
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/generation/utils.py", line 3214, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 843, in forward
    outputs = self.model(
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 566, in forward
    layer_outputs = decoder_layer(
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 247, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 162, in forward
    query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt

(effi_cot) nee7ne@uvavast:~/EfficientCoT$ clear
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ for dataset in svamp gsm8k multiarith; do python main.py --mode evaluate --config mistral --dataset $dataset; done
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and canno
t be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:15<00:00,  1.32it/s]
Average time taken for each sample: 0.7540430021286011, Average time taken for contemplation: 0.021470651626586915
Evaluation results: {'numerical_accuracy': 0.43, 'close_match_rate': 0.46, 'mean_relative_error': np.float64(0.5652987710987263), 'median_relative_error': np.float64(0.060990712074303406)}
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and canno
t be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:21<00:00,  1.22it/s]
Average time taken for each sample: 0.8106666207313538, Average time taken for contemplation: 0.020750818252563478
Evaluation results: {'numerical_accuracy': 0.07, 'close_match_rate': 0.07, 'mean_relative_error': np.float64(0.7969226701140045), 'median_relative_error': np.float64(0.463884430176565)}
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used
 so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https
://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                                                                                | 0/100 [00:00<?, ?it/s]The attention mask is not set and canno
t be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:19<00:00,  1.26it/s]
Average time taken for each sample: 0.787376720905304, Average time taken for contemplation: 0.020692949295043946
Evaluation results: {'numerical_accuracy': 0.16, 'close_match_rate': 0.16, 'mean_relative_error': np.float64(0.3656063809889079), 'median_relative_error': np.float64(0.28348214285714285)}
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ python main.py --mode evaluate --max_contemp_tokens 1 --eval_temp 0.8 --dataset svamp --config mistral
^CTraceback (most recent call last):
  File "/home/nee7ne/EfficientCoT/main.py", line 210, in <module>
    main()
  File "/home/nee7ne/EfficientCoT/main.py", line 178, in main
    contemp_generator = ContemplationGenerator.from_pretrained(
  File "/home/nee7ne/EfficientCoT/models/contemp_generator.py", line 81, in from_pretrained
    model = cls(
  File "/home/nee7ne/EfficientCoT/models/contemp_generator.py", line 13, in __init__
    self.tokenizer = AutoTokenizer.from_pretrained(student_model_name)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 912, in from_pretrained
    tokenizer_class_from_name(config_tokenizer_class) is not None
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 609, in tokenizer_class_from_name
    module = importlib.import_module(f".{module_name}", "transformers.models")
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1024, in _find_and_load
  File "<frozen importlib._bootstrap>", line 173, in __exit__
KeyboardInterrupt

(effi_cot) nee7ne@uvavast:~/EfficientCoT$ clear
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ python main.py --mode train_sentence_transformer --config mistral
^CTraceback (most recent call last):
  File "/home/nee7ne/EfficientCoT/main.py", line 13, in <module>
    from baselines.baselines import run_baseline
  File "/home/nee7ne/EfficientCoT/baselines/baselines.py", line 4, in <module>
    from transformers import pipeline
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1851, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1863, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/pipelines/__init__.py", line 49, in <module>
    from .audio_classification import AudioClassificationPipeline
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/pipelines/audio_classification.py", line 21, in <module>
    from .base import Pipeline, build_pipeline_init_args
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/pipelines/base.py", line 68, in <module>
    from ..modeling_utils import PreTrainedModel
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/modeling_utils.py", line 53, in <module>
    from .loss.loss_utils import LOSS_MAPPING
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/loss/loss_utils.py", line 21, in <module>
    from .loss_rt_detr import RTDetrForObjectDetectionLoss
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/loss/loss_rt_detr.py", line 46, in <module>
    class RTDetrHungarianMatcher(nn.Module):
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/loss/loss_rt_detr.py", line 73, in RTDetrHungarianMatcher
    def forward(self, outputs, targets):
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 138, in __call__
    return cast(F, context_decorator(self.clone, func))
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 70, in context_decorator
    def context_decorator(ctx, func):
KeyboardInterrupt

(effi_cot) nee7ne@uvavast:~/EfficientCoT$ clear
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ git add .
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ git commit -m "checkpoint for apr 7th meeting"
[main d5fa1d2] checkpoint for apr 7th meeting
 12 files changed, 2588 insertions(+), 189 deletions(-)
 create mode 100755 run_efficot_on_datasets.sh
 create mode 100755 run_para_search.sh
 delete mode 100755 scripts/run_efficot_on_datasets.sh
 create mode 100644 tmux_output.txt
 create mode 100644 tmux_output_2.txt
 delete mode 100644 utils/hidden_states.py
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ git puh
git: 'puh' is not a git command. See 'git --help'.

The most similar command is
        push
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ git push
Enumerating objects: 31, done.
Counting objects: 100% (31/31), done.
Delta compression using up to 48 threads
Compressing objects: 100% (17/17), done.
Writing objects: 100% (18/18), 18.50 KiB | 4.62 MiB/s, done.
Total 18 (delta 12), reused 0 (delta 0), pack-reused 0
remote: Resolving deltas: 100% (12/12), completed with 10 local objects.
remote:
remote: GitHub found 1 vulnerability on YinhanHe123/EfficientCoT's default branch (1 moderate). To find out more, visit:
remote:      https://github.com/YinhanHe123/EfficientCoT/security/dependabot/1
remote:
To https://github.com/YinhanHe123/EfficientCoT.git
   3ef79a8..d5fa1d2  main -> main
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ clear
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ python main.py --mode train_sentence_transformer --config mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Traceback (most recent call last):
  File "/home/nee7ne/EfficientCoT/main.py", line 210, in <module>
    main()
  File "/home/nee7ne/EfficientCoT/main.py", line 133, in main
    sentence_transformer = train_sentence_transformer(
  File "/home/nee7ne/EfficientCoT/training/train_sent_trans.py", line 60, in train_sentence_transformer
    base_model = base_model.to(device)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3162, in to
    return super().to(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1343, in to
    return self._apply(convert)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 903, in _apply
    module._apply(fn)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 903, in _apply
    module._apply(fn)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 903, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 930, in _apply
    param_applied = fn(param)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1329, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 4.12 MiB is free. Process 2602980 has 54.61 GiB memory in use. Including non-PyTorch memory,
this process has 24.52 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 1.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTOR
CH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ nvidia-smi
Tue Apr  8 16:25:22 2025
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100 80GB PCIe          Off | 00000000:01:00.0 Off |                    0 |
| N/A   35C    P0              73W / 300W |    804MiB / 81920MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A100 80GB PCIe          Off | 00000000:41:00.0 Off |                    0 |
| N/A   72C    P0             301W / 300W |  55932MiB / 81920MiB |     97%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA A100 80GB PCIe          Off | 00000000:81:00.0 Off |                    0 |
| N/A   32C    P0              54W / 300W |      3MiB / 81920MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA A100 80GB PCIe          Off | 00000000:C1:00.0 Off |                    0 |
| N/A   37C    P0              77W / 300W |  15982MiB / 81920MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A   1820723      C   python                                      796MiB |
|    1   N/A  N/A   2602980      C   python                                    55924MiB |
|    3   N/A  N/A    586997      C   python                                    15974MiB |
+---------------------------------------------------------------------------------------+
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ export CUDA_VISIBLE_DEVICES=0,2
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ export CUDA_VISIBLE_DEVICES=0,2,3
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ python main.py --mode train_sentence_transformer --config mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
2025-04-08 16:26:05,708 [INFO] Logging to ./logs/sentence_transformer_16_to_20
2025-04-08 16:26:05,708 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/data/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/gsm8k', 'checkpoint_path': './checkpoin
ts/effi_cot/vanilla/mistral/gsm8k', 'result_path': './results/effi_cot/vanilla/mistral/gsm8k', 'experiment_name': 'effi_cot_vanilla_42_gsm8k_mistral', 'learning_rate': 1e-05, 'weight_decay': 0.01, 'num_epochs': 12,
'train_sen_trans_epochs': 15, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 150, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 400, 'max_contemp_tokens': 5, 'start_layer_idx': 16,
'end_layer_idx': 20, 'reasoning_pairs_path': '/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'eval_temp': 0.3}
Epoch 1/15 - Training:   1%|█▍                                                                                                                   | 1/80 [00:01<02:06,  1.60s/it]Epoch 1/15 - Training:   1%|█▍
                                                                                                          | 1/80 [00:02<03:41,  2.80s/it]
Traceback (most recent call last):
  File "/home/nee7ne/EfficientCoT/main.py", line 210, in <module>
    main()
  File "/home/nee7ne/EfficientCoT/main.py", line 133, in main
    sentence_transformer = train_sentence_transformer(
  File "/home/nee7ne/EfficientCoT/training/train_sent_trans.py", line 194, in train_sentence_transformer
    train_loss += loss.item()
KeyboardInterrupt

(effi_cot) nee7ne@uvavast:~/EfficientCoT$ cd /data/nee7ne/
(effi_cot) nee7ne@uvavast:/data/nee7ne$ cd huggingface
(effi_cot) nee7ne@uvavast:/data/nee7ne/huggingface$ ls
datasets  hub  modules  stored_tokens  token
(effi_cot) nee7ne@uvavast:/data/nee7ne/huggingface$ cd hub
(effi_cot) nee7ne@uvavast:/data/nee7ne/huggingface/hub$ ls
datasets--ChilleD--MultiArith  datasets--gsm8k                models--meta-llama--Llama-2-13b-hf      models--mistralai--Mistral-7B-Instruct-v0.2
datasets--ChilleD--SVAMP       datasets--microsoft--ms_marco  models--meta-llama--Llama-2-7b-chat     models--princeton-nlp--Sheared-LLaMA-1.3B
datasets--CogComp--trec        datasets--openai--gsm8k        models--meta-llama--Llama-2-7b-chat-hf  models--ProbeMedicalYonseiMAILab--medllama3-v20
datasets--eriktks--conll2003   models--gpt2                   models--meta-llama--Llama-2-7b-hf       version.txt
(effi_cot) nee7ne@uvavast:/data/nee7ne/huggingface/hub$ python main.py --mode train_contemp_generator --config mistral
python: can't open file '/data/nee7ne/huggingface/hub/main.py': [Errno 2] No such file or directory
(effi_cot) nee7ne@uvavast:/data/nee7ne/huggingface/hub$ c
c: command not found
(effi_cot) nee7ne@uvavast:/data/nee7ne/huggingface/hub$ cd
(effi_cot) nee7ne@uvavast:~$ cd EfficientCoT
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ python main.py --mode train_contemp_generator --config mistral
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 600/600 [00:00<00:00, 8.99MB/s]
model.safetensors:   8%|█████████▍                                                                                     model.safetensors:   8%|█████████▋
                       model.safetensors:   9%|█████████▉                                                                                     model.safetensors:   9%|██████████▏
                                              model.safetensors:   9%|██████████▌                                                                                    model.safetensors:   9%|██████████▊
                                                                     model.safetensors:  10%|███████████                                                                                    model.safetensors:  10%|███
████████▎                                                                                   model.safetensors:  10%|███████████▌                                                                                   mode
l.safetensors:  10%|███████████▉                                                                                   model.safetensors:  10%|████████████▏
                   model.safetensors:  11%|████████████▍                                                                                  model.safetensors:  11%|████████████▋
                                          model.safetensors:  11%|████████████▉                                                                                  model.safetensors:  11%|█████████████▎
                                                        model.safetensors: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.40G/4.40G [01:16
<00:00, 57.3MB/s]
tokenizer_config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 776/776 [00:00<00:00, 8.
45MB/s]
tokenizer.model: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500k/500k [00:00<00:00, 66
.1MB/s]
tokenizer.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.84M/1.84M [00:00<00:00, 43
.7MB/s]
special_tokens_map.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 414/414 [00:00<00:00, 2.
91MB/s]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.
49it/s]
2025-04-08 16:31:18,767 [INFO] Logging to ./logs/contemp_generator
2025-04-08 16:31:18,768 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/data/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/gsm8k', 'checkpoint_path': './checkpoin
ts/effi_cot/vanilla/mistral/gsm8k', 'result_path': './results/effi_cot/vanilla/mistral/gsm8k', 'experiment_name': 'effi_cot_vanilla_42_gsm8k_mistral', 'learning_rate': 1e-05, 'weight_decay': 0.01, 'num_epochs': 12,
'train_sen_trans_epochs': 15, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 150, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 400, 'max_contemp_tokens': 5, 'start_layer_idx': 16,
'end_layer_idx': 20, 'reasoning_pairs_path': '/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'eval_temp': 0.3, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_mode
l_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'openai/gsm8k', 'teacher_hidden_dim
': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
Starting training contemplation generator...
Epoch 1/12:  21%|████████████████████████████████████▌                                                                                                                                       | 85/400 [00:37<02:15,  2.
33it/s]Epoch 1/12:  21%|████████████████████████████████████▌                                                                                                                                       | 85/400 [00:38<02:
21,  2.23it/s]
Traceback (most recent call last):
  File "/home/nee7ne/EfficientCoT/main.py", line 210, in <module>
    main()
  File "/home/nee7ne/EfficientCoT/main.py", line 166, in main
    train_contemplation_generator(
  File "/home/nee7ne/EfficientCoT/training/train_contemp_gen.py", line 184, in train_contemplation_generator
    loss, total_loss, reason_loss, ans_loss = process_item(
  File "/home/nee7ne/EfficientCoT/training/train_contemp_gen.py", line 346, in process_item
    l_ans = compute_loss_ans(contemp_states, teacher_model, teacher_tokenizer,
  File "/home/nee7ne/EfficientCoT/training/train_contemp_gen.py", line 51, in compute_loss_ans
    teacher_outputs = teacher_model(
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 843, in forward
    outputs = self.model(
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 566, in forward
    layer_outputs = decoder_layer(
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 247, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 167, in forward
    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 90, in apply_rotary_pos_emb
    q_embed = (q * cos) + (rotate_half(q) * sin)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 61, in rotate_half
    def rotate_half(x):
KeyboardInterrupt

(effi_cot) nee7ne@uvavast:~/EfficientCoT$ clear

(effi_cot) nee7ne@uvavast:~/EfficientCoT$ python main.py --mode train_contemp_generator --config mistral
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.
51it/s]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.
50it/s]
2025-04-08 16:39:38,005 [INFO] Logging to ./logs/contemp_generator
2025-04-08 16:39:38,005 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/data/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/gsm8k', 'checkpoint_path': './checkpoin
ts/effi_cot/vanilla/mistral/gsm8k', 'result_path': './results/effi_cot/vanilla/mistral/gsm8k', 'experiment_name': 'effi_cot_vanilla_42_gsm8k_mistral', 'learning_rate': 1e-05, 'weight_decay': 0.01, 'num_epochs': 12,
'train_sen_trans_epochs': 15, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 150, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 400, 'max_contemp_tokens': 5, 'start_layer_idx': 16,
'end_layer_idx': 20, 'reasoning_pairs_path': '/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'eval_temp': 0.3, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_mode
l_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'openai/gsm8k', 'teacher_hidden_dim
': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
Starting training contemplation generator...
Epoch 1/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:56<00:00,  2.
26it/s]
2025-04-08 16:42:34,993 [INFO] Step 0 - total_loss: 1.7892, reason_loss: 0.9840, ans_loss: 2.0576
Epoch 1 - Loss: 1.7892 (Reason: 0.9840, Ans: 2.0576)
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.
16it/s]
Evaluation - Total Loss: 1.6518, Reason Loss: 0.9664, Answer Loss: 1.8802
2025-04-08 16:43:06,603 [INFO] Step 0 - eval_loss: 1.6518
Validation Loss: 1.6518
Epoch 2/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:58<00:00,  2.
24it/s]
2025-04-08 16:46:05,401 [INFO] Step 1 - total_loss: 1.5833, reason_loss: 0.9587, ans_loss: 1.7915
Epoch 2 - Loss: 1.5833 (Reason: 0.9587, Ans: 1.7915)
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.
15it/s]
Evaluation - Total Loss: 1.6162, Reason Loss: 0.9412, Answer Loss: 1.8412
2025-04-08 16:46:37,118 [INFO] Step 1 - eval_loss: 1.6162
Validation Loss: 1.6162
Epoch 3/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [05:37<00:00,  1.
18it/s]
2025-04-08 16:52:14,989 [INFO] Step 2 - total_loss: 1.5359, reason_loss: 0.9399, ans_loss: 1.7345
Epoch 3 - Loss: 1.5359 (Reason: 0.9399, Ans: 1.7345)
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:03<00:00,  1.
58it/s]
Evaluation - Total Loss: 1.5512, Reason Loss: 0.9284, Answer Loss: 1.7588
2025-04-08 16:53:18,095 [INFO] Step 2 - eval_loss: 1.5512
Validation Loss: 1.5512
Epoch 4/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [05:20<00:00,  1.
25it/s]
2025-04-08 16:58:38,414 [INFO] Step 3 - total_loss: 1.4715, reason_loss: 0.9307, ans_loss: 1.6518
Epoch 4 - Loss: 1.4715 (Reason: 0.9307, Ans: 1.6518)
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:04<00:00,  1.
55it/s]
Evaluation - Total Loss: 1.5171, Reason Loss: 0.9202, Answer Loss: 1.7161
2025-04-08 16:59:42,979 [INFO] Step 3 - eval_loss: 1.5171
Validation Loss: 1.5171
Epoch 5/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [05:17<00:00,  1.
26it/s]
2025-04-08 17:05:00,124 [INFO] Step 4 - total_loss: 1.4093, reason_loss: 0.9240, ans_loss: 1.5711
Epoch 5 - Loss: 1.4093 (Reason: 0.9240, Ans: 1.5711)
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:04<00:00,  1.
55it/s]
Evaluation - Total Loss: 1.5310, Reason Loss: 0.9139, Answer Loss: 1.7367
2025-04-08 17:06:04,734 [INFO] Step 4 - eval_loss: 1.5310
Validation Loss: 1.5310
Epoch 6/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [05:19<00:00,  1.
25it/s]
2025-04-08 17:11:24,239 [INFO] Step 5 - total_loss: 1.3669, reason_loss: 0.9196, ans_loss: 1.5160
Epoch 6 - Loss: 1.3669 (Reason: 0.9196, Ans: 1.5160)
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:04<00:00,  1.
55it/s]
Evaluation - Total Loss: 1.5279, Reason Loss: 0.9123, Answer Loss: 1.7330
2025-04-08 17:12:28,904 [INFO] Step 5 - eval_loss: 1.5279
Validation Loss: 1.5279
Epoch 7/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [05:27<00:00,  1.
22it/s]
2025-04-08 17:17:55,909 [INFO] Step 6 - total_loss: 1.3044, reason_loss: 0.9182, ans_loss: 1.4331
Epoch 7 - Loss: 1.3044 (Reason: 0.9182, Ans: 1.4331)
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:55<00:00,  1.
80it/s]
Evaluation - Total Loss: 1.5128, Reason Loss: 0.9107, Answer Loss: 1.7136
2025-04-08 17:18:51,570 [INFO] Step 6 - eval_loss: 1.5128
Validation Loss: 1.5128
Epoch 8/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [05:40<00:00,  1.
18it/s]
2025-04-08 17:24:31,865 [INFO] Step 7 - total_loss: 1.2826, reason_loss: 0.9186, ans_loss: 1.4039
Epoch 8 - Loss: 1.2826 (Reason: 0.9186, Ans: 1.4039)
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:43<00:00,  2.
30it/s]
Evaluation - Total Loss: 1.5236, Reason Loss: 0.9117, Answer Loss: 1.7275
2025-04-08 17:25:15,310 [INFO] Step 7 - eval_loss: 1.5236
Validation Loss: 1.5236
Epoch 9/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [05:41<00:00,  1.
17it/s]
2025-04-08 17:30:56,796 [INFO] Step 8 - total_loss: 1.2502, reason_loss: 0.9199, ans_loss: 1.3603
Epoch 9 - Loss: 1.2502 (Reason: 0.9199, Ans: 1.3603)
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:02<00:00,  1.
60it/s]
Evaluation - Total Loss: 1.4827, Reason Loss: 0.9124, Answer Loss: 1.6728
2025-04-08 17:31:59,170 [INFO] Step 8 - eval_loss: 1.4827
Validation Loss: 1.4827
Epoch 10/12: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [05:21<00:00,  1.
24it/s]
2025-04-08 17:37:20,470 [INFO] Step 9 - total_loss: 1.2152, reason_loss: 0.9205, ans_loss: 1.3135
Epoch 10 - Loss: 1.2152 (Reason: 0.9205, Ans: 1.3135)
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:04<00:00,  1.
55it/s]
Evaluation - Total Loss: 1.5303, Reason Loss: 0.9151, Answer Loss: 1.7353
2025-04-08 17:38:25,149 [INFO] Step 9 - eval_loss: 1.5303
Validation Loss: 1.5303
Epoch 11/12: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [05:20<00:00,  1.
25it/s]
2025-04-08 17:43:45,259 [INFO] Step 10 - total_loss: 1.1528, reason_loss: 0.9238, ans_loss: 1.2291
Epoch 11 - Loss: 1.1528 (Reason: 0.9238, Ans: 1.2291)
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:04<00:00,  1.
55it/s]
Evaluation - Total Loss: 1.5449, Reason Loss: 0.9166, Answer Loss: 1.7543
2025-04-08 17:44:49,950 [INFO] Step 10 - eval_loss: 1.5449
Validation Loss: 1.5449
Epoch 12/12:   0%|                                                                                                                                                                                    | 0/400 [00:00<?,
 ?it/s]
Traceback (most recent call last):
  File "/home/nee7ne/EfficientCoT/main.py", line 210, in <module>
    main()
  File "/home/nee7ne/EfficientCoT/main.py", line 166, in main
    train_contemplation_generator(
  File "/home/nee7ne/EfficientCoT/training/train_contemp_gen.py", line 202, in train_contemplation_generator
    optimizer.step()
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/optim/adamw.py", line 243, in step
    adamw(
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/optim/adamw.py", line 875, in adamw
    func(
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/optim/adamw.py", line 699, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 30.56 MiB is free. Process 1820723 has 796.00 MiB memory in use. Including non-PyTorch memory
, this process has 47.53 GiB memory in use. Process 2640894 has 30.79 GiB memory in use. Of the allocated memory 47.00 GiB is allocated by PyTorch, and 42.48 MiB is reserved by PyTorch but unallocated. If reserved b
ut unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environm
ent-variables)
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ python main.py --mode train_contemp_generator --config mistral
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.
51it/s]
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.
51it/s]
2025-04-08 20:30:26,821 [INFO] Logging to ./logs/contemp_generator
2025-04-08 20:30:26,821 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/data/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/gsm8k', 'checkpoint_path': './checkpoin
ts/effi_cot/vanilla/mistral/gsm8k', 'result_path': './results/effi_cot/vanilla/mistral/gsm8k', 'experiment_name': 'effi_cot_vanilla_42_gsm8k_mistral', 'learning_rate': 1e-05, 'weight_decay': 0.01, 'num_epochs': 12,
'train_sen_trans_epochs': 15, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 150, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 400, 'max_contemp_tokens': 5, 'start_layer_idx': 16,
'end_layer_idx': 20, 'reasoning_pairs_path': '/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'eval_temp': 0.3, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_mode
l_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'openai/gsm8k', 'teacher_hidden_dim
': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
Starting training contemplation generator...
Epoch 1/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:57<00:00,  2.
26it/s]
2025-04-08 20:33:23,938 [INFO] Step 0 - total_loss: 1.7893, reason_loss: 0.9840, ans_loss: 2.0577
Epoch 1 - Loss: 1.7893 (Reason: 0.9840, Ans: 2.0577)
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.
17it/s]
Evaluation - Total Loss: 1.6519, Reason Loss: 0.9664, Answer Loss: 1.8804
2025-04-08 20:33:55,462 [INFO] Step 0 - eval_loss: 1.6519
Validation Loss: 1.6519
Epoch 2/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:58<00:00,  2.
24it/s]
2025-04-08 20:36:53,738 [INFO] Step 1 - total_loss: 1.5827, reason_loss: 0.9587, ans_loss: 1.7907
Epoch 2 - Loss: 1.5827 (Reason: 0.9587, Ans: 1.7907)
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.
16it/s]
Evaluation - Total Loss: 1.6161, Reason Loss: 0.9412, Answer Loss: 1.8410
2025-04-08 20:37:25,339 [INFO] Step 1 - eval_loss: 1.6161
Validation Loss: 1.6161
Epoch 3/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:58<00:00,  2.
24it/s]
2025-04-08 20:40:23,849 [INFO] Step 2 - total_loss: 1.5239, reason_loss: 0.9391, ans_loss: 1.7189
Epoch 3 - Loss: 1.5239 (Reason: 0.9391, Ans: 1.7189)
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.
16it/s]
Evaluation - Total Loss: 1.5435, Reason Loss: 0.9269, Answer Loss: 1.7491
2025-04-08 20:40:55,527 [INFO] Step 2 - eval_loss: 1.5435
Validation Loss: 1.5435
Epoch 4/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:58<00:00,  2.
24it/s]
2025-04-08 20:43:54,307 [INFO] Step 3 - total_loss: 1.4857, reason_loss: 0.9291, ans_loss: 1.6713
Epoch 4 - Loss: 1.4857 (Reason: 0.9291, Ans: 1.6713)
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.
16it/s]
Evaluation - Total Loss: 1.5533, Reason Loss: 0.9184, Answer Loss: 1.7649
2025-04-08 20:44:25,950 [INFO] Step 3 - eval_loss: 1.5533
Validation Loss: 1.5533
Epoch 5/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:58<00:00,  2.
24it/s]
2025-04-08 20:47:24,729 [INFO] Step 4 - total_loss: 1.4093, reason_loss: 0.9224, ans_loss: 1.5716
Epoch 5 - Loss: 1.4093 (Reason: 0.9224, Ans: 1.5716)
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.
16it/s]
Evaluation - Total Loss: 1.5333, Reason Loss: 0.9130, Answer Loss: 1.7401
2025-04-08 20:47:56,389 [INFO] Step 4 - eval_loss: 1.5333
Validation Loss: 1.5333
Epoch 6/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:58<00:00,  2.
24it/s]
2025-04-08 20:50:55,176 [INFO] Step 5 - total_loss: 1.3715, reason_loss: 0.9189, ans_loss: 1.5223
Epoch 6 - Loss: 1.3715 (Reason: 0.9189, Ans: 1.5223)
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.
16it/s]
Evaluation - Total Loss: 1.5470, Reason Loss: 0.9102, Answer Loss: 1.7593
2025-04-08 20:51:26,837 [INFO] Step 5 - eval_loss: 1.5470
Validation Loss: 1.5470
Epoch 7/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:58<00:00,  2.
24it/s]
2025-04-08 20:54:25,672 [INFO] Step 6 - total_loss: 1.3208, reason_loss: 0.9170, ans_loss: 1.4554
Epoch 7 - Loss: 1.3208 (Reason: 0.9170, Ans: 1.4554)
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:52<00:00,  1.
91it/s]
Evaluation - Total Loss: 1.5815, Reason Loss: 0.9071, Answer Loss: 1.8063
2025-04-08 20:55:18,076 [INFO] Step 6 - eval_loss: 1.5815
Validation Loss: 1.5815
Epoch 8/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [05:39<00:00,  1.
18it/s]
2025-04-08 21:00:57,798 [INFO] Step 7 - total_loss: 1.2893, reason_loss: 0.9173, ans_loss: 1.4133
Epoch 8 - Loss: 1.2893 (Reason: 0.9173, Ans: 1.4133)
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:43<00:00,  2.
30it/s]
Evaluation - Total Loss: 1.6193, Reason Loss: 0.9079, Answer Loss: 1.8565
2025-04-08 21:01:41,317 [INFO] Step 7 - eval_loss: 1.6193
Validation Loss: 1.6193
Epoch 9/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [05:40<00:00,  1.
18it/s]
2025-04-08 21:07:21,325 [INFO] Step 8 - total_loss: 1.2359, reason_loss: 0.9175, ans_loss: 1.3421
Epoch 9 - Loss: 1.2359 (Reason: 0.9175, Ans: 1.3421)
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:52<00:00,  1.
89it/s]
Evaluation - Total Loss: 1.6395, Reason Loss: 0.9078, Answer Loss: 1.8834
2025-04-08 21:08:14,099 [INFO] Step 8 - eval_loss: 1.6395
Validation Loss: 1.6395
Epoch 10/12:  32%|█████████████████████████████████████████████████████▌                                                                                                                    | 126/400 [01:45<03:57,  1.
Epoch 10/12:  32%|█████████████████████████████████████████████████████▉                                                                                                                    | 127/400 [01:45<03:40,  1.
Epoch 10/12:  32%|██████████████████████████████████████████████████████▍                                                                                                                   | 128/400 [01:46<03:56,  1.
Epoch 10/12:  32%|██████████████████████████████████████████████████████▊                                                                                                                   | 129/400 [01:47<03:49,  1.
Epoch 10/12:  32%|███████████████████████████████████████████████████████▎                                                                                                                  | 130/400 [01:48<03:37,  1.
Epoch 10/12:  33%|███████████████████████████████████████████████████████▋                                                                                                                  | 131/400 [01:49<03:52,  1.
Epoch 10/12:  33%|████████████████████████████████████████████████████████                                                                                                                  | 132/400 [01:50<04:16,  1.
Epoch 10/12:  33%|████████████████████████████████████████████████████████▌                                                                                                                 | 133/400 [01:51<03:54,  1.
Epoch 10/12:  34%|████████████████████████████████████████████████████████▉                                                                                                                 | 134/400 [01:52<03:50,  1.
Epoch 10/12:  34%|█████████████████████████████████████████████████████████▍                                                                                                                | 135/400 [01:53<03:59,  1.
Epoch 10/12:  34%|█████████████████████████████████████████████████████████▊                                                                                                                | 136/400 [01:53<03:50,  1.
Epoch 10/12:  34%|██████████████████████████████████████████████████████████▏                                                                                                               | 137/400 [01:54<03:52,  1.
Epoch 10/12:  34%|██████████████████████████████████████████████████████████▋                                                                                                               | 138/400 [01:55<04:01,  1.
Epoch 10/12:  35%|███████████████████████████████████████████████████████████                                                                                                               | 139/400 [01:56<03:53,  1.
Epoch 10/12:  35%|███████████████████████████████████████████████████████████▍                                                                                                              | 140/400 [01:57<03:55,  1.
Epoch 10/12:  35%|███████████████████████████████████████████████████████████▉                                                                                                              | 141/400 [01:58<04:02,  1.
Epoch 10/12:  36%|████████████████████████████████████████████████████████████▎                                                                                                             | 142/400 [01:59<03:49,  1.
Epoch 10/12:  36%|████████████████████████████████████████████████████████████▊                                                                                                             | 143/400 [02:00<03:58,  1.
Epoch 10/12:  36%|█████████████████████████████████████████████████████████████▏                                                                                                            | 144/400 [02:01<03:46,  1.
Epoch 10/12:  36%|█████████████████████████████████████████████████████████████▋                                                                                                            | 145/400 [02:02<04:04,  1.
Epoch 10/12:  36%|██████████████████████████████████████████████████████████████                                                                                                            | 146/400 [02:03<03:59,  1.
Epoch 10/12:  37%|██████████████████████████████████████████████████████████████▍                                                                                                           | 147/400 [02:04<04:02,  1.
Epoch 10/12:  37%|██████████████████████████████████████████████████████████████▉                                                                                                           | 148/400 [02:04<03:46,  1.
Epoch 10/12:  37%|███████████████████████████████████████████████████████████████▎                                                                                                          | 149/400 [02:05<03:41,  1.
Epoch 10/12:  38%|███████████████████████████████████████████████████████████████▊                                                                                                          | 150/400 [02:06<03:31,  1.
Epoch 10/12:  38%|████████████████████████████████████████████████████████████████▏                                                                                                         | 151/400 [02:07<03:25,  1.
Epoch 10/12:  38%|████████████████████████████████████████████████████████████████▌                                                                                                         | 152/400 [02:08<03:20,  1.
Epoch 10/12:  38%|█████████████████████████████████████████████████████████████████                                                                                                         | 153/400 [02:08<03:17,  1.
Epoch 10/12:  38%|█████████████████████████████████████████████████████████████████▍                                                                                                        | 154/400 [02:09<03:25,  1.
Epoch 10/12:  39%|█████████████████████████████████████████████████████████████████▉                                                                                                        | 155/400 [02:10<03:38,  1.
Epoch 10/12:  39%|██████████████████████████████████████████████████████████████████▎                                                                                                       | 156/400 [02:11<03:39,  1.
Epoch 10/12:  39%|██████████████████████████████████████████████████████████████████▋                                                                                                       | 157/400 [02:12<03:47,  1.
Epoch 10/12:  40%|███████████████████████████████████████████████████████████████████▏                                                                                                      | 158/400 [02:13<03:44,  1.
Epoch 10/12:  40%|███████████████████████████████████████████████████████████████████▌                                                                                                      | 159/400 [02:14<03:32,  1.
Epoch 10/12:  40%|████████████████████████████████████████████████████████████████████                                                                                                      | 160/400 [02:15<03:24,  1.
Epoch 10/12:  40%|████████████████████████████████████████████████████████████████████▍                                                                                                     | 161/400 [02:16<03:33,  1.
Epoch 10/12:  40%|████████████████████████████████████████████████████████████████████▊                                                                                                     | 162/400 [02:17<03:25,  1.
Epoch 10/12:  41%|█████████████████████████████████████████████████████████████████████▎                                                                                                    | 163/400 [02:18<03:34,  1.
Epoch 10/12:  41%|█████████████████████████████████████████████████████████████████████▋                                                                                                    | 164/400 [02:19<03:40,  1.
Epoch 10/12:  41%|██████████████████████████████████████████████████████████████████████▏                                                                                                   | 165/400 [02:19<03:32,  1.
Epoch 10/12:  42%|██████████████████████████████████████████████████████████████████████▌                                                                                                   | 166/400 [02:20<03:39,  1.
Epoch 10/12:  42%|██████████████████████████████████████████████████████████████████████▉                                                                                                   | 167/400 [02:21<03:50,  1.
Epoch 10/12:  42%|███████████████████████████████████████████████████████████████████████▍                                                                                                  | 168/400 [02:22<03:45,  1.
Epoch 10/12:  42%|███████████████████████████████████████████████████████████████████████▊                                                                                                  | 169/400 [02:23<03:48,  1.
Epoch 10/12:  42%|████████████████████████████████████████████████████████████████████████▎                                                                                                 | 170/400 [02:24<03:36,  1.
Epoch 10/12:  43%|████████████████████████████████████████████████████████████████████████▋                                                                                                 | 171/400 [02:25<03:14,  1.
Epoch 10/12:  43%|█████████████████████████████████████████████████████████████████████████                                                                                                 | 172/400 [02:26<03:02,  1.
Epoch 10/12:  43%|█████████████████████████████████████████████████████████████████████████▌                                                                                                | 173/400 [02:26<02:53,  1.
Epoch 10/12:  44%|█████████████████████████████████████████████████████████████████████████▉                                                                                                | 174/400 [02:27<03:08,  1.
Epoch 10/12:  44%|██████████████████████████████████████████████████████████████████████████▍                                                                                               | 175/400 [02:28<03:03,  1.
Epoch 10/12:  44%|██████████████████████████████████████████████████████████████████████████▊                                                                                               | 176/400 [02:29<03:21,  1.
Epoch 10/12:  44%|███████████████████████████████████████████████████████████████████████████▏                                                                                              | 177/400 [02:30<03:13,  1.
Epoch 10/12:  44%|███████████████████████████████████████████████████████████████████████████▋                                                                                              | 178/400 [02:31<03:15,  1.
Epoch 10/12:  45%|████████████████████████████████████████████████████████████████████████████                                                                                              | 179/400 [02:32<03:31,  1.
Epoch 10/12:  45%|████████████████████████████████████████████████████████████████████████████▌                                                                                             | 180/400 [02:33<03:34,  1.
Epoch 10/12:  45%|████████████████████████████████████████████████████████████████████████████▉                                                                                             | 181/400 [02:34<03:24,  1.
Epoch 10/12:  46%|█████████████████████████████████████████████████████████████████████████████▎                                                                                            | 182/400 [02:35<03:21,  1.
Epoch 10/12:  46%|█████████████████████████████████████████████████████████████████████████████▊                                                                                            | 183/400 [02:36<03:26,  1.
Epoch 10/12:  46%|██████████████████████████████████████████████████████████████████████████████▏                                                                                           | 184/400 [02:37<03:29,  1.
Epoch 10/12:  46%|██████████████████████████████████████████████████████████████████████████████▋                                                                                           | 185/400 [02:38<03:37,  1.
Epoch 10/12:  46%|███████████████████████████████████████████████████████████████████████████████                                                                                           | 186/400 [02:39<03:16,  1.
Epoch 10/12:  47%|███████████████████████████████████████████████████████████████████████████████▍                                                                                          | 187/400 [02:39<03:00,  1.
Epoch 10/12:  47%|███████████████████████████████████████████████████████████████████████████████▉                                                                                          | 188/400 [02:40<03:08,  1.
Epoch 10/12:  47%|████████████████████████████████████████████████████████████████████████████████▎                                                                                         | 189/400 [02:41<02:56,  1.
Epoch 10/12:  48%|████████████████████████████████████████████████████████████████████████████████▊                                                                                         | 190/400 [02:42<02:51,  1.
Epoch 10/12:  48%|█████████████████████████████████████████████████████████████████████████████████▏                                                                                        | 191/400 [02:43<02:51,  1.
Epoch 10/12:  48%|█████████████████████████████████████████████████████████████████████████████████▌                                                                                        | 192/400 [02:44<03:01,  1.
Epoch 10/12:  48%|██████████████████████████████████████████████████████████████████████████████████                                                                                        | 193/400 [02:45<03:09,  1.
Epoch 10/12:  48%|██████████████████████████████████████████████████████████████████████████████████▍                                                                                       | 194/400 [02:46<03:14,  1.
Epoch 10/12:  49%|██████████████████████████████████████████████████████████████████████████████████▉                                                                                       | 195/400 [02:47<03:17,  1.
Epoch 10/12:  49%|███████████████████████████████████████████████████████████████████████████████████▎                                                                                      | 196/400 [02:47<03:12,  1.
Epoch 10/12:  49%|███████████████████████████████████████████████████████████████████████████████████▋                                                                                      | 197/400 [02:48<02:55,  1.
Epoch 10/12:  50%|████████████████████████████████████████████████████████████████████████████████████▏                                                                                     | 198/400 [02:49<02:57,  1.
Epoch 10/12:  50%|████████████████████████████████████████████████████████████████████████████████████▌                                                                                     | 199/400 [02:50<03:04,  1.
Epoch 10/12:  50%|█████████████████████████████████████████████████████████████████████████████████████                                                                                     | 200/400 [02:51<03:09,  1.
Epoch 10/12:  50%|█████████████████████████████████████████████████████████████████████████████████████▍                                                                                    | 201/400 [02:52<03:12,  1.
Epoch 10/12:  50%|█████████████████████████████████████████████████████████████████████████████████████▊                                                                                    | 202/400 [02:53<03:09,  1.
Epoch 10/12:  51%|██████████████████████████████████████████████████████████████████████████████████████▎                                                                                   | 203/400 [02:54<03:01,  1.
Epoch 10/12:  51%|██████████████████████████████████████████████████████████████████████████████████████▋                                                                                   | 204/400 [02:55<02:51,  1.
Epoch 10/12:  51%|███████████████████████████████████████████████████████████████████████████████████████                                                                                   | 205/400 [02:55<02:39,  1.
Epoch 10/12:  52%|███████████████████████████████████████████████████████████████████████████████████████▌                                                                                  | 206/400 [02:56<02:44,  1.
Epoch 10/12:  52%|███████████████████████████████████████████████████████████████████████████████████████▉                                                                                  | 207/400 [02:57<02:44,  1.
Epoch 10/12:  52%|████████████████████████████████████████████████████████████████████████████████████████▍                                                                                 | 208/400 [02:58<02:28,  1.
Epoch 10/12:  52%|████████████████████████████████████████████████████████████████████████████████████████▊                                                                                 | 209/400 [02:58<02:29,  1.
Epoch 10/12:  52%|█████████████████████████████████████████████████████████████████████████████████████████▎                                                                                | 210/400 [02:59<02:28,  1.
Epoch 10/12:  53%|█████████████████████████████████████████████████████████████████████████████████████████▋                                                                                | 211/400 [03:00<02:34,  1.
Epoch 10/12:  53%|██████████████████████████████████████████████████████████████████████████████████████████                                                                                | 212/400 [03:01<02:44,  1.
Epoch 10/12:  53%|██████████████████████████████████████████████████████████████████████████████████████████▌                                                                               | 213/400 [03:02<02:57,  1.
Epoch 10/12:  54%|██████████████████████████████████████████████████████████████████████████████████████████▉                                                                               | 214/400 [03:03<03:00,  1.
Epoch 10/12:  54%|███████████████████████████████████████████████████████████████████████████████████████████▍                                                                              | 215/400 [03:04<02:55,  1.
Epoch 10/12:  54%|███████████████████████████████████████████████████████████████████████████████████████████▊                                                                              | 216/400 [03:05<02:48,  1.
Epoch 10/12:  54%|████████████████████████████████████████████████████████████████████████████████████████████▏                                                                             | 217/400 [03:06<02:58,  1.
Epoch 10/12:  55%|████████████████████████████████████████████████████████████████████████████████████████████▋                                                                             | 218/400 [03:07<02:50,  1.
Epoch 10/12:  55%|█████████████████████████████████████████████████████████████████████████████████████████████                                                                             | 219/400 [03:08<02:47,  1.
Epoch 10/12:  55%|█████████████████████████████████████████████████████████████████████████████████████████████▌                                                                            | 220/400 [03:09<02:50,  1.
Epoch 10/12:  55%|█████████████████████████████████████████████████████████████████████████████████████████████▉                                                                            | 221/400 [03:10<02:47,  1.
Epoch 10/12:  56%|██████████████████████████████████████████████████████████████████████████████████████████████▎                                                                           | 222/400 [03:11<02:33,  1.
Epoch 10/12:  56%|██████████████████████████████████████████████████████████████████████████████████████████████▊                                                                           | 223/400 [03:11<02:27,  1.
Epoch 10/12:  56%|███████████████████████████████████████████████████████████████████████████████████████████████▏                                                                          | 224/400 [03:12<02:27,  1.
Epoch 10/12:  56%|███████████████████████████████████████████████████████████████████████████████████████████████▋                                                                          | 225/400 [03:13<02:31,  1.
Epoch 10/12:  56%|████████████████████████████████████████████████████████████████████████████████████████████████                                                                          | 226/400 [03:14<02:44,  1.
Epoch 10/12:  57%|████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                         | 227/400 [03:15<02:42,  1.
Epoch 10/12:  57%|████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                         | 228/400 [03:16<02:45,  1.
Epoch 10/12:  57%|█████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                        | 229/400 [03:17<02:35,  1.
Epoch 10/12:  57%|█████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                        | 230/400 [03:18<02:27,  1.
Epoch 10/12:  58%|██████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                       | 231/400 [03:18<02:10,  1.
Epoch 10/12:  58%|██████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                       | 232/400 [03:19<01:53,  1.
Epoch 10/12:  58%|███████████████████████████████████████████████████████████████████████████████████████████████████                                                                       | 233/400 [03:19<01:47,  1.
Epoch 10/12:  58%|███████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                      | 234/400 [03:20<01:34,  1.
Epoch 10/12:  59%|███████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                      | 235/400 [03:20<01:30,  1.
Epoch 10/12:  59%|████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                     | 236/400 [03:21<01:25,  1.
Epoch 10/12:  59%|████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                     | 237/400 [03:21<01:21,  1.
Epoch 10/12:  60%|█████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                    | 238/400 [03:22<01:24,  1.
Epoch 10/12:  60%|█████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                    | 239/400 [03:22<01:23,  1.
Epoch 10/12:  60%|██████████████████████████████████████████████████████████████████████████████████████████████████████                                                                    | 240/400 [03:23<01:22,  1.
Epoch 10/12:  60%|██████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                   | 241/400 [03:23<01:16,  2.
Epoch 10/12:  60%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                   | 242/400 [03:24<01:14,  2.
Epoch 10/12:  61%|███████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                  | 243/400 [03:24<01:15,  2.
Epoch 10/12:  61%|███████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                  | 244/400 [03:25<01:15,  2.
Epoch 10/12:  61%|████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                 | 245/400 [03:25<01:10,  2.
Epoch 10/12:  62%|████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                 | 246/400 [03:25<01:07,  2.
Epoch 10/12:  62%|████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                 | 247/400 [03:26<01:06,  2.
Epoch 10/12:  62%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                | 248/400 [03:26<01:11,  2.
Epoch 10/12:  62%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                | 249/400 [03:27<01:07,  2.
Epoch 10/12:  62%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                               | 250/400 [03:27<01:06,  2.
Epoch 10/12:  63%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                               | 251/400 [03:27<01:03,  2.
Epoch 10/12:  63%|███████████████████████████████████████████████████████████████████████████████████████████████████████████                                                               | 252/400 [03:28<01:06,  2.
Epoch 10/12:  63%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                              | 253/400 [03:28<01:03,  2.
Epoch 10/12:  64%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                              | 254/400 [03:29<01:04,  2.
Epoch 10/12:  64%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                             | 255/400 [03:29<01:06,  2.
Epoch 10/12:  64%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                             | 256/400 [03:30<01:02,  2.
Epoch 10/12:  64%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                            | 257/400 [03:30<01:03,  2.
Epoch 10/12:  64%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                            | 258/400 [03:31<01:03,  2.
Epoch 10/12:  65%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                            | 259/400 [03:31<01:01,  2.
Epoch 10/12:  65%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                           | 260/400 [03:31<00:59,  2.
Epoch 10/12:  65%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                           | 261/400 [03:32<00:57,  2.
Epoch 10/12:  66%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                          | 262/400 [03:32<01:00,  2.
Epoch 10/12:  66%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                          | 263/400 [03:33<01:03,  2.
Epoch 10/12:  66%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                         | 264/400 [03:33<01:00,  2.
Epoch 10/12:  66%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                         | 265/400 [03:34<00:59,  2.
Epoch 10/12:  66%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                         | 266/400 [03:34<00:59,  2.
Epoch 10/12:  67%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                        | 267/400 [03:35<00:57,  2.
Epoch 10/12:  67%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                        | 268/400 [03:35<01:02,  2.
Epoch 10/12:  67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                       | 269/400 [03:35<00:57,  2.
Epoch 10/12:  68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                       | 270/400 [03:36<00:54,  2.
Epoch 10/12:  68%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                      | 271/400 [03:36<00:55,  2.
Epoch 10/12:  68%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                      | 272/400 [03:37<00:58,  2.
Epoch 10/12:  68%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                      | 273/400 [03:37<00:57,  2.
Epoch 10/12:  68%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                     | 274/400 [03:38<00:54,  2.
Epoch 10/12:  69%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                     | 275/400 [03:38<00:53,  2.
Epoch 10/12:  69%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                    | 276/400 [03:39<00:55,  2.
Epoch 10/12:  69%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                    | 277/400 [03:39<00:57,  2.
Epoch 10/12:  70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                   | 278/400 [03:39<00:54,  2.
Epoch 10/12:  70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                   | 279/400 [03:40<00:53,  2.
Epoch 10/12:  70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                   | 280/400 [03:40<00:54,  2.
Epoch 10/12:  70%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                  | 281/400 [03:41<00:53,  2.
Epoch 10/12:  70%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                  | 282/400 [03:41<00:50,  2.
Epoch 10/12:  71%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                 | 283/400 [03:42<00:48,  2.
Epoch 10/12:  71%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                 | 284/400 [03:42<01:02,  1.
Epoch 10/12:  71%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                | 285/400 [03:43<01:18,  1.
Epoch 10/12:  72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                | 286/400 [03:44<01:20,  1.
Epoch 10/12:  72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                | 287/400 [03:45<01:22,  1.
Epoch 10/12:  72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                               | 288/400 [03:46<01:30,  1.
Epoch 10/12:  72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                               | 289/400 [03:47<01:40,  1.
Epoch 10/12:  72%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                              | 290/400 [03:48<01:32,  1.
Epoch 10/12:  73%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                              | 291/400 [03:49<01:30,  1.
Epoch 10/12:  73%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                              | 292/400 [03:49<01:29,  1.
Epoch 10/12:  73%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                             | 293/400 [03:50<01:29,  1.
Epoch 10/12:  74%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                             | 294/400 [03:51<01:31,  1.
Epoch 10/12:  74%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                            | 295/400 [03:52<01:28,  1.
Epoch 10/12:  74%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                            | 296/400 [03:53<01:25,  1.
Epoch 10/12:  74%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                           | 297/400 [03:54<01:22,  1.
Epoch 10/12:  74%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                           | 298/400 [03:54<01:24,  1.
Epoch 10/12:  75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                           | 299/400 [03:55<01:21,  1.
Epoch 10/12:  75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                          | 300/400 [03:56<01:27,  1.
Epoch 10/12:  75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                          | 301/400 [03:57<01:30,  1.
Epoch 10/12:  76%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                         | 302/400 [03:58<01:32,  1.
Epoch 10/12:  76%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                         | 303/400 [03:59<01:29,  1.
Epoch 10/12:  76%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                        | 304/400 [04:00<01:34,  1.
Epoch 10/12:  76%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                        | 305/400 [04:01<01:38,  1.
Epoch 10/12:  76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                        | 306/400 [04:02<01:28,  1.
Epoch 10/12:  77%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                       | 307/400 [04:03<01:23,  1.
Epoch 10/12:  77%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                       | 308/400 [04:04<01:19,  1.
Epoch 10/12:  77%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                      | 309/400 [04:05<01:17,  1.
Epoch 10/12:  78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                      | 310/400 [04:05<01:16,  1.
Epoch 10/12:  78%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                     | 311/400 [04:06<01:22,  1.
Epoch 10/12:  78%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                     | 312/400 [04:08<01:23,  1.
Epoch 10/12:  78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                     | 313/400 [04:08<01:15,  1.
Epoch 10/12:  78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                    | 314/400 [04:09<01:12,  1.
Epoch 10/12:  79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                    | 315/400 [04:10<01:16,  1.
Epoch 10/12:  79%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                   | 316/400 [04:11<01:16,  1.
Epoch 10/12:  79%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                   | 317/400 [04:12<01:11,  1.
Epoch 10/12:  80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                  | 318/400 [04:13<01:11,  1.
Epoch 10/12:  80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 319/400 [04:13<01:09,  1.
Epoch 10/12:  80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                  | 320/400 [04:14<01:07,  1.
Epoch 10/12:  80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                 | 321/400 [04:15<01:08,  1.
Epoch 10/12:  80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                 | 322/400 [04:16<01:08,  1.
Epoch 10/12:  81%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                | 323/400 [04:17<01:10,  1.
Epoch 10/12:  81%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                | 324/400 [04:18<01:12,  1.
Epoch 10/12:  81%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                               | 325/400 [04:19<01:12,  1.
Epoch 10/12:  82%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                               | 326/400 [04:20<01:07,  1.
Epoch 10/12:  82%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                               | 327/400 [04:21<01:01,  1.
Epoch 10/12:  82%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                              | 328/400 [04:21<01:02,  1.
Epoch 10/12:  82%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                              | 329/400 [04:22<01:02,  1.
Epoch 10/12:  82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                             | 330/400 [04:23<01:02,  1.
Epoch 10/12:  83%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                             | 331/400 [04:24<00:57,  1.
Epoch 10/12:  83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                             | 332/400 [04:25<01:03,  1.
Epoch 10/12:  83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                            | 333/400 [04:26<01:00,  1.
Epoch 10/12:  84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                            | 334/400 [04:26<00:51,  1.
Epoch 10/12:  84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                           | 335/400 [04:28<00:56,  1.
Epoch 10/12:  84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                           | 336/400 [04:29<01:00,  1.
Epoch 10/12:  84%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                          | 337/400 [04:30<00:57,  1.
Epoch 10/12:  84%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                          | 338/400 [04:30<00:56,  1.
Epoch 10/12:  85%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                          | 339/400 [04:32<00:59,  1.
Epoch 10/12:  85%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 340/400 [04:32<00:52,  1.
Epoch 10/12:  85%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                         | 341/400 [04:33<00:50,  1.
Epoch 10/12:  86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                        | 342/400 [04:34<00:49,  1.
Epoch 10/12:  86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                        | 343/400 [04:35<00:46,  1.
Epoch 10/12:  86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                       | 344/400 [04:36<00:47,  1.
Epoch 10/12:  86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                       | 345/400 [04:36<00:43,  1.
Epoch 10/12:  86%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                       | 346/400 [04:37<00:43,  1.
Epoch 10/12:  87%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                      | 347/400 [04:38<00:45,  1.
Epoch 10/12:  87%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                      | 348/400 [04:39<00:42,  1.
Epoch 10/12:  87%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                     | 349/400 [04:40<00:45,  1.
Epoch 10/12:  88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                     | 350/400 [04:41<00:42,  1.
Epoch 10/12:  88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                    | 351/400 [04:41<00:41,  1.
Epoch 10/12:  88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                    | 352/400 [04:42<00:40,  1.
Epoch 10/12:  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                    | 353/400 [04:43<00:39,  1.
Epoch 10/12:  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                   | 354/400 [04:44<00:39,  1.
Epoch 10/12:  89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                   | 355/400 [04:45<00:36,  1.
Epoch 10/12:  89%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                  | 356/400 [04:46<00:37,  1.
Epoch 10/12:  89%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                  | 357/400 [04:47<00:38,  1.
Epoch 10/12:  90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                 | 358/400 [04:47<00:36,  1.
Epoch 10/12:  90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                 | 359/400 [04:49<00:38,  1.
Epoch 10/12:  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 360/400 [04:50<00:38,  1.
Epoch 10/12:  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                | 361/400 [04:51<00:37,  1.
Epoch 10/12:  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                | 362/400 [04:52<00:38,  1.
Epoch 10/12:  91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎               | 363/400 [04:53<00:39,  1.
Epoch 10/12:  91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋               | 364/400 [04:54<00:35,  1.
Epoch 10/12:  91%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏              | 365/400 [04:54<00:30,  1.
Epoch 10/12:  92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌              | 366/400 [04:55<00:30,  1.
Epoch 10/12:  92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉              | 367/400 [04:56<00:30,  1.
Epoch 10/12:  92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍             | 368/400 [04:57<00:30,  1.
Epoch 10/12:  92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊             | 369/400 [04:58<00:28,  1.
Epoch 10/12:  92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎            | 370/400 [04:59<00:27,  1.
Epoch 10/12:  93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋            | 371/400 [05:00<00:28,  1.
Epoch 10/12:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████            | 372/400 [05:01<00:26,  1.
Epoch 10/12:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌           | 373/400 [05:02<00:24,  1.
Epoch 10/12:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉           | 374/400 [05:03<00:22,  1.
Epoch 10/12:  94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍          | 375/400 [05:03<00:22,  1.
Epoch 10/12:  94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊          | 376/400 [05:04<00:19,  1.
Epoch 10/12:  94%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏         | 377/400 [05:05<00:18,  1.
Epoch 10/12:  94%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋         | 378/400 [05:06<00:17,  1.
Epoch 10/12:  95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████         | 379/400 [05:07<00:18,  1.
Epoch 10/12:  95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 380/400 [05:08<00:17,  1.
Epoch 10/12:  95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉        | 381/400 [05:08<00:15,  1.
Epoch 10/12:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎       | 382/400 [05:09<00:14,  1.
Epoch 10/12:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊       | 383/400 [05:10<00:13,  1.
Epoch 10/12:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏      | 384/400 [05:11<00:12,  1.
Epoch 10/12:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋      | 385/400 [05:11<00:12,  1.
Epoch 10/12:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████      | 386/400 [05:13<00:12,  1.
Epoch 10/12:  97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍     | 387/400 [05:14<00:12,  1.
Epoch 10/12:  97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉     | 388/400 [05:15<00:11,  1.
Epoch 10/12:  97%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎    | 389/400 [05:15<00:10,  1.
Epoch 10/12:  98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 390/400 [05:16<00:09,  1.
Epoch 10/12:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏   | 391/400 [05:17<00:08,  1.
Epoch 10/12:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 392/400 [05:18<00:07,  1.
Epoch 10/12:  98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████   | 393/400 [05:19<00:06,  1.
Epoch 10/12:  98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍  | 394/400 [05:20<00:05,  1.
Epoch 10/12:  99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 395/400 [05:21<00:04,  1.
Epoch 10/12:  99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 396/400 [05:22<00:03,  1.
Epoch 10/12:  99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋ | 397/400 [05:23<00:02,  1.
Epoch 10/12: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏| 398/400 [05:24<00:01,  1.
Epoch 10/12: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌| 399/400 [05:25<00:00,  1.
Epoch 10/12: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [05:26<00:00,  1.
Epoch 10/12: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [05:26<00:00,  1.
23it/s]
2025-04-08 21:13:40,288 [INFO] Step 9 - total_loss: 1.2083, reason_loss: 0.9181, ans_loss: 1.3051
Epoch 10 - Loss: 1.2083 (Reason: 0.9181, Ans: 1.3051)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:04<00:00,  1.55it/s]
Evaluation - Total Loss: 1.6885, Reason Loss: 0.9101, Answer Loss: 1.9480
2025-04-08 21:14:44,753 [INFO] Step 9 - eval_loss: 1.6885
Validation Loss: 1.6885
Epoch 11/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [05:10<00:00,  1.29it/s]
2025-04-08 21:19:55,419 [INFO] Step 10 - total_loss: 1.1979, reason_loss: 0.9209, ans_loss: 1.2903
Epoch 11 - Loss: 1.1979 (Reason: 0.9209, Ans: 1.2903)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:04<00:00,  1.55it/s]
Evaluation - Total Loss: 1.5848, Reason Loss: 0.9140, Answer Loss: 1.8084
2025-04-08 21:20:59,981 [INFO] Step 10 - eval_loss: 1.5848
Validation Loss: 1.5848
Epoch 12/12:   0%|                                                                                                                                                                             | 0/400 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/nee7ne/EfficientCoT/main.py", line 210, in <module>
    main()
  File "/home/nee7ne/EfficientCoT/main.py", line 166, in main
    train_contemplation_generator(
  File "/home/nee7ne/EfficientCoT/training/train_contemp_gen.py", line 202, in train_contemplation_generator
    optimizer.step()
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/optim/optimizer.py", line 493, in wrapper
    out = func(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/optim/adamw.py", line 243, in step
    adamw(
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/optim/adamw.py", line 875, in adamw
    func(
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/optim/adamw.py", line 699, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 6.56 MiB is free. Process 1820723 has 796.00 MiB memory in use. Including non-PyTorch memory,
 this process has 47.58 GiB memory in use. Process 2694839 has 30.77 GiB memory in use. Of the allocated memory 47.04 GiB is allocated by PyTorch, and 42.48 MiB is reserved by PyTorch but unallocated. If reserved bu
t unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environme
nt-variables)
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ export CUDA_VISIBLE_DEVICES=2
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ python main.py --mode train_contemp_generator --config mistral
Loading checkpoint shards:  67%|██████████████████████████████████████████████████████████████████████████████████████████████████████                                                   | 2/3 [00:01<00:00,  1.48it/s]
Loading checkpoint shards:  67%|██████████████████████████████████████████████████████████████████████████████████████████████████████                                                   | 2/3 [00:01<00:00,  1.21it/s]
Traceback (most recent call last):
  File "/home/nee7ne/EfficientCoT/main.py", line 210, in <module>
    main()
  File "/home/nee7ne/EfficientCoT/main.py", line 153, in main
    sentence_transformer = CustomizedSentenceTransformer.from_pretrained(
  File "/home/nee7ne/EfficientCoT/models/sentence_transformer.py", line 204, in from_pretrained
    model = cls(
  File "/home/nee7ne/EfficientCoT/models/sentence_transformer.py", line 13, in __init__
    base_model = AutoModel.from_pretrained(base_model_name)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/modeling_utils.py", line 262, in _wrapper
    return func(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4319, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4921, in _load_pretrained_model
    error_msgs += _load_state_dict_into_model(
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/modeling_utils.py", line 717, in _load_state_dict_into_model
    load(model_to_load, state_dict, prefix=start_prefix, assign_to_params_buffers=assign_to_params_buffers)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/modeling_utils.py", line 715, in load
    load(child, state_dict, prefix + name + ".", assign_to_params_buffers)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/modeling_utils.py", line 715, in load
    load(child, state_dict, prefix + name + ".", assign_to_params_buffers)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/modeling_utils.py", line 715, in load
    load(child, state_dict, prefix + name + ".", assign_to_params_buffers)
  [Previous line repeated 1 more time]
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/modeling_utils.py", line 711, in load
    module._load_from_state_dict(*args)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2438, in _load_from_state_dict
    param.copy_(input_param)
KeyboardInterrupt

(effi_cot) nee7ne@uvavast:~/EfficientCoT$ ^C
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ clear
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ python main.py --mode train_contemp_generator --config mistral --dataset svamp
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.50it/s]
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
2025-04-08 21:33:49,129 [INFO] Logging to ./logs/contemp_generator
2025-04-08 21:33:49,129 [INFO] Hyperparameters: {'config_name': 'mistral', 'log_dir': './logs', 'model_save_path': '/data/nee7ne/effi_cot/saved_models/effi_cot/vanilla/mistral/svamp', 'checkpoint_path': './checkpoin
ts/effi_cot/vanilla/mistral/svamp', 'result_path': './results/effi_cot/vanilla/mistral/svamp', 'experiment_name': 'effi_cot_vanilla_42_svamp_mistral', 'learning_rate': 1e-05, 'weight_decay': 0.01, 'num_epochs': 12,
'train_sen_trans_epochs': 15, 'batch_size': 4, 'alpha': 0.25, 'save_interval': 1, 'max_seq_length': 150, 'embedding_dim': 768, 'seed': 42, 'max_reasoning_pairs': 400, 'max_contemp_tokens': 5, 'start_layer_idx': 16,
'end_layer_idx': 20, 'reasoning_pairs_path': '/home/nee7ne/EfficientCoT/gen_datasets', 'device': 0, 'ccot_stage': 'encode', 'eval_temp': 0.3, 'teacher_model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'student_mode
l_name': 'optimum/mistral-1.1b-testing', 'student_model_path': './saved_models/contemp_generator', 'sentence_transformer_path': './saved_models/sentence_transformer', 'data_path': 'ChilleD/SVAMP', 'teacher_hidden_di
m': 4096, 'contemp_seq_length': 32, 'contemp_layer_index': 16}
Starting training contemplation generator...
Epoch 1/12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:08<00:00,  3.11it/s]
2025-04-08 21:35:57,636 [INFO] Step 0 - total_loss: 1.6044, reason_loss: 1.0000, ans_loss: 1.8059
Epoch 1 - Loss: 1.6044 (Reason: 1.0000, Ans: 1.8059)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:20<00:00,  4.97it/s]
Evaluation - Total Loss: 1.4581, Reason Loss: 0.9753, Answer Loss: 1.6191
2025-04-08 21:36:17,749 [INFO] Step 0 - eval_loss: 1.4581
Validation Loss: 1.4581
Epoch 2/12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:14<00:00,  2.98it/s]
2025-04-08 21:38:32,116 [INFO] Step 1 - total_loss: 1.3221, reason_loss: 0.9893, ans_loss: 1.4331
Epoch 2 - Loss: 1.3221 (Reason: 0.9893, Ans: 1.4331)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:20<00:00,  4.96it/s]
Evaluation - Total Loss: 1.4138, Reason Loss: 0.9500, Answer Loss: 1.5683
2025-04-08 21:38:52,285 [INFO] Step 1 - eval_loss: 1.4138
Validation Loss: 1.4138
Epoch 3/12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:09<00:00,  3.08it/s]
2025-04-08 21:41:02,081 [INFO] Step 2 - total_loss: 1.2585, reason_loss: 0.9816, ans_loss: 1.3508
Epoch 3 - Loss: 1.2585 (Reason: 0.9816, Ans: 1.3508)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:20<00:00,  4.94it/s]
Evaluation - Total Loss: 1.4024, Reason Loss: 0.9333, Answer Loss: 1.5587
2025-04-08 21:41:22,319 [INFO] Step 2 - eval_loss: 1.4024
Validation Loss: 1.4024
Epoch 4/12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:14<00:00,  2.98it/s]
2025-04-08 21:43:36,410 [INFO] Step 3 - total_loss: 1.2203, reason_loss: 0.9751, ans_loss: 1.3020
Epoch 4 - Loss: 1.2203 (Reason: 0.9751, Ans: 1.3020)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:20<00:00,  4.95it/s]
Evaluation - Total Loss: 1.3313, Reason Loss: 0.9263, Answer Loss: 1.4663
2025-04-08 21:43:56,606 [INFO] Step 3 - eval_loss: 1.3313
Validation Loss: 1.3313
Epoch 5/12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:09<00:00,  3.08it/s]
2025-04-08 21:46:06,354 [INFO] Step 4 - total_loss: 1.1774, reason_loss: 0.9724, ans_loss: 1.2458
Epoch 5 - Loss: 1.1774 (Reason: 0.9724, Ans: 1.2458)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:20<00:00,  4.94it/s]
Evaluation - Total Loss: 1.3511, Reason Loss: 0.9185, Answer Loss: 1.4953
2025-04-08 21:46:26,579 [INFO] Step 4 - eval_loss: 1.3511
Validation Loss: 1.3511
Epoch 6/12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:12<00:00,  3.01it/s]
2025-04-08 21:48:39,453 [INFO] Step 5 - total_loss: 1.1307, reason_loss: 0.9694, ans_loss: 1.1844
Epoch 6 - Loss: 1.1307 (Reason: 0.9694, Ans: 1.1844)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:20<00:00,  4.94it/s]
Evaluation - Total Loss: 1.3605, Reason Loss: 0.9160, Answer Loss: 1.5087
2025-04-08 21:48:59,688 [INFO] Step 5 - eval_loss: 1.3605
Validation Loss: 1.3605
Epoch 7/12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:09<00:00,  3.08it/s]
2025-04-08 21:51:09,391 [INFO] Step 6 - total_loss: 1.0857, reason_loss: 0.9690, ans_loss: 1.1246
Epoch 7 - Loss: 1.0857 (Reason: 0.9690, Ans: 1.1246)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:20<00:00,  4.94it/s]
Evaluation - Total Loss: 1.3900, Reason Loss: 0.9115, Answer Loss: 1.5495
2025-04-08 21:51:29,645 [INFO] Step 6 - eval_loss: 1.3900
Validation Loss: 1.3900
Epoch 8/12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:14<00:00,  2.98it/s]
2025-04-08 21:53:43,967 [INFO] Step 7 - total_loss: 1.0863, reason_loss: 0.9682, ans_loss: 1.1257
Epoch 8 - Loss: 1.0863 (Reason: 0.9682, Ans: 1.1257)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:20<00:00,  4.95it/s]
Evaluation - Total Loss: 1.4204, Reason Loss: 0.9075, Answer Loss: 1.5913
2025-04-08 21:54:04,183 [INFO] Step 7 - eval_loss: 1.4204
Validation Loss: 1.4204
Epoch 9/12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:08<00:00,  3.12it/s]
2025-04-08 21:56:12,491 [INFO] Step 8 - total_loss: 1.0600, reason_loss: 0.9666, ans_loss: 1.0911
Epoch 9 - Loss: 1.0600 (Reason: 0.9666, Ans: 1.0911)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:20<00:00,  4.94it/s]
Evaluation - Total Loss: 1.3475, Reason Loss: 0.9023, Answer Loss: 1.4960
2025-04-08 21:56:32,717 [INFO] Step 8 - eval_loss: 1.3475
Validation Loss: 1.3475
Epoch 10/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:09<00:00,  3.09it/s]
2025-04-08 21:58:42,111 [INFO] Step 9 - total_loss: 1.0192, reason_loss: 0.9653, ans_loss: 1.0371
Epoch 10 - Loss: 1.0192 (Reason: 0.9653, Ans: 1.0371)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:20<00:00,  4.86it/s]
Evaluation - Total Loss: 1.3728, Reason Loss: 0.8987, Answer Loss: 1.5309
2025-04-08 21:59:02,692 [INFO] Step 9 - eval_loss: 1.3728
Validation Loss: 1.3728
Epoch 11/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [02:10<00:00,  3.07it/s]
2025-04-08 22:01:12,939 [INFO] Step 10 - total_loss: 0.9921, reason_loss: 0.9664, ans_loss: 1.0006
Epoch 11 - Loss: 0.9921 (Reason: 0.9664, Ans: 1.0006)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:20<00:00,  4.95it/s]
Evaluation - Total Loss: 1.4870, Reason Loss: 0.9028, Answer Loss: 1.6818
2025-04-08 22:01:33,154 [INFO] Step 10 - eval_loss: 1.4870
Validation Loss: 1.4870
Epoch 12/12: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [03:03<00:00,  2.18it/s]
2025-04-08 22:04:36,296 [INFO] Step 11 - total_loss: 0.9908, reason_loss: 0.9650, ans_loss: 0.9995
Epoch 12 - Loss: 0.9908 (Reason: 0.9650, Ans: 0.9995)
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:20<00:00,  4.96it/s]
Evaluation - Total Loss: 1.3920, Reason Loss: 0.9043, Answer Loss: 1.5546
2025-04-08 22:04:56,469 [INFO] Step 11 - eval_loss: 1.3920
Validation Loss: 1.3920
Saved best model with validation loss: 1.3313
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ bash run_para_search.sh
Starting evaluation runs with 45 combinations...
[1/45] Running with max_contemp_tokens=1, eval_temp=0.1, dataset=svamp, config=small, config=small
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference:  31%|████████████████████████████████████████████████▉                                                                                                             | 31/100 [00:48<01:53,  1.65s/it]
Running inference:  31%|████████████████████████████████████████████████▉                                                                                                             | 31/100 [00:50<01:51,  1.61s/it]
Traceback (most recent call last):
  File "/home/nee7ne/EfficientCoT/main.py", line 210, in <module>
    main()
  File "/home/nee7ne/EfficientCoT/main.py", line 181, in main
    results = run_inference(
  File "/home/nee7ne/EfficientCoT/inference/inference.py", line 152, in run_inference
    outputs = teacher_model.generate(
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/generation/utils.py", line 2223, in generate
    result = self._sample(
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/generation/utils.py", line 3204, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/site-packages/transformers/generation/utils.py", line 431, in prepare_inputs_for_generation
    and position_ids_key in set(inspect.signature(self.forward).parameters.keys())
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/inspect.py", line 3254, in signature
    return Signature.from_callable(obj, follow_wrapped=follow_wrapped,
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/inspect.py", line 3002, in from_callable
    return _signature_from_callable(obj, sigcls=cls,
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/inspect.py", line 2401, in _signature_from_callable
    sig = _get_signature_of(obj.__func__)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/inspect.py", line 2463, in _signature_from_callable
    return _signature_from_function(sigcls, obj,
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/inspect.py", line 2334, in _signature_from_function
    parameters.append(Parameter(name, annotation=annotation,
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/inspect.py", line 2639, in __init__
    self._kind = _ParameterKind(kind)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/enum.py", line 385, in __call__
    return cls.__new__(cls, value)
  File "/home/nee7ne/.conda/envs/effi_cot/lib/python3.10/enum.py", line 682, in __new__
    if type(value) is cls:
KeyboardInterrupt

(effi_cot) nee7ne@uvavast:~/EfficientCoT$ bash run_para_search.sh
Starting evaluation runs with 45 combinations...
[1/45] Running with max_contemp_tokens=1, eval_temp=0.1, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:20<00:00,  1.41s/it]
Average time taken for each sample: 1.3852276754379274, Average time taken for contemplation: 0.022134201526641847
Evaluation results: {'numerical_accuracy': 0.42, 'close_match_rate': 0.45, 'mean_relative_error': np.float64(0.6537527969625532), 'median_relative_error': np.float64(0.07324561403508772)}
[2/45] Running with max_contemp_tokens=1, eval_temp=0.2, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference:  89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                 | 89/100 [02:21<00:18,  1.68s/it]
Running inference:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏               | 90/100 [02:23<00:17,  1.70s/it]
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:39<00:00,  1.60s/it]
Average time taken for each sample: 1.5657473015785217, Average time taken for contemplation: 0.022007808685302735
Evaluation results: {'numerical_accuracy': 0.42, 'close_match_rate': 0.45, 'mean_relative_error': np.float64(0.6528126260223823), 'median_relative_error': np.float64(0.07324561403508772)}
[3/45] Running with max_contemp_tokens=1, eval_temp=0.3, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:36<00:00,  1.04it/s]
Average time taken for each sample: 0.9531884360313415, Average time taken for contemplation: 0.0204865026473999
Evaluation results: {'numerical_accuracy': 0.43, 'close_match_rate': 0.46, 'mean_relative_error': np.float64(0.5652987710987263), 'median_relative_error': np.float64(0.060990712074303406)}
[4/45] Running with max_contemp_tokens=1, eval_temp=0.4, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:16<00:00,  1.30it/s]
Average time taken for each sample: 0.764965226650238, Average time taken for contemplation: 0.03095583915710449
Evaluation results: {'numerical_accuracy': 0.43, 'close_match_rate': 0.46, 'mean_relative_error': np.float64(0.4944686480015308), 'median_relative_error': np.float64(0.060990712074303406)}
[5/45] Running with max_contemp_tokens=1, eval_temp=0.5, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:17<00:00,  1.29it/s]
Average time taken for each sample: 0.7740735387802125, Average time taken for contemplation: 0.020186514854431153
Evaluation results: {'numerical_accuracy': 0.43, 'close_match_rate': 0.46, 'mean_relative_error': np.float64(0.7145745387966954), 'median_relative_error': np.float64(0.07352941176470588)}
[6/45] Running with max_contemp_tokens=1, eval_temp=0.6, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:17<00:00,  1.30it/s]
Average time taken for each sample: 0.7686142325401306, Average time taken for contemplation: 0.020125701427459716
Evaluation results: {'numerical_accuracy': 0.44, 'close_match_rate': 0.47, 'mean_relative_error': np.float64(0.4760997428946087), 'median_relative_error': np.float64(0.04793028322440087)}
[7/45] Running with max_contemp_tokens=1, eval_temp=0.7, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:14<00:00,  1.33it/s]
Average time taken for each sample: 0.7460236978530884, Average time taken for contemplation: 0.019838552474975586
Evaluation results: {'numerical_accuracy': 0.46, 'close_match_rate': 0.49, 'mean_relative_error': np.float64(0.45326240670486256), 'median_relative_error': np.float64(0.014722169561877014)}
[8/45] Running with max_contemp_tokens=1, eval_temp=0.8, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:14<00:00,  1.35it/s]
Average time taken for each sample: 0.7373793768882752, Average time taken for contemplation: 0.01976987361907959
Evaluation results: {'numerical_accuracy': 0.45, 'close_match_rate': 0.48, 'mean_relative_error': np.float64(0.4570014654219723), 'median_relative_error': np.float64(0.02795248078266946)}
[9/45] Running with max_contemp_tokens=1, eval_temp=0.9, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:17<00:00,  1.29it/s]
Average time taken for each sample: 0.7689350581169129, Average time taken for contemplation: 0.029057724475860594
Evaluation results: {'numerical_accuracy': 0.47, 'close_match_rate': 0.49, 'mean_relative_error': np.float64(0.4776513865029417), 'median_relative_error': np.float64(0.014722169561877014)}
[10/45] Running with max_contemp_tokens=2, eval_temp=0.1, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.50it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:14<00:00,  1.34it/s]
Average time taken for each sample: 0.7438073515892029, Average time taken for contemplation: 0.01987224817276001
Evaluation results: {'numerical_accuracy': 0.42, 'close_match_rate': 0.45, 'mean_relative_error': np.float64(0.6537527969625532), 'median_relative_error': np.float64(0.07324561403508772)}
[11/45] Running with max_contemp_tokens=2, eval_temp=0.2, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:15<00:00,  1.33it/s]
Average time taken for each sample: 0.749945650100708, Average time taken for contemplation: 0.02651074409484863
Evaluation results: {'numerical_accuracy': 0.42, 'close_match_rate': 0.45, 'mean_relative_error': np.float64(0.6528126260223823), 'median_relative_error': np.float64(0.07324561403508772)}
[12/45] Running with max_contemp_tokens=2, eval_temp=0.3, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:16<00:00,  1.30it/s]
Average time taken for each sample: 0.7660952687263489, Average time taken for contemplation: 0.019866297245025633
Evaluation results: {'numerical_accuracy': 0.43, 'close_match_rate': 0.46, 'mean_relative_error': np.float64(0.5652987710987263), 'median_relative_error': np.float64(0.060990712074303406)}
[13/45] Running with max_contemp_tokens=2, eval_temp=0.4, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:16<00:00,  1.31it/s]
Average time taken for each sample: 0.7589286041259765, Average time taken for contemplation: 0.019342937469482423
Evaluation results: {'numerical_accuracy': 0.43, 'close_match_rate': 0.46, 'mean_relative_error': np.float64(0.4944686480015308), 'median_relative_error': np.float64(0.060990712074303406)}
[14/45] Running with max_contemp_tokens=2, eval_temp=0.5, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:17<00:00,  1.29it/s]
Average time taken for each sample: 0.7711406373977661, Average time taken for contemplation: 0.019957940578460693
Evaluation results: {'numerical_accuracy': 0.43, 'close_match_rate': 0.46, 'mean_relative_error': np.float64(0.7145745387966954), 'median_relative_error': np.float64(0.07352941176470588)}
[15/45] Running with max_contemp_tokens=2, eval_temp=0.6, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:16<00:00,  1.32it/s]
Average time taken for each sample: 0.7566686701774598, Average time taken for contemplation: 0.019559974670410155
Evaluation results: {'numerical_accuracy': 0.44, 'close_match_rate': 0.47, 'mean_relative_error': np.float64(0.4760997428946087), 'median_relative_error': np.float64(0.04793028322440087)}
[16/45] Running with max_contemp_tokens=2, eval_temp=0.7, dataset=svamp, config=small, config=mistral
Loading checkpoint shards:  67%|██████████████████████████████████████████████████████████████████████████████████████████████████████                                                   | 2/3 [00:01<00:00,  1.43it/s]
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.42it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:15<00:00,  1.32it/s]
Average time taken for each sample: 0.7546680569648743, Average time taken for contemplation: 0.027992484569549562
Evaluation results: {'numerical_accuracy': 0.46, 'close_match_rate': 0.49, 'mean_relative_error': np.float64(0.45326240670486256), 'median_relative_error': np.float64(0.014722169561877014)}
[17/45] Running with max_contemp_tokens=2, eval_temp=0.8, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.43it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:15<00:00,  1.33it/s]
Average time taken for each sample: 0.7489342355728149, Average time taken for contemplation: 0.01979517936706543
Evaluation results: {'numerical_accuracy': 0.45, 'close_match_rate': 0.48, 'mean_relative_error': np.float64(0.4570014654219723), 'median_relative_error': np.float64(0.02795248078266946)}
[18/45] Running with max_contemp_tokens=2, eval_temp=0.9, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:17<00:00,  1.29it/s]
Average time taken for each sample: 0.769687077999115, Average time taken for contemplation: 0.027142093181610108
Evaluation results: {'numerical_accuracy': 0.47, 'close_match_rate': 0.49, 'mean_relative_error': np.float64(0.4776513865029417), 'median_relative_error': np.float64(0.014722169561877014)}
[19/45] Running with max_contemp_tokens=3, eval_temp=0.1, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.34it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:16<00:00,  1.31it/s]
Average time taken for each sample: 0.762331862449646, Average time taken for contemplation: 0.019941182136535646
Evaluation results: {'numerical_accuracy': 0.42, 'close_match_rate': 0.45, 'mean_relative_error': np.float64(0.6537527969625532), 'median_relative_error': np.float64(0.07324561403508772)}
[20/45] Running with max_contemp_tokens=3, eval_temp=0.2, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.43it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:15<00:00,  1.33it/s]
Average time taken for each sample: 0.7480683612823487, Average time taken for contemplation: 0.019921507835388184
Evaluation results: {'numerical_accuracy': 0.42, 'close_match_rate': 0.45, 'mean_relative_error': np.float64(0.6528126260223823), 'median_relative_error': np.float64(0.07324561403508772)}
[21/45] Running with max_contemp_tokens=3, eval_temp=0.3, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:15<00:00,  1.33it/s]
Average time taken for each sample: 0.7485843467712402, Average time taken for contemplation: 0.019782862663269042
Evaluation results: {'numerical_accuracy': 0.43, 'close_match_rate': 0.46, 'mean_relative_error': np.float64(0.5652987710987263), 'median_relative_error': np.float64(0.060990712074303406)}
[22/45] Running with max_contemp_tokens=3, eval_temp=0.4, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:16<00:00,  1.31it/s]
Average time taken for each sample: 0.7615448379516602, Average time taken for contemplation: 0.02754600524902344
Evaluation results: {'numerical_accuracy': 0.43, 'close_match_rate': 0.46, 'mean_relative_error': np.float64(0.4944686480015308), 'median_relative_error': np.float64(0.060990712074303406)}
[23/45] Running with max_contemp_tokens=3, eval_temp=0.5, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:17<00:00,  1.29it/s]
Average time taken for each sample: 0.7705133438110352, Average time taken for contemplation: 0.02789883852005005
Evaluation results: {'numerical_accuracy': 0.43, 'close_match_rate': 0.46, 'mean_relative_error': np.float64(0.7145745387966954), 'median_relative_error': np.float64(0.07352941176470588)}
[24/45] Running with max_contemp_tokens=3, eval_temp=0.6, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:17<00:00,  1.29it/s]
Average time taken for each sample: 0.7730192351341247, Average time taken for contemplation: 0.030831890106201174
Evaluation results: {'numerical_accuracy': 0.44, 'close_match_rate': 0.47, 'mean_relative_error': np.float64(0.4760997428946087), 'median_relative_error': np.float64(0.04793028322440087)}
[25/45] Running with max_contemp_tokens=3, eval_temp=0.7, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:15<00:00,  1.32it/s]
Average time taken for each sample: 0.7538184666633606, Average time taken for contemplation: 0.019781143665313722
Evaluation results: {'numerical_accuracy': 0.46, 'close_match_rate': 0.49, 'mean_relative_error': np.float64(0.45326240670486256), 'median_relative_error': np.float64(0.014722169561877014)}
[26/45] Running with max_contemp_tokens=3, eval_temp=0.8, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:15<00:00,  1.32it/s]
Average time taken for each sample: 0.7514687657356263, Average time taken for contemplation: 0.01956632614135742
Evaluation results: {'numerical_accuracy': 0.45, 'close_match_rate': 0.48, 'mean_relative_error': np.float64(0.4570014654219723), 'median_relative_error': np.float64(0.02795248078266946)}
[27/45] Running with max_contemp_tokens=3, eval_temp=0.9, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:17<00:00,  1.29it/s]
Average time taken for each sample: 0.7713335180282592, Average time taken for contemplation: 0.020238544940948486
Evaluation results: {'numerical_accuracy': 0.47, 'close_match_rate': 0.49, 'mean_relative_error': np.float64(0.4776513865029417), 'median_relative_error': np.float64(0.014722169561877014)}
[28/45] Running with max_contemp_tokens=4, eval_temp=0.1, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉           | 93/100 [01:10<00:05,  1.23it/s]
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:15<00:00,  1.33it/s]
Average time taken for each sample: 0.7500523781776428, Average time taken for contemplation: 0.019485809803009034
Evaluation results: {'numerical_accuracy': 0.42, 'close_match_rate': 0.45, 'mean_relative_error': np.float64(0.6537527969625532), 'median_relative_error': np.float64(0.07324561403508772)}
[29/45] Running with max_contemp_tokens=4, eval_temp=0.2, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:14<00:00,  1.34it/s]
Average time taken for each sample: 0.7434488940238952, Average time taken for contemplation: 0.019671573638916015
Evaluation results: {'numerical_accuracy': 0.42, 'close_match_rate': 0.45, 'mean_relative_error': np.float64(0.6528126260223823), 'median_relative_error': np.float64(0.07324561403508772)}
[30/45] Running with max_contemp_tokens=4, eval_temp=0.3, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:15<00:00,  1.33it/s]
Average time taken for each sample: 0.7495186686515808, Average time taken for contemplation: 0.020006656646728516
Evaluation results: {'numerical_accuracy': 0.43, 'close_match_rate': 0.46, 'mean_relative_error': np.float64(0.5652987710987263), 'median_relative_error': np.float64(0.060990712074303406)}
[31/45] Running with max_contemp_tokens=4, eval_temp=0.4, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:16<00:00,  1.31it/s]
Average time taken for each sample: 0.7624064683914185, Average time taken for contemplation: 0.019764294624328615
Evaluation results: {'numerical_accuracy': 0.43, 'close_match_rate': 0.46, 'mean_relative_error': np.float64(0.4944686480015308), 'median_relative_error': np.float64(0.060990712074303406)}
[32/45] Running with max_contemp_tokens=4, eval_temp=0.5, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:16<00:00,  1.31it/s]
Average time taken for each sample: 0.7629517722129822, Average time taken for contemplation: 0.020005152225494385
Evaluation results: {'numerical_accuracy': 0.43, 'close_match_rate': 0.46, 'mean_relative_error': np.float64(0.7145745387966954), 'median_relative_error': np.float64(0.07352941176470588)}
[33/45] Running with max_contemp_tokens=4, eval_temp=0.6, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:17<00:00,  1.29it/s]
Average time taken for each sample: 0.7723683619499206, Average time taken for contemplation: 0.019617085456848146
Evaluation results: {'numerical_accuracy': 0.44, 'close_match_rate': 0.47, 'mean_relative_error': np.float64(0.4760997428946087), 'median_relative_error': np.float64(0.04793028322440087)}
[34/45] Running with max_contemp_tokens=4, eval_temp=0.7, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:15<00:00,  1.32it/s]
Average time taken for each sample: 0.7517770910263062, Average time taken for contemplation: 0.02696176052093506
Evaluation results: {'numerical_accuracy': 0.46, 'close_match_rate': 0.49, 'mean_relative_error': np.float64(0.45326240670486256), 'median_relative_error': np.float64(0.014722169561877014)}
[35/45] Running with max_contemp_tokens=4, eval_temp=0.8, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:15<00:00,  1.32it/s]
Average time taken for each sample: 0.7542448973655701, Average time taken for contemplation: 0.027233662605285643
Evaluation results: {'numerical_accuracy': 0.45, 'close_match_rate': 0.48, 'mean_relative_error': np.float64(0.4570014654219723), 'median_relative_error': np.float64(0.02795248078266946)}
[36/45] Running with max_contemp_tokens=4, eval_temp=0.9, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:17<00:00,  1.30it/s]
Average time taken for each sample: 0.7682880520820617, Average time taken for contemplation: 0.020305135250091554
Evaluation results: {'numerical_accuracy': 0.47, 'close_match_rate': 0.49, 'mean_relative_error': np.float64(0.4776513865029417), 'median_relative_error': np.float64(0.014722169561877014)}
[37/45] Running with max_contemp_tokens=5, eval_temp=0.1, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:16<00:00,  1.31it/s]
Average time taken for each sample: 0.7600142931938172, Average time taken for contemplation: 0.019466946125030516
Evaluation results: {'numerical_accuracy': 0.42, 'close_match_rate': 0.45, 'mean_relative_error': np.float64(0.6537527969625532), 'median_relative_error': np.float64(0.07324561403508772)}
[38/45] Running with max_contemp_tokens=5, eval_temp=0.2, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:16<00:00,  1.31it/s]
Average time taken for each sample: 0.7594492506980896, Average time taken for contemplation: 0.0196488356590271
Evaluation results: {'numerical_accuracy': 0.42, 'close_match_rate': 0.45, 'mean_relative_error': np.float64(0.6528126260223823), 'median_relative_error': np.float64(0.07324561403508772)}
[39/45] Running with max_contemp_tokens=5, eval_temp=0.3, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:15<00:00,  1.32it/s]
Average time taken for each sample: 0.7532699036598206, Average time taken for contemplation: 0.020772511959075927
Evaluation results: {'numerical_accuracy': 0.43, 'close_match_rate': 0.46, 'mean_relative_error': np.float64(0.5652987710987263), 'median_relative_error': np.float64(0.060990712074303406)}
[40/45] Running with max_contemp_tokens=5, eval_temp=0.4, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:16<00:00,  1.31it/s]
Average time taken for each sample: 0.7621551156044006, Average time taken for contemplation: 0.01968179702758789
Evaluation results: {'numerical_accuracy': 0.43, 'close_match_rate': 0.46, 'mean_relative_error': np.float64(0.4944686480015308), 'median_relative_error': np.float64(0.060990712074303406)}
[41/45] Running with max_contemp_tokens=5, eval_temp=0.5, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:18<00:00,  1.27it/s]
Average time taken for each sample: 0.7820719051361084, Average time taken for contemplation: 0.02056593656539917
Evaluation results: {'numerical_accuracy': 0.43, 'close_match_rate': 0.46, 'mean_relative_error': np.float64(0.7145745387966954), 'median_relative_error': np.float64(0.07352941176470588)}
[42/45] Running with max_contemp_tokens=5, eval_temp=0.6, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:16<00:00,  1.31it/s]
Average time taken for each sample: 0.7572700643539428, Average time taken for contemplation: 0.020187768936157226
Evaluation results: {'numerical_accuracy': 0.44, 'close_match_rate': 0.47, 'mean_relative_error': np.float64(0.4760997428946087), 'median_relative_error': np.float64(0.04793028322440087)}
[43/45] Running with max_contemp_tokens=5, eval_temp=0.7, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:15<00:00,  1.33it/s]
Average time taken for each sample: 0.747286810874939, Average time taken for contemplation: 0.020228378772735596
Evaluation results: {'numerical_accuracy': 0.46, 'close_match_rate': 0.49, 'mean_relative_error': np.float64(0.45326240670486256), 'median_relative_error': np.float64(0.014722169561877014)}
[44/45] Running with max_contemp_tokens=5, eval_temp=0.8, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:14<00:00,  1.34it/s]
Average time taken for each sample: 0.7418958902359009, Average time taken for contemplation: 0.02067842483520508
Evaluation results: {'numerical_accuracy': 0.45, 'close_match_rate': 0.48, 'mean_relative_error': np.float64(0.4570014654219723), 'median_relative_error': np.float64(0.02795248078266946)}
[45/45] Running with max_contemp_tokens=5, eval_temp=0.9, dataset=svamp, config=small, config=mistral
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                                                                                                                       | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reli
able results.
Running inference: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:18<00:00,  1.27it/s]
Average time taken for each sample: 0.7842970490455627, Average time taken for contemplation: 0.019596397876739502
Evaluation results: {'numerical_accuracy': 0.47, 'close_match_rate': 0.49, 'mean_relative_error': np.float64(0.4776513865029417), 'median_relative_error': np.float64(0.014722169561877014)}
All evaluation runs completed!
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ tmux capture-pane -p -S - > no_cot_mistral_svamp.txt

