
Starting evaluation runs with 45 combinations...
[1/45] Running with max_contemp_tokens=1, eval_temp=0.1
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:23<00:00,  1.19it/s]
Average time taken for each sample: 0.8296963024139404, Average time taken for contemplation: 0.019457228183746338
Evaluation results: {'numerical_accuracy': 0.08, 'close_match_rate': 0.08, 'mean_relative_error': np.float64(0.7159380446332568), 'median_relativ
e_error': np.float64(0.4298913043478261)}
[2/45] Running with max_contemp_tokens=1, eval_temp=0.2
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:24<00:00,  1.18it/s]
Average time taken for each sample: 0.839068284034729, Average time taken for contemplation: 0.03032745361328125
Evaluation results: {'numerical_accuracy': 0.06, 'close_match_rate': 0.06, 'mean_relative_error': np.float64(1.0103535353535354e+23), 'median_rel
ative_error': np.float64(0.4)}
[3/45] Running with max_contemp_tokens=1, eval_temp=0.3
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:23<00:00,  1.19it/s]
Average time taken for each sample: 0.8319929099082947, Average time taken for contemplation: 0.019942245483398437
Evaluation results: {'numerical_accuracy': 0.09, 'close_match_rate': 0.09, 'mean_relative_error': np.float64(2.2720429224494266e+25), 'median_rel
ative_error': np.float64(0.3333333333333333)}
[4/45] Running with max_contemp_tokens=1, eval_temp=0.4
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:25<00:00,  1.17it/s]
Average time taken for each sample: 0.8454724740982056, Average time taken for contemplation: 0.028980669975280763
Evaluation results: {'numerical_accuracy': 0.11, 'close_match_rate': 0.11, 'mean_relative_error': np.float64(1.2115000179777543e+25), 'median_rel
ative_error': np.float64(0.37003968253968256)}
[5/45] Running with max_contemp_tokens=1, eval_temp=0.5
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:25<00:00,  1.17it/s]
Average time taken for each sample: 0.8492119574546814, Average time taken for contemplation: 0.030872979164123536
Evaluation results: {'numerical_accuracy': 0.09, 'close_match_rate': 0.09, 'mean_relative_error': np.float64(6.264049029982363e+25), 'median_rela
tive_error': np.float64(0.4)}
[6/45] Running with max_contemp_tokens=1, eval_temp=0.6
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.41it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:24<00:00,  1.18it/s]
Average time taken for each sample: 0.8410246348381043, Average time taken for contemplation: 0.02604766368865967
Evaluation results: {'numerical_accuracy': 0.09, 'close_match_rate': 0.09, 'mean_relative_error': np.float64(1.7746915379908578e+24), 'median_rel
ative_error': np.float64(0.4)}
[7/45] Running with max_contemp_tokens=1, eval_temp=0.7
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.39it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:24<00:00,  1.18it/s]
Average time taken for each sample: 0.8409321904182434, Average time taken for contemplation: 0.030173587799072265
Evaluation results: {'numerical_accuracy': 0.16, 'close_match_rate': 0.16, 'mean_relative_error': np.float64(1.8968621399176958e+19), 'median_rel
ative_error': np.float64(0.38152985074626866)}
[8/45] Running with max_contemp_tokens=1, eval_temp=0.8
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:23<00:00,  1.20it/s]
Average time taken for each sample: 0.8256060004234314, Average time taken for contemplation: 0.020634543895721436
Evaluation results: {'numerical_accuracy': 0.1, 'close_match_rate': 0.1, 'mean_relative_error': np.float64(1.095713356234028e+26), 'median_relati
ve_error': np.float64(0.37003968253968256)}
[9/45] Running with max_contemp_tokens=1, eval_temp=0.9
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:24<00:00,  1.18it/s]
Average time taken for each sample: 0.8392538595199585, Average time taken for contemplation: 0.029964346885681153
Evaluation results: {'numerical_accuracy': 0.08, 'close_match_rate': 0.08, 'mean_relative_error': np.float64(1.0993119543061832e+26), 'median_rel
ative_error': np.float64(0.375)}
[10/45] Running with max_contemp_tokens=2, eval_temp=0.1
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:29<00:00,  1.11it/s]
Average time taken for each sample: 0.8906188607215881, Average time taken for contemplation: 0.042934017181396486
Evaluation results: {'numerical_accuracy': 0.02, 'close_match_rate': 0.02, 'mean_relative_error': np.float64(4.5202315466426086e+27), 'median_rel
ative_error': np.float64(0.9788763066202091)}
[11/45] Running with max_contemp_tokens=2, eval_temp=0.2
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:27<00:00,  1.14it/s]
Average time taken for each sample: 0.8684141540527344, Average time taken for contemplation: 0.02316554069519043
Evaluation results: {'numerical_accuracy': 0.01, 'close_match_rate': 0.01, 'mean_relative_error': np.float64(4.5202315466426086e+27), 'median_rel
ative_error': np.float64(0.9319444444444445)}
[12/45] Running with max_contemp_tokens=2, eval_temp=0.3
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.10it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:27<00:00,  1.15it/s]
Average time taken for each sample: 0.8651813101768494, Average time taken for contemplation: 0.020687241554260254
Evaluation results: {'numerical_accuracy': 0.03, 'close_match_rate': 0.03, 'mean_relative_error': np.float64(4.5100347212228665e+27), 'median_rel
ative_error': np.float64(0.9530826558265583)}
[13/45] Running with max_contemp_tokens=2, eval_temp=0.4
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:29<00:00,  1.12it/s]
Average time taken for each sample: 0.889160521030426, Average time taken for contemplation: 0.0430707049369812
Evaluation results: {'numerical_accuracy': 0.03, 'close_match_rate': 0.03, 'mean_relative_error': np.float64(6.892361111121179e+25), 'median_rela
tive_error': np.float64(0.8898989898989899)}
[14/45] Running with max_contemp_tokens=2, eval_temp=0.5
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.42it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:27<00:00,  1.15it/s]
Average time taken for each sample: 0.8647405672073364, Average time taken for contemplation: 0.02005600929260254
Evaluation results: {'numerical_accuracy': 0.06, 'close_match_rate': 0.06, 'mean_relative_error': np.float64(7558111.860477585), 'median_relative
_error': np.float64(0.8307291666666667)}
[15/45] Running with max_contemp_tokens=2, eval_temp=0.6
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:27<00:00,  1.15it/s]
Average time taken for each sample: 0.865353639125824, Average time taken for contemplation: 0.019489648342132567
Evaluation results: {'numerical_accuracy': 0.07, 'close_match_rate': 0.07, 'mean_relative_error': np.float64(1.0970685185193018e+26), 'median_rel
ative_error': np.float64(0.8556547619047619)}
[16/45] Running with max_contemp_tokens=2, eval_temp=0.7
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.20it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:27<00:00,  1.15it/s]
Average time taken for each sample: 0.8644020223617553, Average time taken for contemplation: 0.019710044860839843
Evaluation results: {'numerical_accuracy': 0.07, 'close_match_rate': 0.07, 'mean_relative_error': np.float64(6.215032738563945e+25), 'median_rela
tive_error': np.float64(0.8588039867109634)}
[17/45] Running with max_contemp_tokens=2, eval_temp=0.8
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.22it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:27<00:00,  1.15it/s]
Average time taken for each sample: 0.8654996204376221, Average time taken for contemplation: 0.020338003635406495
Evaluation results: {'numerical_accuracy': 0.06, 'close_match_rate': 0.06, 'mean_relative_error': np.float64(1.2625308824196514e+27), 'median_rel
ative_error': np.float64(0.8660714285714286)}
[18/45] Running with max_contemp_tokens=2, eval_temp=0.9
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:28<00:00,  1.13it/s]
Average time taken for each sample: 0.8765544605255127, Average time taken for contemplation: 0.030951454639434814
Evaluation results: {'numerical_accuracy': 0.04, 'close_match_rate': 0.04, 'mean_relative_error': np.float64(901473713.1308862), 'median_relative
_error': np.float64(0.8819444444444444)}
[19/45] Running with max_contemp_tokens=3, eval_temp=0.1
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:29<00:00,  1.11it/s]
Average time taken for each sample: 0.8916132855415344, Average time taken for contemplation: 0.020169146060943603
Evaluation results: {'numerical_accuracy': 0.0, 'close_match_rate': 0.0, 'mean_relative_error': np.float64(2.1557152121251647e+29), 'median_relat
ive_error': np.float64(0.9166666666666666)}
[20/45] Running with max_contemp_tokens=3, eval_temp=0.2
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:30<00:00,  1.10it/s]
Average time taken for each sample: 0.9011459875106812, Average time taken for contemplation: 0.028362677097320557
Evaluation results: {'numerical_accuracy': 0.01, 'close_match_rate': 0.01, 'mean_relative_error': np.float64(3.603542310056853e+29), 'median_rela
tive_error': np.float64(0.9166666666666666)}
[21/45] Running with max_contemp_tokens=3, eval_temp=0.3
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference:  44%|██████████████████████████████████████▋                         Running inference:  45%|█████████████████████████████████
██████▌                        Running inference:  46%|████████████████████████████████████████▍                       Running inference:  47%|██
███████████████████████████████████████▎                      Running inference:  48%|██████████████████████████████████████████▏
     Running inference:  49%|███████████████████████████████████████████                     Running inference:  50%|████████████████████████████
████████████████                    Running inference:  51%|████████████████████████████████████████████▉                   Running inference:  5
2%|█████████████████████████████████████████████▊                  Running inference:  53%|██████████████████████████████████████████████▋
          Running inference:  54%|███████████████████████████████████████████████▌                Running inference:  55%|███████████████████████
█████████████████████████▍               Running inference:  56%|█████████████████████████████████████████████████▎              Running inferenc
e:  57%|██████████████████████████████████████████████████▏             Running inference:  58%|█████████████████████████████████████████████████
██             Running inference:  59%|███████████████████████████████████████████████████▉            Running inference:  60%|██████████████████
██████████████████████████████████▊           Running inference:  61%|█████████████████████████████████████████████████████▋          Running inf
erence:  62%|██████████████████████████████████████████████████████▌         Running inference:  63%|████████████████████████████████████████████
███████████▍        Running inference:  64%|████████████████████████████████████████████████████████▎       Running inference:  65%|█████████████
████████████████████████████████████████████▏      Running inference:  66%|██████████████████████████████████████████████████████████      Runnin
g inference:  67%|██████████████████████████████████████████████████████████▉     Running inference:  68%|███████████████████████████████████████
████████████████████▊    Running inference:  69%|████████████████████████████████████████████████████████████▋   Running inference:  70%|████████
█████████████████████████████████████████████████████▌  Running inference:  71%|██████████████████████████████████████████████████████████████▍ R
unning inference:  72%|███████████████████████████████████████████████████████████████▎Running inference:  73%|██████████████████████████████████
██████████████████████████████Running inference:  74%|████████████████████████████████████████████████████████████████Running inference:  75%|███
█████████████████████████████████████████████████████████████Running inference:  76%|████████████████████████████████████████████████████████████
████Running inference:  77%|████████████████████████████████████████████████████████████████Running inference:  78%|█████████████████████████████
███████████████████████████████████Running inference:  79%|████████████████████████████████████████████████████████████████Running inference:  80
%|████████████████████████████████████████████████████████████████Running inference:  81%|███████████████████████████████████████████████████████
█████████Running inference:  82%|████████████████████████████████████████████████████████████████Running inference:  83%|████████████████████████
████████████████████████████████████████Running inference:  84%|████████████████████████████████████████████████████████████████Running inference
:  85%|████████████████████████████████████████████████████████████████Running inference:  86%|██████████████████████████████████████████████████
██████████████Running inference:  87%|████████████████████████████████████████████████████████████████Running inference:  88%|███████████████████
█████████████████████████████████████████████Running inference:  89%|████████████████████████████████████████████████████████████████Running infe
rence:  90%|████████████████████████████████████████████████████████████████Running inference:  91%|█████████████████████████████████████████████
███████████████████Running inference:  92%|████████████████████████████████████████████████████████████████Running inference:  93%|██████████████
██████████████████████████████████████████████████Running inference:  94%|████████████████████████████████████████████████████████████████Running
 inference:  95%|████████████████████████████████████████████████████████████████Running inference:  96%|████████████████████████████████████████
████████████████████████Running inference:  97%|████████████████████████████████████████████████████████████████Running inference:  98%|█████████
███████████████████████████████████████████████████████Running inference:  99%|████████████████████████████████████████████████████████████████Ru
nning inference: 100%|████████████████████████████████████████████████████████████████Running inference: 100%|███████████████████████████████████
████████████████████████████████████████████████████| 100/100 [01:30<00:00,  1.10it/s]
Average time taken for each sample: 0.9021233177185058, Average time taken for contemplation: 0.03224907159805298
Evaluation results: {'numerical_accuracy': 0.01, 'close_match_rate': 0.01, 'mean_relative_error': np.float64(1.67314726078848e+29), 'median_relat
ive_error': np.float64(0.9166666666666666)}
[22/45] Running with max_contemp_tokens=3, eval_temp=0.4
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.50it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:29<00:00,  1.11it/s]
Average time taken for each sample: 0.8892963099479675, Average time taken for contemplation: 0.019683284759521483
Evaluation results: {'numerical_accuracy': 0.01, 'close_match_rate': 0.01, 'mean_relative_error': np.float64(1.7026151000228307e+29), 'median_rel
ative_error': np.float64(0.8708333333333333)}
[23/45] Running with max_contemp_tokens=3, eval_temp=0.5
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:29<00:00,  1.12it/s]
Average time taken for each sample: 0.8889831113815307, Average time taken for contemplation: 0.019592597484588622
Evaluation results: {'numerical_accuracy': 0.01, 'close_match_rate': 0.01, 'mean_relative_error': np.float64(2.216220653347897e+29), 'median_rela
tive_error': np.float64(0.8535714285714285)}
[24/45] Running with max_contemp_tokens=3, eval_temp=0.6
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:29<00:00,  1.12it/s]
Average time taken for each sample: 0.8889339756965637, Average time taken for contemplation: 0.019630603790283203
Evaluation results: {'numerical_accuracy': 0.01, 'close_match_rate': 0.01, 'mean_relative_error': np.float64(2.3975162266055886e+29), 'median_rel
ative_error': np.float64(0.8191056910569106)}
[25/45] Running with max_contemp_tokens=3, eval_temp=0.7
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:29<00:00,  1.11it/s]
Average time taken for each sample: 0.8905645608901978, Average time taken for contemplation: 0.01992342472076416
Evaluation results: {'numerical_accuracy': 0.0, 'close_match_rate': 0.0, 'mean_relative_error': np.float64(5.144337438270048e+29), 'median_relati
ve_error': np.float64(0.8786764705882353)}
[26/45] Running with max_contemp_tokens=3, eval_temp=0.8
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:29<00:00,  1.11it/s]
Average time taken for each sample: 0.8901009845733643, Average time taken for contemplation: 0.02012627840042114
Evaluation results: {'numerical_accuracy': 0.0, 'close_match_rate': 0.0, 'mean_relative_error': np.float64(5.99511473556926e+29), 'median_relativ
e_error': np.float64(0.8856209150326797)}
[27/45] Running with max_contemp_tokens=3, eval_temp=0.9
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:31<00:00,  1.09it/s]
Average time taken for each sample: 0.9089910221099854, Average time taken for contemplation: 0.020739524364471434
Evaluation results: {'numerical_accuracy': 0.0, 'close_match_rate': 0.0, 'mean_relative_error': np.float64(6.508952444877772e+29), 'median_relati
ve_error': np.float64(0.9194444444444445)}
[28/45] Running with max_contemp_tokens=4, eval_temp=0.1
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:32<00:00,  1.09it/s]
Average time taken for each sample: 0.9127111721038819, Average time taken for contemplation: 0.019487214088439942
Evaluation results: {'numerical_accuracy': 0.06, 'close_match_rate': 0.06, 'mean_relative_error': np.float64(5.82727138307816e+29), 'median_relat
ive_error': np.float64(0.9106312292358804)}
[29/45] Running with max_contemp_tokens=4, eval_temp=0.2
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:32<00:00,  1.09it/s]
Average time taken for each sample: 0.9130604767799377, Average time taken for contemplation: 0.01972464084625244
Evaluation results: {'numerical_accuracy': 0.06, 'close_match_rate': 0.06, 'mean_relative_error': np.float64(5.827976476300382e+29), 'median_rela
tive_error': np.float64(0.9)}
[30/45] Running with max_contemp_tokens=4, eval_temp=0.3
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:32<00:00,  1.09it/s]
Average time taken for each sample: 0.9121639347076416, Average time taken for contemplation: 0.019621055126190185
Evaluation results: {'numerical_accuracy': 0.06, 'close_match_rate': 0.06, 'mean_relative_error': np.float64(1.9195265365179494e+30), 'median_rel
ative_error': np.float64(0.9054114490161002)}
[31/45] Running with max_contemp_tokens=4, eval_temp=0.4
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:32<00:00,  1.09it/s]
Average time taken for each sample: 0.9130129456520081, Average time taken for contemplation: 0.01987168550491333
Evaluation results: {'numerical_accuracy': 0.03, 'close_match_rate': 0.03, 'mean_relative_error': np.float64(1.9155270657405386e+30), 'median_rel
ative_error': np.float64(0.9054114490161002)}
[32/45] Running with max_contemp_tokens=4, eval_temp=0.5
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:32<00:00,  1.08it/s]
Average time taken for each sample: 0.9141566634178162, Average time taken for contemplation: 0.019891347885131836
Evaluation results: {'numerical_accuracy': 0.03, 'close_match_rate': 0.03, 'mean_relative_error': np.float64(1.732311275986351e+30), 'median_rela
tive_error': np.float64(0.911437908496732)}
[33/45] Running with max_contemp_tokens=4, eval_temp=0.6
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:32<00:00,  1.08it/s]
Average time taken for each sample: 0.9163477182388305, Average time taken for contemplation: 0.020603659152984618
Evaluation results: {'numerical_accuracy': 0.02, 'close_match_rate': 0.02, 'mean_relative_error': np.float64(3.267378380031469e+30), 'median_rela
tive_error': np.float64(0.8980850373255437)}
[34/45] Running with max_contemp_tokens=4, eval_temp=0.7
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.42it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:32<00:00,  1.08it/s]
Average time taken for each sample: 0.917126362323761, Average time taken for contemplation: 0.019607903957366942
Evaluation results: {'numerical_accuracy': 0.02, 'close_match_rate': 0.02, 'mean_relative_error': np.float64(1.0562157186720535e+30), 'median_rel
ative_error': np.float64(0.8917410714285714)}
[35/45] Running with max_contemp_tokens=4, eval_temp=0.8
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.36it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:32<00:00,  1.08it/s]
Average time taken for each sample: 0.9142929220199585, Average time taken for contemplation: 0.019967963695526125
Evaluation results: {'numerical_accuracy': 0.03, 'close_match_rate': 0.03, 'mean_relative_error': np.float64(3.008561392554968e+30), 'median_rela
tive_error': np.float64(0.8878406708595388)}
[36/45] Running with max_contemp_tokens=4, eval_temp=0.9
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:32<00:00,  1.08it/s]
Average time taken for each sample: 0.917183964252472, Average time taken for contemplation: 0.01997572898864746
Evaluation results: {'numerical_accuracy': 0.03, 'close_match_rate': 0.03, 'mean_relative_error': np.float64(4.583857899976477e+30), 'median_rela
tive_error': np.float64(0.8862533692722372)}
[37/45] Running with max_contemp_tokens=5, eval_temp=0.1
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:34<00:00,  1.06it/s]
Average time taken for each sample: 0.938821930885315, Average time taken for contemplation: 0.01978581190109253
Evaluation results: {'numerical_accuracy': 0.13, 'close_match_rate': 0.13, 'mean_relative_error': np.float64(1481481481482.4956), 'median_relativ
e_error': np.float64(0.5714285714285714)}
[38/45] Running with max_contemp_tokens=5, eval_temp=0.2
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:34<00:00,  1.06it/s]
Average time taken for each sample: 0.9382364869117736, Average time taken for contemplation: 0.020750954151153564
Evaluation results: {'numerical_accuracy': 0.11, 'close_match_rate': 0.11, 'mean_relative_error': np.float64(0.5685126041445023), 'median_relativ
e_error': np.float64(0.5555555555555556)}
[39/45] Running with max_contemp_tokens=5, eval_temp=0.3
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:34<00:00,  1.06it/s]
Average time taken for each sample: 0.9371638703346252, Average time taken for contemplation: 0.020002920627593995
Evaluation results: {'numerical_accuracy': 0.14, 'close_match_rate': 0.14, 'mean_relative_error': np.float64(14814881481482.4), 'median_relative_
error': np.float64(0.5)}
[40/45] Running with max_contemp_tokens=5, eval_temp=0.4
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:34<00:00,  1.06it/s]
Average time taken for each sample: 0.937928376197815, Average time taken for contemplation: 0.019873883724212647
Evaluation results: {'numerical_accuracy': 0.12, 'close_match_rate': 0.12, 'mean_relative_error': np.float64(1.085854990381744), 'median_relative
_error': np.float64(0.5)}
[41/45] Running with max_contemp_tokens=5, eval_temp=0.5
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.46it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:34<00:00,  1.06it/s]
Average time taken for each sample: 0.9364743328094483, Average time taken for contemplation: 0.01985661506652832
Evaluation results: {'numerical_accuracy': 0.13, 'close_match_rate': 0.13, 'mean_relative_error': np.float64(0.5186106145626678), 'median_relativ
e_error': np.float64(0.4392857142857143)}
[42/45] Running with max_contemp_tokens=5, eval_temp=0.6
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.43it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:34<00:00,  1.06it/s]
Average time taken for each sample: 0.9370740246772766, Average time taken for contemplation: 0.019610545635223388
Evaluation results: {'numerical_accuracy': 0.12, 'close_match_rate': 0.12, 'mean_relative_error': np.float64(15.345054455139561), 'median_relativ
e_error': np.float64(0.3333333333333333)}
[43/45] Running with max_contemp_tokens=5, eval_temp=0.7
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:34<00:00,  1.06it/s]
Average time taken for each sample: 0.9384540295600892, Average time taken for contemplation: 0.019681339263916017
Evaluation results: {'numerical_accuracy': 0.15, 'close_match_rate': 0.15, 'mean_relative_error': np.float64(15.640135364485024), 'median_relativ
e_error': np.float64(0.3611111111111111)}
[44/45] Running with max_contemp_tokens=5, eval_temp=0.8
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:34<00:00,  1.06it/s]
Average time taken for each sample: 0.9372717118263245, Average time taken for contemplation: 0.019767706394195558
Evaluation results: {'numerical_accuracy': 0.13, 'close_match_rate': 0.13, 'mean_relative_error': np.float64(0.4520015492234944), 'median_relativ
e_error': np.float64(0.3115079365079365)}
[45/45] Running with max_contemp_tokens=5, eval_temp=0.9
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.50it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:34<00:00,  1.06it/s]
Average time taken for each sample: 0.9373657441139222, Average time taken for contemplation: 0.020177042484283446
Evaluation results: {'numerical_accuracy': 0.1, 'close_match_rate': 0.1, 'mean_relative_error': np.float64(16.557543858958343), 'median_relative_
error': np.float64(0.35416666666666663)}
[46/45] Running with max_contemp_tokens=1, eval_temp=0.1
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.50it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:24<00:00,  1.19it/s]
Average time taken for each sample: 0.836590428352356, Average time taken for contemplation: 0.01939387321472168
Evaluation results: {'numerical_accuracy': 0.21, 'close_match_rate': 0.23, 'mean_relative_error': np.float64(7758.231898619078), 'median_relative
_error': np.float64(0.1598639455782313)}
[47/45] Running with max_contemp_tokens=1, eval_temp=0.2
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:24<00:00,  1.19it/s]
Average time taken for each sample: 0.8369550800323486, Average time taken for contemplation: 0.019859139919281007
Evaluation results: {'numerical_accuracy': 0.24, 'close_match_rate': 0.25, 'mean_relative_error': np.float64(1.0871950538172823), 'median_relativ
e_error': np.float64(0.10526315789473684)}
[48/45] Running with max_contemp_tokens=1, eval_temp=0.3
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:01<00:00,  1.51it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:24<00:00,  1.18it/s]
Average time taken for each sample: 0.837882707118988, Average time taken for contemplation: 0.019802768230438233
Evaluation results: {'numerical_accuracy': 0.27, 'close_match_rate': 0.28, 'mean_relative_error': np.float64(3.272786777941417e+24), 'median_rela
tive_error': np.float64(0.15151515151515152)}
[49/45] Running with max_contemp_tokens=1, eval_temp=0.4
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:24<00:00,  1.18it/s]
Average time taken for each sample: 0.8376837992668151, Average time taken for contemplation: 0.019999165534973145
Evaluation results: {'numerical_accuracy': 0.28, 'close_match_rate': 0.29, 'mean_relative_error': np.float64(3.306878306878307e+24), 'median_rela
tive_error': np.float64(0.11215932914046121)}
[50/45] Running with max_contemp_tokens=1, eval_temp=0.5
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:24<00:00,  1.19it/s]
Average time taken for each sample: 0.8360010862350464, Average time taken for contemplation: 0.019244725704193114
Evaluation results: {'numerical_accuracy': 0.26, 'close_match_rate': 0.27, 'mean_relative_error': np.float64(3.341687552213868e+24), 'median_rela
tive_error': np.float64(0.13008130081300814)}
[51/45] Running with max_contemp_tokens=1, eval_temp=0.6
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:01<00:00,  1.53it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:24<00:00,  1.19it/s]
Average time taken for each sample: 0.8362696218490601, Average time taken for contemplation: 0.019382071495056153
Evaluation results: {'numerical_accuracy': 0.24, 'close_match_rate': 0.25, 'mean_relative_error': np.float64(3.272786777941417e+24), 'median_rela
tive_error': np.float64(0.17894736842105263)}
[52/45] Running with max_contemp_tokens=1, eval_temp=0.7
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:24<00:00,  1.18it/s]
Average time taken for each sample: 0.8405707311630249, Average time taken for contemplation: 0.02001977205276489
Evaluation results: {'numerical_accuracy': 0.27, 'close_match_rate': 0.28, 'mean_relative_error': np.float64(4.2187122577861715e+24), 'median_rel
ative_error': np.float64(0.1111111111111111)}
[53/45] Running with max_contemp_tokens=1, eval_temp=0.8
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:24<00:00,  1.18it/s]
Average time taken for each sample: 0.840841703414917, Average time taken for contemplation: 0.020222678184509277
Evaluation results: {'numerical_accuracy': 0.24, 'close_match_rate': 0.25, 'mean_relative_error': np.float64(4.218712260550807e+24), 'median_rela
tive_error': np.float64(0.18181818181818182)}
[54/45] Running with max_contemp_tokens=1, eval_temp=0.9
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:24<00:00,  1.18it/s]
Average time taken for each sample: 0.8376730442047119, Average time taken for contemplation: 0.019775543212890625
Evaluation results: {'numerical_accuracy': 0.22, 'close_match_rate': 0.23, 'mean_relative_error': np.float64(3.2393909944930353e+24), 'median_rel
ative_error': np.float64(0.18274582560296848)}
[55/45] Running with max_contemp_tokens=2, eval_temp=0.1
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:26<00:00,  1.15it/s]
Average time taken for each sample: 0.8623983788490296, Average time taken for contemplation: 0.019755394458770753
Evaluation results: {'numerical_accuracy': 0.35, 'close_match_rate': 0.38, 'mean_relative_error': np.float64(1.0111110169350179e+18), 'median_rel
ative_error': np.float64(0.1111111111111111)}
[56/45] Running with max_contemp_tokens=2, eval_temp=0.2
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:26<00:00,  1.15it/s]
Average time taken for each sample: 0.861526403427124, Average time taken for contemplation: 0.019608690738677978
Evaluation results: {'numerical_accuracy': 0.36, 'close_match_rate': 0.39, 'mean_relative_error': np.float64(1.1045685836354839e+18), 'median_rel
ative_error': np.float64(0.11215932914046121)}
[57/45] Running with max_contemp_tokens=2, eval_temp=0.3
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:26<00:00,  1.15it/s]
Average time taken for each sample: 0.8624485969543457, Average time taken for contemplation: 0.020545361042022706
Evaluation results: {'numerical_accuracy': 0.37, 'close_match_rate': 0.4, 'mean_relative_error': np.float64(1.5704631262324147), 'median_relative
_error': np.float64(0.10702614379084967)}
[58/45] Running with max_contemp_tokens=2, eval_temp=0.4
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:26<00:00,  1.15it/s]
Average time taken for each sample: 0.8631338119506836, Average time taken for contemplation: 0.020011742115020752
Evaluation results: {'numerical_accuracy': 0.37, 'close_match_rate': 0.4, 'mean_relative_error': np.float64(1.841482579546862), 'median_relative_
error': np.float64(0.09483726150392817)}
[59/45] Running with max_contemp_tokens=2, eval_temp=0.5
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:01<00:00,  1.51it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:26<00:00,  1.15it/s]
Average time taken for each sample: 0.863367338180542, Average time taken for contemplation: 0.019756579399108888
Evaluation results: {'numerical_accuracy': 0.36, 'close_match_rate': 0.39, 'mean_relative_error': np.float64(1.5713584564743195), 'median_relativ
e_error': np.float64(0.1073737373737374)}
[60/45] Running with max_contemp_tokens=2, eval_temp=0.6
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:01<00:00,  1.53it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:26<00:00,  1.15it/s]
Average time taken for each sample: 0.8608592009544372, Average time taken for contemplation: 0.019478662014007567
Evaluation results: {'numerical_accuracy': 0.35, 'close_match_rate': 0.38, 'mean_relative_error': np.float64(2.971411059357812), 'median_relative
_error': np.float64(0.10657596371882086)}
[61/45] Running with max_contemp_tokens=2, eval_temp=0.7
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:27<00:00,  1.14it/s]
Average time taken for each sample: 0.8706180095672608, Average time taken for contemplation: 0.028239777088165285
Evaluation results: {'numerical_accuracy': 0.36, 'close_match_rate': 0.4, 'mean_relative_error': np.float64(2.819463603448942), 'median_relative_
error': np.float64(0.10702614379084967)}
[62/45] Running with max_contemp_tokens=2, eval_temp=0.8
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.50it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:27<00:00,  1.14it/s]
Average time taken for each sample: 0.8686021161079407, Average time taken for contemplation: 0.025980253219604493
Evaluation results: {'numerical_accuracy': 0.37, 'close_match_rate': 0.41, 'mean_relative_error': np.float64(3.2170676949928367), 'median_relativ
e_error': np.float64(0.10040312421264802)}
[63/45] Running with max_contemp_tokens=2, eval_temp=0.9
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:27<00:00,  1.15it/s]
Average time taken for each sample: 0.8653661298751831, Average time taken for contemplation: 0.019799425601959228
Evaluation results: {'numerical_accuracy': 0.36, 'close_match_rate': 0.4, 'mean_relative_error': np.float64(3.219801480731048), 'median_relative_
error': np.float64(0.10641564318034906)}
[64/45] Running with max_contemp_tokens=3, eval_temp=0.1
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:30<00:00,  1.11it/s]
Average time taken for each sample: 0.8940321040153504, Average time taken for contemplation: 0.026731762886047363
Evaluation results: {'numerical_accuracy': 0.37, 'close_match_rate': 0.38, 'mean_relative_error': np.float64(2097.0880664314573), 'median_relativ
e_error': np.float64(0.115995115995116)}
[65/45] Running with max_contemp_tokens=3, eval_temp=0.2
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.50it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:30<00:00,  1.11it/s]
Average time taken for each sample: 0.8934467577934265, Average time taken for contemplation: 0.026663978099823
Evaluation results: {'numerical_accuracy': 0.37, 'close_match_rate': 0.38, 'mean_relative_error': np.float64(1.4812851038143378e+18), 'median_rel
ative_error': np.float64(0.10702614379084967)}
[66/45] Running with max_contemp_tokens=3, eval_temp=0.3
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:29<00:00,  1.12it/s]
Average time taken for each sample: 0.88694899559021, Average time taken for contemplation: 0.019887757301330567
Evaluation results: {'numerical_accuracy': 0.33, 'close_match_rate': 0.34, 'mean_relative_error': np.float64(4.16558224076694e+17), 'median_relat
ive_error': np.float64(0.1111111111111111)}
[67/45] Running with max_contemp_tokens=3, eval_temp=0.4
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:29<00:00,  1.12it/s]
Average time taken for each sample: 0.8859915328025818, Average time taken for contemplation: 0.019223830699920653
Evaluation results: {'numerical_accuracy': 0.34, 'close_match_rate': 0.35, 'mean_relative_error': np.float64(4.16558224076694e+17), 'median_relat
ive_error': np.float64(0.10702614379084967)}
[68/45] Running with max_contemp_tokens=3, eval_temp=0.5
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:29<00:00,  1.12it/s]
Average time taken for each sample: 0.8871365189552307, Average time taken for contemplation: 0.019968047142028808
Evaluation results: {'numerical_accuracy': 0.34, 'close_match_rate': 0.35, 'mean_relative_error': np.float64(2.0580761361727835), 'median_relativ
e_error': np.float64(0.1111111111111111)}
[69/45] Running with max_contemp_tokens=3, eval_temp=0.6
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:29<00:00,  1.12it/s]
Average time taken for each sample: 0.8869800329208374, Average time taken for contemplation: 0.01985849380493164
Evaluation results: {'numerical_accuracy': 0.32, 'close_match_rate': 0.33, 'mean_relative_error': np.float64(11.46165258312813), 'median_relative
_error': np.float64(0.11915626746135222)}
[70/45] Running with max_contemp_tokens=3, eval_temp=0.7
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.51it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:29<00:00,  1.12it/s]
Average time taken for each sample: 0.8861064100265503, Average time taken for contemplation: 0.019644229412078856
Evaluation results: {'numerical_accuracy': 0.34, 'close_match_rate': 0.35, 'mean_relative_error': np.float64(9.41805988960009e+21), 'median_relat
ive_error': np.float64(0.1111111111111111)}
[71/45] Running with max_contemp_tokens=3, eval_temp=0.8
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.51it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:29<00:00,  1.12it/s]
Average time taken for each sample: 0.8879091691970825, Average time taken for contemplation: 0.020031819343566893
Evaluation results: {'numerical_accuracy': 0.29, 'close_match_rate': 0.3, 'mean_relative_error': np.float64(15.162171071079733), 'median_relative
_error': np.float64(0.13136627967136444)}
[72/45] Running with max_contemp_tokens=3, eval_temp=0.9
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.53it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:30<00:00,  1.11it/s]
Average time taken for each sample: 0.8963245606422424, Average time taken for contemplation: 0.029416687488555908
Evaluation results: {'numerical_accuracy': 0.28, 'close_match_rate': 0.29, 'mean_relative_error': np.float64(1.624606973325685), 'median_relative
_error': np.float64(0.13136627967136444)}
[73/45] Running with max_contemp_tokens=4, eval_temp=0.1
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.44it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:31<00:00,  1.09it/s]
Average time taken for each sample: 0.9117153310775756, Average time taken for contemplation: 0.01939098596572876
Evaluation results: {'numerical_accuracy': 0.27, 'close_match_rate': 0.29, 'mean_relative_error': np.float64(1.0077155414101778e+22), 'median_rel
ative_error': np.float64(0.1111111111111111)}
[74/45] Running with max_contemp_tokens=4, eval_temp=0.2
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                                                                                 | 0/100 [00:00<?, ?it/s]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpec
ted behavior. Please pass your input's `attention_mask` to obtain reliable results.
Running inference: 100%|███████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:31<00:00,  1.09it/s]
Average time taken for each sample: 0.9118772268295288, Average time taken for contemplation: 0.0200567626953125
Evaluation results: {'numerical_accuracy': 0.27, 'close_match_rate': 0.28, 'mean_relative_error': np.float64(4.1690467696235177e+18), 'median_rel
ative_error': np.float64(0.12459827907941115)}
[75/45] Running with max_contemp_tokens=4, eval_temp=0.3
^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[AYou are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.Llama
TokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to
 use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was
added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore th
is message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:31<00:00,  1.09it/s]
Average time taken for each sample: 0.9123409223556519, Average time taken for contemplation: 0.020040993690490724
Evaluation results: {'numerical_accuracy': 0.29, 'close_match_rate': 0.3, 'mean_relative_error': np.float64(5.055300028040912e+28), 'median_relat
ive_error': np.float64(0.1111111111111111)}
[76/45] Running with max_contemp_tokens=4, eval_temp=0.4
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:01<00:00,  1.52it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:31<00:00,  1.09it/s]
Average time taken for each sample: 0.9129472541809082, Average time taken for contemplation: 0.019556937217712404
Evaluation results: {'numerical_accuracy': 0.29, 'close_match_rate': 0.3, 'mean_relative_error': np.float64(1.6112656867536115), 'median_relative
_error': np.float64(0.1111111111111111)}
[77/45] Running with max_contemp_tokens=4, eval_temp=0.5
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:31<00:00,  1.09it/s]
Average time taken for each sample: 0.9119463777542114, Average time taken for contemplation: 0.019805803298950195
Evaluation results: {'numerical_accuracy': 0.3, 'close_match_rate': 0.33, 'mean_relative_error': np.float64(1000.8514777732217), 'median_relative
_error': np.float64(0.1111111111111111)}
[78/45] Running with max_contemp_tokens=4, eval_temp=0.6
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:31<00:00,  1.09it/s]
Average time taken for each sample: 0.9122499418258667, Average time taken for contemplation: 0.019738683700561522
Evaluation results: {'numerical_accuracy': 0.25, 'close_match_rate': 0.28, 'mean_relative_error': np.float64(1.7156017632351672), 'median_relativ
e_error': np.float64(0.12966967616370223)}
[79/45] Running with max_contemp_tokens=4, eval_temp=0.7
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.50it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:31<00:00,  1.09it/s]
Average time taken for each sample: 0.9121808767318725, Average time taken for contemplation: 0.019685966968536375
Evaluation results: {'numerical_accuracy': 0.25, 'close_match_rate': 0.28, 'mean_relative_error': np.float64(2.187572464331233), 'median_relative
_error': np.float64(0.16228070175438597)}
[80/45] Running with max_contemp_tokens=4, eval_temp=0.8
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:31<00:00,  1.09it/s]
Average time taken for each sample: 0.911938796043396, Average time taken for contemplation: 0.019822838306427
Evaluation results: {'numerical_accuracy': 0.24, 'close_match_rate': 0.28, 'mean_relative_error': np.float64(2.1087085691510397), 'median_relativ
e_error': np.float64(0.20854236343366778)}
[81/45] Running with max_contemp_tokens=4, eval_temp=0.9
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.45it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:31<00:00,  1.09it/s]
Average time taken for each sample: 0.9121308183670044, Average time taken for contemplation: 0.019519588947296142
Evaluation results: {'numerical_accuracy': 0.23, 'close_match_rate': 0.27, 'mean_relative_error': np.float64(5.688077961150276e+21), 'median_rela
tive_error': np.float64(0.20854236343366778)}
[82/45] Running with max_contemp_tokens=5, eval_temp=0.1
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:34<00:00,  1.06it/s]
Average time taken for each sample: 0.9370247077941894, Average time taken for contemplation: 0.019701619148254395
Evaluation results: {'numerical_accuracy': 0.11, 'close_match_rate': 0.11, 'mean_relative_error': np.float64(7.303709310738527e+31), 'median_rela
tive_error': np.float64(0.436456400742115)}
[83/45] Running with max_contemp_tokens=5, eval_temp=0.2
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.47it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:34<00:00,  1.06it/s]
Average time taken for each sample: 0.9358118414878845, Average time taken for contemplation: 0.019217612743377684
Evaluation results: {'numerical_accuracy': 0.15, 'close_match_rate': 0.15, 'mean_relative_error': np.float64(6.771923320438044e+31), 'median_rela
tive_error': np.float64(0.3484848484848485)}
[84/45] Running with max_contemp_tokens=5, eval_temp=0.3
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:34<00:00,  1.06it/s]
Average time taken for each sample: 0.9361671447753906, Average time taken for contemplation: 0.019405150413513185
Evaluation results: {'numerical_accuracy': 0.1, 'close_match_rate': 0.11, 'mean_relative_error': np.float64(5.66081218246396e+31), 'median_relati
ve_error': np.float64(0.5831299572910311)}
[85/45] Running with max_contemp_tokens=5, eval_temp=0.4
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:34<00:00,  1.06it/s]
Average time taken for each sample: 0.9362880039215088, Average time taken for contemplation: 0.01960136890411377
Evaluation results: {'numerical_accuracy': 0.11, 'close_match_rate': 0.13, 'mean_relative_error': np.float64(4.653721461354177e+31), 'median_rela
tive_error': np.float64(0.36658379382204115)}
[86/45] Running with max_contemp_tokens=5, eval_temp=0.5
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:01<00:00,  1.53it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:34<00:00,  1.06it/s]
Average time taken for each sample: 0.9370154786109924, Average time taken for contemplation: 0.019582242965698243
Evaluation results: {'numerical_accuracy': 0.14, 'close_match_rate': 0.15, 'mean_relative_error': np.float64(4.6537214603756115e+31), 'median_rel
ative_error': np.float64(0.3911320975381557)}
[87/45] Running with max_contemp_tokens=5, eval_temp=0.6
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.50it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:34<00:00,  1.06it/s]
Average time taken for each sample: 0.9369751000404358, Average time taken for contemplation: 0.019997284412384034
Evaluation results: {'numerical_accuracy': 0.13, 'close_match_rate': 0.15, 'mean_relative_error': np.float64(4.550942471787728e+31), 'median_rela
tive_error': np.float64(0.42130488559059986)}
[88/45] Running with max_contemp_tokens=5, eval_temp=0.7
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:01<00:00,  1.51it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:34<00:00,  1.06it/s]
Average time taken for each sample: 0.9389610314369201, Average time taken for contemplation: 0.020578184127807618
Evaluation results: {'numerical_accuracy': 0.13, 'close_match_rate': 0.14, 'mean_relative_error': np.float64(4.550942410538437e+31), 'median_rela
tive_error': np.float64(0.36363636363636365)}
[89/45] Running with max_contemp_tokens=5, eval_temp=0.8
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.49it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:34<00:00,  1.06it/s]
Average time taken for each sample: 0.9364949560165405, Average time taken for contemplation: 0.019266233444213868
Evaluation results: {'numerical_accuracy': 0.15, 'close_match_rate': 0.16, 'mean_relative_error': np.float64(4.4460753754602435e+31), 'median_rel
ative_error': np.float64(0.317958373566228)}
[90/45] Running with max_contemp_tokens=5, eval_temp=0.9
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expecte
d, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `leg
acy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://gi
thub.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|██████████████████████████| 3/3 [00:02<00:00,  1.48it/s]
Running inference:   0%|                                        | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from
 input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` t
o obtain reliable results.
Running inference: 100%|██████████████████████████████| 100/100 [01:34<00:00,  1.06it/s]
Average time taken for each sample: 0.9369127798080444, Average time taken for contemplation: 0.019934332370758055
Evaluation results: {'numerical_accuracy': 0.11, 'close_match_rate': 0.12, 'mean_relative_error': np.float64(4.446101114767098e+31), 'median_rela
tive_error': np.float64(0.43578021978021975)}
All combinations completed.
(effi_cot) nee7ne@uvavast:~/EfficientCoT$ tmux capture-pane -p -S - > /home/nee7ne/EfficientCoT/tmux_output.txt

